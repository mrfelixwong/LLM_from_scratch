{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Your First Transformer\n\nTraining a transformer is like learning to play piano - you practice, make mistakes, learn from them, and gradually improve.\n\n## Learning Objectives\n\n1. **Training Fundamentals**: What actually happens during training\n2. **The Training Loop**: The 4-step process that creates intelligence\n3. **Watching Learning**: See your model get smarter in real-time\n4. **When to Stop**: Recognize training completion signals\n\nBy the end, you'll understand exactly how transformers learn language!\n\nLet's demystify the training process! ðŸŽ¯"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, List, Dict\nimport math\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What is Training?\n\n**Training = Learning through repetition and feedback**\n\n**Piano analogy**:\n1. Play a song (make prediction)\n2. Teacher says \"wrong note\" (calculate loss)  \n3. Adjust finger placement (update weights)\n4. Try again and improve (repeat)\n\n**Transformer training**:\n1. Predict next word given context\n2. Check if prediction is correct (loss function)\n3. Adjust model weights slightly (backpropagation)\n4. Repeat millions of times (training loop)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "attempts = [\"Wrong note!\", \"Better!\", \"Almost!\", \"Perfect!\"]\nscores = [8.5, 6.2, 3.1, 1.0]\n\nprint(\"ðŸŽ¹ Piano Learning:\")\nfor i, (attempt, score) in enumerate(zip(attempts, scores)):\n    print(f\"Practice {i+1}: {attempt:<12} (Mistake level: {score})\")\n\nprint(f\"\\nMistake level: {scores[0]} â†’ {scores[-1]} (Learning!)\")\n\nprint(\"\\nðŸ¤– Transformer Learning:\")\npredictions = [\"dog\", \"table\", \"floor\", \"mat\"]\nconfidence = [20, 45, 60, 95]\n\nprint(\"Context: 'The cat sat on the ___'\")\nprint(\"Correct answer: 'mat'\")\n\nfor i, (pred, conf) in enumerate(zip(predictions, confidence)):\n    status = \"âœ…\" if pred == \"mat\" else \"âŒ\"\n    print(f\"Attempt {i+1}: '{pred}' ({conf}% sure) {status}\")\n\nprint(\"\\nðŸ’¡ Each wrong guess teaches the model!\")\nprint(\"Model learns: 'When I see \\\"on the\\\", try \\\"mat\\\" not \\\"dog\\\"'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Training Loop: 4 Simple Steps\n\n**The training recipe**:\n1. ðŸ³ **Forward pass**: Make a prediction\n2. ðŸ§‘â€ðŸ³ **Loss calculation**: Check how wrong you are  \n3. âš¡ **Backward pass**: Calculate adjustments\n4. ðŸ”„ **Weight update**: Apply adjustments and repeat\n\nThis simple loop creates all AI intelligence!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TinyTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model=32, n_heads=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n        self.norm = nn.LayerNorm(d_model)\n        self.output = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm(x + attn_out)\n        return self.output(x)\n\n# Create simple training data\ntext = \"The cat sat on the mat\"\nchars = sorted(set(text))\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nvocab_size = len(chars)\n\n# Convert text to indices\nindices = [char_to_idx[ch] for ch in text]\ninputs = torch.tensor(indices[:-1]).unsqueeze(0)\ntargets = torch.tensor(indices[1:]).unsqueeze(0)\n\nprint(f\"Training text: '{text}'\")\nprint(f\"Vocabulary: {chars}\")\nprint(f\"Input sequence: {inputs[0].tolist()}\")\nprint(f\"Target sequence: {targets[0].tolist()}\")\n\nmodel = TinyTransformer(vocab_size)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\ndef training_step(model, inputs, targets, optimizer):\n    # Step 1: Forward pass\n    logits = model(inputs)\n    print(\"1ï¸âƒ£ Forward pass: Made predictions âœ“\")\n    \n    # Step 2: Calculate loss\n    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n    print(f\"2ï¸âƒ£ Loss calculation: {loss.item():.3f} âœ“\")\n    \n    # Step 3: Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    print(\"3ï¸âƒ£ Backward pass: Calculated gradients âœ“\")\n    \n    # Step 4: Update weights\n    optimizer.step()\n    print(\"4ï¸âƒ£ Weight update: Applied changes âœ“\\n\")\n    \n    return loss.item()\n\nprint(\"ðŸš€ Training steps:\")\nlosses = []\nfor step in range(3):\n    print(f\"Step {step + 1}:\")\n    loss = training_step(model, inputs, targets, optimizer)\n    losses.append(loss)\n\nprint(\"ðŸ“Š Learning progress:\")\nfor i, loss in enumerate(losses):\n    trend = \"ðŸ“‰\" if i > 0 and loss < losses[i-1] else \"ðŸ“ˆ\"\n    print(f\"Step {i+1}: Loss = {loss:.3f} {trend}\")\n\nprint(\"\\nâœ… Training loop complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Watching Learning Happen\n\nLet's train for more steps and watch the transformer get smarter! We'll see loss decrease and generated text improve."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_text(model, char_to_idx, idx_to_char, start_text=\"The\", max_length=10):\n    model.eval()\n    with torch.no_grad():\n        # Convert start text to indices\n        current = [char_to_idx.get(ch, 0) for ch in start_text]\n        \n        for _ in range(max_length - len(start_text)):\n            if len(current) == 0:\n                break\n                \n            # Get prediction for next character\n            x = torch.tensor(current).unsqueeze(0)\n            logits = model(x)\n            next_char_logits = logits[0, -1, :]\n            \n            # Sample next character\n            probs = F.softmax(next_char_logits, dim=-1)\n            next_idx = torch.multinomial(probs, 1).item()\n            current.append(next_idx)\n        \n        # Convert back to text\n        result = ''.join(idx_to_char.get(i, '?') for i in current)\n        return result\n    \nmodel.train()\n\n# Create index-to-character mapping\nidx_to_char = {i: ch for ch, i in char_to_idx.items()}\n\n# Extended training with monitoring\nlosses = []\nsamples = []\n\nprint(\"ðŸ”¬ Watching transformer learn:\")\nprint(\"Step | Loss  | Generated Sample\")\nprint(\"-\" * 35)\n\nfor step in range(50):\n    # Training step\n    optimizer.zero_grad()\n    logits = model(inputs)\n    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    # Generate sample every 10 steps\n    if step % 10 == 0:\n        sample = generate_text(model, char_to_idx, idx_to_char, \"The\", 15)\n        samples.append((step, sample))\n        print(f\"{step:4d} | {loss.item():.3f} | '{sample}'\")\n\nprint(f\"\\nðŸ“ˆ Learning curve:\")\nplt.figure(figsize=(10, 6))\nplt.plot(losses, 'b-', linewidth=2, alpha=0.8)\nplt.title('ðŸ§  Transformer Learning Progress')\nplt.xlabel('Training Step')\nplt.ylabel('Loss (Lower = Better)')\nplt.grid(True, alpha=0.3)\n\n# Add annotations\nplt.annotate('ðŸ¤” Confused', xy=(0, losses[0]), \n            xytext=(5, losses[0] + 0.2), fontsize=12,\n            arrowprops=dict(arrowstyle='->', color='red'))\n            \nplt.annotate('ðŸŽ¯ Learning!', xy=(len(losses)-1, losses[-1]), \n            xytext=(len(losses)-10, losses[-1] + 0.2), fontsize=12,\n            arrowprops=dict(arrowstyle='->', color='green'))\n\nplt.show()\n\nprint(f\"\\nðŸŽ­ Generated samples over time:\")\nfor step, sample in samples:\n    interpretation = \"Random gibberish\" if step == 0 else \"Getting better!\" if step < 30 else \"Much improved!\"\n    print(f\"Step {step:2d}: '{sample}' ({interpretation})\")\n\nprint(f\"\\nâœ¨ Loss improved from {losses[0]:.3f} to {losses[-1]:.3f}\")\nprint(\"That's the transformer learning language patterns!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## When to Stop Training\n\n**How do you know when training is complete?**\n\n**3 key signals**:\n1. **Loss stops decreasing** - Model isn't learning anymore\n2. **Generated text looks good** - Output makes sense\n3. **Validation performance levels off** - No more generalization improvement\n\nThink of it like learning to drive - you know you're ready when you can handle real traffic safely!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate training completion signals\nsteps = np.arange(100)\nloss_curve = 3.0 * np.exp(-steps/20) + 1.0 + 0.1 * np.sin(steps/5) * np.exp(-steps/30)\n\nsample_progression = [\n    (0, \"ghjk qwerty random\"),      \n    (20, \"The cat dog tree\"),       \n    (40, \"The cat sat on\"),         \n    (60, \"The cat sat on the mat\"), \n    (80, \"The cat sat on the mat.\"),\n    (99, \"The cat sat on the mat.\")\n]\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# Loss curve showing convergence\nax1.plot(steps, loss_curve, 'b-', linewidth=2)\nax1.set_title('ðŸŽ¯ Signal 1: Loss Convergence')\nax1.set_xlabel('Training Step')\nax1.set_ylabel('Loss')\nax1.grid(True, alpha=0.3)\n\nax1.annotate('ðŸ“‰ Rapid learning', xy=(20, loss_curve[20]), \n            xytext=(30, loss_curve[20] + 0.5),\n            arrowprops=dict(arrowstyle='->', color='red'))\nax1.annotate('ðŸŽ¯ Converged!', xy=(80, loss_curve[80]), \n            xytext=(60, loss_curve[80] + 0.3),\n            arrowprops=dict(arrowstyle='->', color='green'))\n\n# Sample quality improvement\nax2.set_xlim(0, 100)\nax2.set_ylim(-0.5, len(sample_progression) - 0.5)\nax2.set_title('ðŸŽ­ Signal 2: Generated Text Quality')\nax2.set_xlabel('Training Step')\n\nfor i, (step, sample) in enumerate(sample_progression):\n    color = 'red' if i < 2 else 'orange' if i < 4 else 'green'\n    ax2.scatter(step, i, s=100, color=color, alpha=0.7)\n    ax2.text(step + 2, i, f'\"{sample}\"', fontsize=10, \n            verticalalignment='center', color=color, fontweight='bold')\n\nax2.set_yticks(range(len(sample_progression)))\nax2.set_yticklabels(['Terrible', 'Bad', 'Getting there', 'Good', 'Great!', 'Perfect'])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ðŸ›‘ Simple stopping rules for beginners:\")\nprint()\nprint(\"1. ðŸ• TIME RULE: Stop after reasonable training time\")\nprint(\"2. ðŸ“‰ LOSS RULE: Stop when loss stops decreasing for 20+ steps\")\nprint(\"3. ðŸŽ¯ GENERATION RULE: Stop when samples look reasonable\")\nprint(\"4. ðŸ’¾ PATIENCE RULE: Save best model, stop if no improvement\")\n\nprint(\"\\nâš ï¸ WARNING: Don't overtrain!\")\nprint(\"Overtraining = memorizing instead of understanding\")\nprint(\"Like cramming for a test vs actually learning the subject\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: You Now Understand Transformer Training!\n\nYou've mastered the core process that creates all language AI!\n\n### The Magic Recipe\n1. **Forward pass**: Model predicts next word\n2. **Loss calculation**: Measure prediction error  \n3. **Backward pass**: Calculate weight adjustments\n4. **Weight update**: Apply adjustments\n5. **Repeat**: Millions of times = intelligence!\n\n### Key Insights\n- **Training = Practice**: Like learning piano through repetition\n- **Loss = Confusion**: Lower loss = better understanding\n- **Learning = Adjustment**: Small weight changes accumulate to intelligence\n- **Stopping = Recognition**: Know when model has learned enough\n\n### What's Next?\nNow you understand how transformers learn! This same process creates ChatGPT, GPT-4, and all language models - just at massive scale.\n\nReady to explore text generation techniques! ðŸš€"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overfitting():\n",
    "    \"\"\"Show how overfitting manifests in language models.\"\"\"\n",
    "    \n",
    "    print(\"Overfitting in Language Models\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a tiny dataset to encourage overfitting\n",
    "    tiny_text = \"The cat sat on the mat. The cat sat on the hat.\"\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Encode the text\n",
    "    tokens = tokenizer.encode(tiny_text, add_special_tokens=False)\n",
    "    print(f\"Training text: '{tiny_text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SimpleTextDataset(tiny_text, tokenizer, block_size=8)\n",
    "    dataloader = create_dataloader(dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nTraining samples:\")\n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if i < 3:  # Show first 3 batches\n",
    "            print(f\"Batch {i}: input shape {input_ids.shape}\")\n",
    "            for j in range(input_ids.shape[0]):\n",
    "                input_text = tokenizer.decode(input_ids[j].tolist(), skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(target_ids[j].tolist(), skip_special_tokens=True)\n",
    "                print(f\"  Input:  '{input_text}'\")\n",
    "                print(f\"  Target: '{target_text}'\")\n",
    "    \n",
    "    print(\"\\nSigns of Overfitting:\")\n",
    "    print(\"â€¢ Training loss goes to zero but validation loss increases\")\n",
    "    print(\"â€¢ Model memorizes training data exactly\")\n",
    "    print(\"â€¢ Poor generalization to new text\")\n",
    "    print(\"â€¢ Generated text becomes repetitive or nonsensical\")\n",
    "    \n",
    "    return dataset, dataloader\n",
    "\n",
    "tiny_dataset, tiny_dataloader = demonstrate_overfitting()\n",
    "\n",
    "def demonstrate_regularization_techniques():\n",
    "    \"\"\"Show different regularization methods.\"\"\"\n",
    "    \n",
    "    print(\"\\nRegularization Techniques for Transformers\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    techniques = {\n",
    "        \"Dropout\": {\n",
    "            \"description\": \"Randomly zero out neurons during training\",\n",
    "            \"implementation\": \"nn.Dropout(p=0.1) in attention and FFN\",\n",
    "            \"effect\": \"Prevents co-adaptation of neurons\"\n",
    "        },\n",
    "        \"Weight Decay\": {\n",
    "            \"description\": \"Add L2 penalty to weights\",\n",
    "            \"implementation\": \"weight_decay=0.01 in optimizer\",\n",
    "            \"effect\": \"Keeps weights small, improves generalization\"\n",
    "        },\n",
    "        \"Gradient Clipping\": {\n",
    "            \"description\": \"Limit gradient magnitude\",\n",
    "            \"implementation\": \"clip_grad_norm_(params, max_norm=1.0)\",\n",
    "            \"effect\": \"Prevents exploding gradients\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"description\": \"Stop when validation loss stops improving\",\n",
    "            \"implementation\": \"Monitor validation loss, save best model\",\n",
    "            \"effect\": \"Prevents overfitting to training data\"\n",
    "        },\n",
    "        \"Data Augmentation\": {\n",
    "            \"description\": \"Increase effective dataset size\",\n",
    "            \"implementation\": \"Paraphrasing, back-translation, masking\",\n",
    "            \"effect\": \"More diverse training examples\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, info in techniques.items():\n",
    "        print(f\"\\n{technique}:\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Implementation: {info['implementation']}\")\n",
    "        print(f\"  Effect: {info['effect']}\")\n",
    "    \n",
    "    # Visualize dropout effect\n",
    "    print(\"\\nDropout Visualization:\")\n",
    "    \n",
    "    # Simulate dropout on a tensor\n",
    "    x = torch.ones(4, 8)  # 4x8 tensor of ones\n",
    "    dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(x, cmap='Blues')\n",
    "    axes[0].set_title('Original Activations')\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Example')\n",
    "    \n",
    "    # With dropout (training mode)\n",
    "    dropout.train()\n",
    "    x_dropout = dropout(x)\n",
    "    axes[1].imshow(x_dropout, cmap='Blues')\n",
    "    axes[1].set_title('With Dropout (Training)')\n",
    "    axes[1].set_xlabel('Feature')\n",
    "    \n",
    "    # Without dropout (eval mode)\n",
    "    dropout.eval()\n",
    "    x_eval = dropout(x)\n",
    "    axes[2].imshow(x_eval, cmap='Blues')\n",
    "    axes[2].set_title('Without Dropout (Evaluation)')\n",
    "    axes[2].set_xlabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice how dropout randomly zeros neurons during training but not evaluation!\")\n",
    "\n",
    "demonstrate_regularization_techniques()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}