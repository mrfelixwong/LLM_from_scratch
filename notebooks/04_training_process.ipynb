{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Transformers: From Random to Intelligent\n\nTraining a transformer is like teaching someone to complete sentences. They start by guessing randomly, but through practice and feedback, they learn language patterns.\n\n## The Learning Process\n1. **Make a guess** - Predict the next word\n2. **Get feedback** - Check if guess was right  \n3. **Adjust approach** - Update internal understanding\n4. **Try again** - Repeat millions of times\n\n## What You'll Learn\n- **Training loop fundamentals** - The 4-step process\n- **Loss and optimization** - How models improve\n- **Watching learning happen** - See intelligence emerge\n- **When to stop** - Recognizing completion"
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, List, Dict\nimport math\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## What is Training?\n\nDemonstrate the core concept through analogy and simple examples."
  },
  {
   "cell_type": "markdown",
   "source": "## Training Loop Fundamentals\n\nShow the 4-step training process that creates AI intelligence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Learning analogy: practice makes perfect\nattempts = [\"Wrong note!\", \"Better!\", \"Almost!\", \"Perfect!\"]\nmistakes = [8.5, 6.2, 3.1, 1.0]\n\nprint(\"ðŸŽ¹ Piano Learning Example:\")\nfor i, (attempt, mistake) in enumerate(zip(attempts, mistakes)):\n    print(f\"Practice {i+1}: {attempt:<12} (Mistake level: {mistake})\")\n\nprint(f\"Mistake reduction: {mistakes[0]} â†’ {mistakes[-1]} (Learning!)\")\n\nprint(\"\\nðŸ¤– Transformer Learning:\")\npredictions = [\"dog\", \"table\", \"floor\", \"mat\"]\nconfidence = [20, 45, 60, 95]\n\nprint(\"Context: 'The cat sat on the ___'\")\nprint(\"Correct answer: 'mat'\")\n\nfor i, (pred, conf) in enumerate(zip(predictions, confidence)):\n    status = \"âœ…\" if pred == \"mat\" else \"âŒ\"\n    print(f\"Attempt {i+1}: '{pred}' ({conf}% sure) {status}\")\n\nprint(\"\\nðŸ’¡ Key insight: Each mistake teaches the model!\")\nprint(\"Model learns: 'After \\\"on the\\\", try \\\"mat\\\" not \\\"dog\\\"'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Simple Training Implementation\n\nBuild a minimal transformer and demonstrate the 4-step training loop."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "class TinyTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model=32, n_heads=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n        self.norm = nn.LayerNorm(d_model)\n        self.output = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm(x + attn_out)\n        return self.output(x)\n\n# Create simple training data\ntext = \"The cat sat on the mat\"\nchars = sorted(set(text))\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nvocab_size = len(chars)\n\n# Convert text to indices for training\nindices = [char_to_idx[ch] for ch in text]\ninputs = torch.tensor(indices[:-1]).unsqueeze(0)   # Input sequence\ntargets = torch.tensor(indices[1:]).unsqueeze(0)   # Target sequence (shifted by 1)\n\nprint(f\"Training text: '{text}'\")\nprint(f\"Vocabulary: {chars}\")\nprint(f\"Vocab size: {vocab_size}\")\nprint(f\"Input sequence: {inputs[0].tolist()}\")\nprint(f\"Target sequence: {targets[0].tolist()}\")\n\n# Initialize model and optimizer\nmodel = TinyTransformer(vocab_size)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\ndef training_step(model, inputs, targets, optimizer):\n    \"\"\"Demonstrate the 4-step training process\"\"\"\n    \n    # Step 1: Forward pass - make predictions\n    logits = model(inputs)\n    print(\"1ï¸âƒ£ Forward pass: Made predictions âœ“\")\n    \n    # Step 2: Calculate loss - measure mistakes  \n    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n    print(f\"2ï¸âƒ£ Loss calculation: {loss.item():.3f} (lower = better) âœ“\")\n    \n    # Step 3: Backward pass - calculate adjustments\n    optimizer.zero_grad()\n    loss.backward()\n    print(\"3ï¸âƒ£ Backward pass: Calculated gradients âœ“\")\n    \n    # Step 4: Update weights - apply adjustments\n    optimizer.step()\n    print(\"4ï¸âƒ£ Weight update: Applied changes âœ“\\n\")\n    \n    return loss.item()\n\nprint(\"ðŸš€ Training steps demonstration:\")\nlosses = []\nfor step in range(3):\n    print(f\"Training Step {step + 1}:\")\n    loss = training_step(model, inputs, targets, optimizer)\n    losses.append(loss)\n\nprint(\"ðŸ“Š Learning progress:\")\nfor i, loss in enumerate(losses):\n    trend = \"ðŸ“‰ Improving!\" if i > 0 and loss < losses[i-1] else \"ðŸ“ˆ\"\n    print(f\"Step {i+1}: Loss = {loss:.3f} {trend}\")\n\nprint(\"\\nâœ… Training loop complete - model got smarter!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Extended Training Session\n\nTrain for more steps and watch the transformer learn to generate text."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def generate_text(model, char_to_idx, idx_to_char, start_text=\"The\", max_length=10):\n    \"\"\"Generate text to see what the model has learned\"\"\"\n    model.eval()\n    with torch.no_grad():\n        current = [char_to_idx.get(ch, 0) for ch in start_text]\n        \n        for _ in range(max_length - len(start_text)):\n            if len(current) == 0:\n                break\n                \n            x = torch.tensor(current).unsqueeze(0)\n            logits = model(x)\n            next_char_logits = logits[0, -1, :]\n            \n            # Sample next character\n            probs = F.softmax(next_char_logits, dim=-1)\n            next_idx = torch.multinomial(probs, 1).item()\n            current.append(next_idx)\n        \n        result = ''.join(idx_to_char.get(i, '?') for i in current)\n        return result\n\nmodel.train()\nidx_to_char = {i: ch for ch, i in char_to_idx.items()}\n\n# Extended training with monitoring\nlosses = []\nsamples = []\n\nprint(\"ðŸ”¬ Watching transformer learn step by step:\")\nprint(\"Step | Loss  | Generated Sample\")\nprint(\"-\" * 40)\n\nfor step in range(50):\n    # Training step\n    optimizer.zero_grad()\n    logits = model(inputs)\n    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    \n    # Generate sample every 10 steps to see progress\n    if step % 10 == 0:\n        sample = generate_text(model, char_to_idx, idx_to_char, \"The\", 15)\n        samples.append((step, sample))\n        print(f\"{step:4d} | {loss.item():.3f} | '{sample}'\")\n\n# Visualize learning curve\nplt.figure(figsize=(10, 6))\nplt.plot(losses, 'b-', linewidth=2, alpha=0.8)\nplt.title('ðŸ§  Transformer Learning Progress')\nplt.xlabel('Training Step')\nplt.ylabel('Loss (Lower = Better)')\nplt.grid(True, alpha=0.3)\n\n# Add annotations to show learning phases\nplt.annotate('ðŸ¤” Random guessing', xy=(0, losses[0]), \n            xytext=(5, losses[0] + 0.2), fontsize=12,\n            arrowprops=dict(arrowstyle='->', color='red'))\n            \nplt.annotate('ðŸŽ¯ Learning patterns!', xy=(len(losses)-1, losses[-1]), \n            xytext=(len(losses)-10, losses[-1] + 0.2), fontsize=12,\n            arrowprops=dict(arrowstyle='->', color='green'))\n\nplt.show()\n\nprint(f\"\\nðŸŽ­ Generated samples over time:\")\nfor step, sample in samples:\n    interpretation = \"Gibberish\" if step == 0 else \"Better\" if step < 30 else \"Much improved!\"\n    print(f\"Step {step:2d}: '{sample}' ({interpretation})\")\n\nprint(f\"\\nâœ¨ Loss improved from {losses[0]:.3f} to {losses[-1]:.3f}\")\nprint(\"The transformer learned to complete sentences!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## When to Stop Training\n\nLearn to recognize the signals that training is complete."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simulate training completion signals\nsteps = np.arange(100)\nloss_curve = 3.0 * np.exp(-steps/20) + 1.0 + 0.1 * np.sin(steps/5) * np.exp(-steps/30)\n\n# Sample progression showing quality improvement\nsample_progression = [\n    (0, \"Random gibberish\"),      \n    (20, \"The cat dog tree\"),       \n    (40, \"The cat sat on\"),         \n    (60, \"The cat sat on the mat\"), \n    (80, \"The cat sat on the mat.\"),\n    (99, \"The cat sat on the mat.\")\n]\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# Loss curve showing convergence\nax1.plot(steps, loss_curve, 'b-', linewidth=2)\nax1.set_title('ðŸŽ¯ Signal 1: Loss Converges (Stops Decreasing)')\nax1.set_xlabel('Training Step')\nax1.set_ylabel('Loss')\nax1.grid(True, alpha=0.3)\n\nax1.annotate('ðŸ“‰ Rapid learning', xy=(20, loss_curve[20]), \n            xytext=(30, loss_curve[20] + 0.5),\n            arrowprops=dict(arrowstyle='->', color='red'))\nax1.annotate('ðŸŽ¯ Converged - ready to stop!', xy=(80, loss_curve[80]), \n            xytext=(60, loss_curve[80] + 0.3),\n            arrowprops=dict(arrowstyle='->', color='green'))\n\n# Sample quality improvement\nax2.set_xlim(0, 100)\nax2.set_ylim(-0.5, len(sample_progression) - 0.5)\nax2.set_title('ðŸŽ­ Signal 2: Generated Text Quality Improves')\nax2.set_xlabel('Training Step')\n\nfor i, (step, sample) in enumerate(sample_progression):\n    color = 'red' if i < 2 else 'orange' if i < 4 else 'green'\n    ax2.scatter(step, i, s=100, color=color, alpha=0.7)\n    ax2.text(step + 2, i, f'\"{sample}\"', fontsize=10, \n            verticalalignment='center', color=color, fontweight='bold')\n\nax2.set_yticks(range(len(sample_progression)))\nax2.set_yticklabels(['Terrible', 'Bad', 'Getting there', 'Good', 'Great!', 'Perfect'])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ðŸ›‘ Simple stopping rules:\")\nprint(\"1. ðŸ“‰ LOSS RULE: Stop when loss plateaus for 20+ steps\")\nprint(\"2. ðŸŽ¯ QUALITY RULE: Stop when generated text looks reasonable\")\nprint(\"3. â° TIME RULE: Stop after reasonable training time\")\nprint(\"4. ðŸ’¾ SAFETY RULE: Save best model, stop if no improvement\")\n\nprint(\"\\nâš ï¸ Warning: Don't overtrain!\")\nprint(\"Overtraining = memorizing instead of understanding\")\nprint(\"Like cramming vs learning - works on test, fails in real world\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nYou now understand how transformers learn!\n\n**The Training Recipe**:\n1. **Forward pass**: Model makes predictions\n2. **Loss calculation**: Measure prediction errors\n3. **Backward pass**: Calculate weight adjustments  \n4. **Weight update**: Apply adjustments and repeat\n\n**Key Insights**:\n- **Training = Practice**: Like learning piano through repetition\n- **Loss = Confusion**: Lower loss means better understanding\n- **Learning = Adjustment**: Small weight changes create intelligence\n- **Stopping = Recognition**: Know when the model has learned enough\n\n**What's Next**: This same process creates ChatGPT, GPT-4, and all language models - just at massive scale with billions of parameters!\n\nReady to explore text generation techniques! ðŸš€"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overfitting():\n",
    "    \"\"\"Show how overfitting manifests in language models.\"\"\"\n",
    "    \n",
    "    print(\"Overfitting in Language Models\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a tiny dataset to encourage overfitting\n",
    "    tiny_text = \"The cat sat on the mat. The cat sat on the hat.\"\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Encode the text\n",
    "    tokens = tokenizer.encode(tiny_text, add_special_tokens=False)\n",
    "    print(f\"Training text: '{tiny_text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SimpleTextDataset(tiny_text, tokenizer, block_size=8)\n",
    "    dataloader = create_dataloader(dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nTraining samples:\")\n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if i < 3:  # Show first 3 batches\n",
    "            print(f\"Batch {i}: input shape {input_ids.shape}\")\n",
    "            for j in range(input_ids.shape[0]):\n",
    "                input_text = tokenizer.decode(input_ids[j].tolist(), skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(target_ids[j].tolist(), skip_special_tokens=True)\n",
    "                print(f\"  Input:  '{input_text}'\")\n",
    "                print(f\"  Target: '{target_text}'\")\n",
    "    \n",
    "    print(\"\\nSigns of Overfitting:\")\n",
    "    print(\"â€¢ Training loss goes to zero but validation loss increases\")\n",
    "    print(\"â€¢ Model memorizes training data exactly\")\n",
    "    print(\"â€¢ Poor generalization to new text\")\n",
    "    print(\"â€¢ Generated text becomes repetitive or nonsensical\")\n",
    "    \n",
    "    return dataset, dataloader\n",
    "\n",
    "tiny_dataset, tiny_dataloader = demonstrate_overfitting()\n",
    "\n",
    "def demonstrate_regularization_techniques():\n",
    "    \"\"\"Show different regularization methods.\"\"\"\n",
    "    \n",
    "    print(\"\\nRegularization Techniques for Transformers\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    techniques = {\n",
    "        \"Dropout\": {\n",
    "            \"description\": \"Randomly zero out neurons during training\",\n",
    "            \"implementation\": \"nn.Dropout(p=0.1) in attention and FFN\",\n",
    "            \"effect\": \"Prevents co-adaptation of neurons\"\n",
    "        },\n",
    "        \"Weight Decay\": {\n",
    "            \"description\": \"Add L2 penalty to weights\",\n",
    "            \"implementation\": \"weight_decay=0.01 in optimizer\",\n",
    "            \"effect\": \"Keeps weights small, improves generalization\"\n",
    "        },\n",
    "        \"Gradient Clipping\": {\n",
    "            \"description\": \"Limit gradient magnitude\",\n",
    "            \"implementation\": \"clip_grad_norm_(params, max_norm=1.0)\",\n",
    "            \"effect\": \"Prevents exploding gradients\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"description\": \"Stop when validation loss stops improving\",\n",
    "            \"implementation\": \"Monitor validation loss, save best model\",\n",
    "            \"effect\": \"Prevents overfitting to training data\"\n",
    "        },\n",
    "        \"Data Augmentation\": {\n",
    "            \"description\": \"Increase effective dataset size\",\n",
    "            \"implementation\": \"Paraphrasing, back-translation, masking\",\n",
    "            \"effect\": \"More diverse training examples\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, info in techniques.items():\n",
    "        print(f\"\\n{technique}:\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Implementation: {info['implementation']}\")\n",
    "        print(f\"  Effect: {info['effect']}\")\n",
    "    \n",
    "    # Visualize dropout effect\n",
    "    print(\"\\nDropout Visualization:\")\n",
    "    \n",
    "    # Simulate dropout on a tensor\n",
    "    x = torch.ones(4, 8)  # 4x8 tensor of ones\n",
    "    dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(x, cmap='Blues')\n",
    "    axes[0].set_title('Original Activations')\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Example')\n",
    "    \n",
    "    # With dropout (training mode)\n",
    "    dropout.train()\n",
    "    x_dropout = dropout(x)\n",
    "    axes[1].imshow(x_dropout, cmap='Blues')\n",
    "    axes[1].set_title('With Dropout (Training)')\n",
    "    axes[1].set_xlabel('Feature')\n",
    "    \n",
    "    # Without dropout (eval mode)\n",
    "    dropout.eval()\n",
    "    x_eval = dropout(x)\n",
    "    axes[2].imshow(x_eval, cmap='Blues')\n",
    "    axes[2].set_title('Without Dropout (Evaluation)')\n",
    "    axes[2].set_xlabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice how dropout randomly zeros neurons during training but not evaluation!\")\n",
    "\n",
    "demonstrate_regularization_techniques()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}