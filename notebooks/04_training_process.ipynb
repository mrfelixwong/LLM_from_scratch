{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Your First Transformer: Like Learning to Cook\n\nTraining a transformer is like learning to cook. You start with a recipe (model), try it out, see how it tastes (check the loss), and then adjust your technique to make it better next time.\n\n## What You'll Learn\n\n1. **What is Training?** - Think of it like practicing piano - getting better with each attempt\n2. **The Training Loop** - Make a prediction, see how wrong you are, adjust, repeat\n3. **Watching Learning Happen** - See your model get smarter in real-time\n4. **When to Stop** - Know when your model has learned enough\n\nBy the end, you'll watch your transformer learn to predict text and understand exactly what's happening at each step!\n\nLet's start cooking! üë©‚Äçüç≥"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. What is Training? üéØ\n\n**Training = Practice Makes Perfect**\n\nImagine you're learning to play piano. At first, you hit wrong notes. But with each practice session:\n1. You play a song (make a prediction)\n2. Your teacher says \"that note was wrong\" (calculate loss)\n3. You adjust your finger placement (update weights)\n4. You try again and get better!\n\n**For transformers, it's the same:**\n- The transformer predicts the next word\n- We check if it's right or wrong (that's the \"loss\")\n- We nudge the model to be slightly better\n- Repeat thousands of times until it's good!\n\nLet's see this in action with the simplest possible example:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demonstrate_learning_concept():\n    \"\"\"Show learning in the simplest possible way.\"\"\"\n    \n    print(\"üéπ LEARNING PIANO ANALOGY\")\n    print(\"=\" * 30)\n    \n    # Simulate \"learning\" with simple numbers\n    attempts = [\"Wrong note!\", \"Better!\", \"Almost!\", \"Perfect!\"]\n    scores = [8.5, 6.2, 3.1, 1.0]  # Lower is better (like loss)\n    \n    print(\"Piano practice sessions:\")\n    for i, (attempt, score) in enumerate(zip(attempts, scores)):\n        print(f\"Practice {i+1}: {attempt:<12} (Mistake level: {score})\")\n    \n    print(f\"\\nSee how the mistake level goes down? That's learning!\")\n    print(f\"Session 1: {scores[0]} mistakes ‚Üí Session 4: {scores[-1]} mistakes\")\n    \n    print(\"\\nü§ñ TRANSFORMER LEARNING (SAME IDEA)\")\n    print(\"=\" * 40)\n    \n    sentences = [\n        \"The cat sat on the ???\",  # What should come next?\n        \"The cat sat on the mat\",  # This is the right answer!\n    ]\n    \n    predictions = [\"dog\", \"table\", \"floor\", \"mat\"]  # Model guesses\n    confidence = [20, 45, 60, 95]  # How sure the model is\n    \n    print(\"Transformer trying to predict next word:\")\n    print(f\"Sentence: '{sentences[0]}'\")\n    print(f\"Correct answer: '{sentences[1].split()[-1]}'\")\n    print()\n    \n    for i, (pred, conf) in enumerate(zip(predictions, confidence)):\n        status = \"‚úÖ CORRECT!\" if pred == \"mat\" else \"‚ùå Wrong\"\n        print(f\"Attempt {i+1}: Predicts '{pred}' ({conf}% sure) {status}\")\n    \n    print(f\"\\nüí° KEY INSIGHT: Each wrong guess teaches the model!\")\n    print(f\"   The model learns: 'When I see \\\"on the\\\", try \\\"mat\\\" not \\\"dog\\\"'\")\n\ndemonstrate_learning_concept()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. The Training Loop - Just 4 Simple Steps! \n\n**Think of it like this simple recipe:**\n1. üç≥ **Make a prediction** (like cracking an egg)\n2. üßë‚Äçüç≥ **Check how wrong you are** (taste it - too salty?)\n3. ‚ö° **Adjust slightly** (add less salt next time)\n4. üîÑ **Repeat** until it tastes perfect!\n\nLet's see the actual code for these 4 steps:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Create the simplest possible transformer for demonstration\n\n# Get a tiny model (easier to understand)\nconfig = create_model_config(\"tiny\")  # Very small for demo\nmodel = GPTModel(**config)\n\nprint(\"üéØ TRAINING SETUP\")\nprint(\"=\" * 20)\nprint(\"Our tiny transformer:\")\nprint(f\"‚Ä¢ Has {model.get_num_params():,} parameters to learn\")\nprint(f\"‚Ä¢ Knows {config['vocab_size']:,} different words\")\nprint(f\"‚Ä¢ Can remember {config['max_len']} words at once\")\nprint()\n\n# Simple training data - just one sentence!\ntext = \"The cat sat on the mat\"\ntokens = tokenizer.encode(text, add_special_tokens=False)\ninputs = torch.tensor(tokens[:-1]).unsqueeze(0)   # All but last token\ntargets = torch.tensor(tokens[1:]).unsqueeze(0)   # All but first token\n\nprint(f\"üìù Training text: '{text}'\")\nprint(\"Model will learn to predict each next word:\")\nfor i in range(len(tokens)-1):\n    input_word = tokenizer.decode([tokens[i]])\n    target_word = tokenizer.decode([tokens[i+1]])\n    print(f\"   '{input_word}' ‚Üí predict '{target_word}'\")\nprint()\n\ndef simple_training_step(model, inputs, targets, optimizer):\n    \"\"\"The 4 steps of training - clearly separated!\"\"\"\n    \n    print(\"üîÑ TRAINING STEP\")\n    print(\"-\" * 15)\n    \n    # Step 1: Make a prediction üç≥\n    logits, _ = model(inputs)  # Model predicts next word probabilities\n    print(\"1Ô∏è‚É£ Made prediction ‚úì\")\n    \n    # Step 2: Check how wrong we are üßë‚Äçüç≥  \n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n    print(f\"2Ô∏è‚É£ Calculated loss: {loss.item():.3f} (lower = better)\")\n    \n    # Step 3: Adjust slightly ‚ö°\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(\"3Ô∏è‚É£ Updated model weights ‚úì\")\n    \n    # Step 4 happens when we call this function again! üîÑ\n    print(\"4Ô∏è‚É£ Ready for next step!\")\n    print()\n    \n    return loss.item()\n\n# Create optimizer (the thing that adjusts the model)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"üöÄ Let's do 3 training steps and watch the loss go down!\")\nprint(\"=\" * 50)\n\n# Do just 3 steps so we can see each one clearly\nlosses = []\nfor step in range(3):\n    print(f\"\\nüìç STEP {step + 1}/3\")\n    loss = simple_training_step(model, inputs, targets, optimizer)\n    losses.append(loss)\n    \n    # Show progress\n    if step == 0:\n        print(\"üë∂ First step - model is just guessing randomly\")\n    elif step == 1:\n        print(\"üß† Second step - model starts to learn patterns\")\n    else:\n        print(\"üéØ Third step - model is getting better!\")\n\nprint(f\"\\nüìä PROGRESS SUMMARY\")\nprint(\"=\" * 20)\nfor i, loss in enumerate(losses):\n    trend = \"\"\n    if i > 0:\n        if loss < losses[i-1]:\n            trend = \"üìâ (Getting better!)\"\n        else:\n            trend = \"üìà (Still learning...)\"\n    print(f\"Step {i+1}: Loss = {loss:.3f} {trend}\")\n\nprint(f\"\\nüéâ Did the loss go down? That means learning happened!\")\nif losses[-1] < losses[0]:\n    print(f\"‚úÖ YES! From {losses[0]:.3f} to {losses[-1]:.3f} - That's improvement!\")\nelse:\n    print(f\"‚è≥ Not quite yet, but that's normal! Training takes many steps.\")\n    \nprint(\"\\nüí° KEY INSIGHT:\")\nprint(\"This is exactly what happens during real transformer training,\")\nprint(\"but repeated millions of times with millions of text examples!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Watching Learning Happen - The Magic Moment! ‚ú®\n\n**This is where the magic happens!**\n\nWe've seen the 4 simple steps. Now let's repeat them hundreds of times and watch our transformer get smarter in real-time! It's like watching a baby learn to talk.\n\n**What to expect:**\n- Loss will start high (model is confused) \n- Loss drops rapidly (\"Aha!\" moments)\n- Generated text gets more coherent\n- Model learns patterns in the language"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's create more training data for more realistic learning\ntraining_text = \"\"\"\nThe cat sat on the mat.\nThe dog ran in the park.\nThe bird flew over the tree.\nThe fish swam in the pond.\nThe sun shines in the sky.\nThe moon glows at night.\nThe children play outside.\nThe flowers bloom in spring.\n\"\"\"\n\n# Create dataset and dataloader\nfrom src.data.dataset import SimpleTextDataset, create_dataloader\ndataset = SimpleTextDataset(training_text, tokenizer, block_size=8)\ndataloader = create_dataloader(dataset, batch_size=2, shuffle=True)\n\nprint(\"üéì LEARNING SETUP\")\nprint(\"=\" * 20)\nprint(f\"Training data: {len(training_text)} characters\")\nprint(f\"Training samples: {len(dataset)} examples\")\nprint(\"Sample sentences the model will learn from:\")\nfor line in training_text.strip().split('\\n'):\n    if line.strip():\n        print(f\"  ‚Ä¢ {line.strip()}\")\n\ndef watch_learning_happen(model, dataloader, optimizer, num_steps=20):\n    \"\"\"Watch the model learn step by step!\"\"\"\n    \n    print(\"\\nüî¨ WATCHING LEARNING HAPPEN\")\n    print(\"=\" * 30)\n    \n    losses = []\n    samples = []\n    \n    # Test before training\n    model.eval()\n    with torch.no_grad():\n        sample_input = torch.tensor(tokenizer.encode(\"The cat\", add_special_tokens=False)[:2]).unsqueeze(0)\n        sample_before = model.generate(sample_input, max_new_tokens=8, temperature=1.0, do_sample=True)\n        sample_before_text = tokenizer.decode(sample_before[0].tolist(), skip_special_tokens=True)\n    \n    print(f\"Before training: 'The cat' ‚Üí '{sample_before_text}'\")\n    print(\"(Probably nonsense - model doesn't know anything yet!)\\n\")\n    \n    # Train and observe\n    step = 0\n    data_iter = iter(dataloader)\n    \n    for _ in range(num_steps):\n        try:\n            batch = next(data_iter)\n        except StopIteration:\n            data_iter = iter(dataloader)  # Reset iterator\n            batch = next(data_iter)\n        \n        # Training step\n        model.train()\n        input_ids, target_ids = batch\n        \n        optimizer.zero_grad()\n        logits, _ = model(input_ids)\n        \n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n        \n        # Generate sample every 5 steps\n        if step % 5 == 0:\n            model.eval()\n            with torch.no_grad():\n                sample = model.generate(sample_input, max_new_tokens=8, temperature=1.0, do_sample=True)\n                sample_text = tokenizer.decode(sample[0].tolist(), skip_special_tokens=True)\n                samples.append((step, sample_text))\n                \n                # Show progress with interpretation\n                if step == 0:\n                    interpretation = \"ü§î Still very confused\"\n                elif step <= 5:\n                    interpretation = \"üß† Starting to recognize patterns\"\n                elif step <= 10:\n                    interpretation = \"üí° Learning word relationships\"\n                else:\n                    interpretation = \"üéØ Getting much better!\"\n                    \n                print(f\"Step {step:2d}: Loss={loss.item():.3f} | Sample: '{sample_text}' | {interpretation}\")\n        \n        step += 1\n    \n    # Final comparison\n    print(\"\\nüìä LEARNING PROGRESS SUMMARY\")\n    print(\"=\" * 30)\n    print(f\"Initial loss: {losses[0]:.3f} (very confused)\")\n    print(f\"Final loss:   {losses[-1]:.3f} (much better!)\")\n    print(f\"Improvement:  {losses[0] - losses[-1]:.3f} loss reduction\")\n    \n    print(\"\\nüé≠ GENERATION SAMPLES OVER TIME:\")\n    for step, sample in samples:\n        print(f\"Step {step:2d}: '{sample}'\")\n    \n    print(\"\\n‚ú® Notice how the text becomes more coherent?\")\n    print(\"That's the transformer learning language patterns!\")\n    \n    return losses, samples\n\n# Watch the magic happen!\nlosses, samples = watch_learning_happen(model, dataloader, optimizer, num_steps=20)\n\n# Plot the learning curve\nplt.figure(figsize=(10, 6))\nplt.plot(losses, 'b-', linewidth=2, alpha=0.8)\nplt.title('üß† Watching the Transformer Get Smarter', fontsize=14, fontweight='bold')\nplt.xlabel('Training Step')\nplt.ylabel('Loss (Confusion Level)')\nplt.grid(True, alpha=0.3)\n\n# Add annotations\nif len(losses) > 5:\n    plt.annotate('ü§î \"What is language?\"', xy=(0, losses[0]), \n                xytext=(2, losses[0] + 0.5), fontsize=12,\n                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n                \n    plt.annotate('üéØ \"I\\'m getting this!\"', xy=(len(losses)-1, losses[-1]), \n                xytext=(len(losses)-5, losses[-1] + 0.3), fontsize=12,\n                arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n\nplt.show()\n\nprint(\"\\nüéâ Congratulations! You just watched a transformer learn!\")\nprint(\"This is exactly how ChatGPT, GPT-4, and all language models learn.\")\nprint(\"The only difference: they do this with billions of examples!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. When to Stop - \"My Model is Trained!\" üèÅ\n\n**How do you know when your transformer is done learning?**\n\nThink of it like learning to drive:\n- **Beginner**: Can't even start the car (high loss, bad predictions)\n- **Learning**: Gets better every lesson (loss going down)\n- **Competent**: Can drive safely (good loss, sensible predictions)\n- **Expert**: Could teach others (very low loss)\n\n**The 3 signals that training is done:**\n1. **Loss stops improving** - \"I'm not getting better anymore\"\n2. **Generated text looks good** - \"My outputs make sense!\"\n3. **Validation loss levels off** - \"I'm not just memorizing\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's see what \"done training\" looks like\ndef show_training_completion_signals():\n    \"\"\"Show the 3 key signals that training is complete.\"\"\"\n    \n    print(\"üèÅ RECOGNIZING WHEN TRAINING IS DONE\")\n    print(\"=\" * 40)\n    \n    # Simulate a complete training run\n    steps = np.arange(100)\n    \n    # 1. Loss curve that levels off\n    loss_curve = 3.0 * np.exp(-steps/20) + 1.0 + 0.1 * np.sin(steps/5) * np.exp(-steps/30)\n    \n    # 2. Sample quality over time\n    sample_quality = [\n        (0, \"ghjk qwerty random\"),      # Random noise\n        (20, \"The cat dog tree\"),       # Some words\n        (40, \"The cat sat on\"),         # Getting structure\n        (60, \"The cat sat on the mat\"), # Good!\n        (80, \"The cat sat on the mat.\"), # Perfect!\n        (99, \"The cat sat on the mat.\") # Stable\n    ]\n    \n    # Plot training completion\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # Loss curve\n    ax1.plot(steps, loss_curve, 'b-', linewidth=2, alpha=0.8)\n    ax1.set_title('üéØ Signal 1: Loss Stops Improving', fontweight='bold')\n    ax1.set_xlabel('Training Step')\n    ax1.set_ylabel('Loss')\n    ax1.grid(True, alpha=0.3)\n    \n    # Annotate key points\n    ax1.annotate('üìâ Rapid learning', xy=(20, loss_curve[20]), \n                xytext=(30, loss_curve[20] + 0.5), fontsize=11,\n                arrowprops=dict(arrowstyle='->', color='red'))\n    ax1.annotate('üéØ Converged!', xy=(80, loss_curve[80]), \n                xytext=(60, loss_curve[80] + 0.3), fontsize=11,\n                arrowprops=dict(arrowstyle='->', color='green'))\n    \n    # Sample quality timeline\n    ax2.set_xlim(0, 100)\n    ax2.set_ylim(-0.5, len(sample_quality) - 0.5)\n    ax2.set_title('üé≠ Signal 2: Generated Text Gets Good', fontweight='bold')\n    ax2.set_xlabel('Training Step')\n    ax2.set_ylabel('Sample Quality')\n    \n    for i, (step, sample) in enumerate(sample_quality):\n        color = 'red' if i < 2 else 'orange' if i < 4 else 'green'\n        ax2.scatter(step, i, s=100, color=color, alpha=0.7)\n        ax2.text(step + 2, i, f'\"{sample}\"', fontsize=10, \n                verticalalignment='center', color=color, fontweight='bold')\n    \n    ax2.set_yticks(range(len(sample_quality)))\n    ax2.set_yticklabels(['Terrible', 'Bad', 'Getting there', 'Good', 'Great!', 'Perfect'])\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"üîç HOW TO RECOGNIZE EACH SIGNAL:\")\n    print(\"-\" * 35)\n    print(\"1. üìâ LOSS STOPS IMPROVING:\")\n    print(\"   ‚Ä¢ Loss curve flattens out\")\n    print(\"   ‚Ä¢ No significant decrease for many steps\")\n    print(\"   ‚Ä¢ Loss might slightly oscillate but doesn't go down\")\n    print()\n    print(\"2. üé≠ GENERATED TEXT LOOKS GOOD:\")\n    print(\"   ‚Ä¢ Output makes grammatical sense\")\n    print(\"   ‚Ä¢ Follows patterns from training data\")\n    print(\"   ‚Ä¢ No more random gibberish\")\n    print()\n    print(\"3. üöß VALIDATION LOSS LEVELS OFF:\")\n    print(\"   ‚Ä¢ Test on unseen data\")\n    print(\"   ‚Ä¢ If validation loss stops decreasing: you're done!\")\n    print(\"   ‚Ä¢ If validation loss increases: you're overfitting!\")\n    \n    print(\"\\n‚ö†Ô∏è  IMPORTANT WARNING:\")\n    print(\"Don't train too long! Overtraining makes models worse.\")\n    print(\"It's like studying for a test by memorizing instead of understanding.\")\n\nshow_training_completion_signals()\n\ndef demonstrate_simple_stopping_criteria():\n    \"\"\"Show simple rules for when to stop training.\"\"\"\n    \n    print(\"\\nüõë SIMPLE STOPPING RULES\")\n    print(\"=\" * 25)\n    \n    print(\"For beginners, use these simple rules:\")\n    print()\n    print(\"1. üïê TIME RULE:\")\n    print(\"   ‚Ä¢ Stop after X hours of training\")\n    print(\"   ‚Ä¢ Good for experiments and learning\")\n    print()\n    print(\"2. üìâ LOSS RULE:\")\n    print(\"   ‚Ä¢ Stop when loss < some threshold (like 2.0)\")\n    print(\"   ‚Ä¢ Stop when loss stops decreasing for 50+ steps\")\n    print()\n    print(\"3. üéØ GENERATION RULE:\")\n    print(\"   ‚Ä¢ Stop when generated samples look reasonable\")\n    print(\"   ‚Ä¢ Test generation every 100 steps\")\n    print()\n    print(\"4. üíæ PATIENCE RULE:\")\n    print(\"   ‚Ä¢ Save model every time validation improves\")\n    print(\"   ‚Ä¢ Stop if no improvement for 10 saves\")\n    print(\"   ‚Ä¢ Use the best saved model\")\n    \n    print(\"\\nüí° PRO TIP:\")\n    print(\"Always save your model regularly!\")\n    print(\"You never know when training might crash or when you've hit the sweet spot.\")\n\ndemonstrate_simple_stopping_criteria()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: You Now Know How Transformers Learn! üéì\n\n**Congratulations!** You've just learned the core of how ALL language models work:\n\n**The Magic Recipe:**\n1. **Show the model some text** (\"The cat sat on the\")\n2. **Ask it to predict the next word** (\"mat\")\n3. **Tell it how wrong it was** (loss = wrongness)\n4. **Nudge it to be slightly better** (adjust weights)\n5. **Repeat millions of times** (practice makes perfect!)\n\nThat's it! This simple process creates ChatGPT, GPT-4, and every transformer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def summarize_what_we_learned():\n    \"\"\"Recap the key insights about transformer training.\"\"\"\n    \n    print(\"üéì WHAT YOU NOW UNDERSTAND\")\n    print(\"=\" * 30)\n    \n    key_insights = [\n        (\"üéØ\", \"Training = next word prediction\", \"The model learns by guessing the next word millions of times\"),\n        (\"üìâ\", \"Loss = wrongness level\", \"Lower loss means better predictions (less confused)\"),\n        (\"üîÑ\", \"Learning = small adjustments\", \"Each mistake teaches the model to be slightly better\"),\n        (\"‚è∞\", \"Time scale matters\", \"Real models train for days/weeks on massive datasets\"),\n        (\"üé≠\", \"Generated text reveals progress\", \"Watch outputs to see if the model is learning language\")\n    ]\n    \n    for emoji, concept, explanation in key_insights:\n        print(f\"{emoji} {concept:25} ‚Üí {explanation}\")\n    \n    print(\"\\nüåü THE BIG PICTURE\")\n    print(\"=\" * 20)\n    print(\"This simple process creates the most advanced AI:\")\n    print(\"‚Ä¢ ChatGPT: Trained on internet text for months\")\n    print(\"‚Ä¢ GPT-4: 1.76 trillion parameters, massive compute\")\n    print(\"‚Ä¢ Claude: Advanced training with human feedback\")\n    print(\"‚Ä¢ Your model: Same principles, smaller scale!\")\n    \n    print(\"\\nüöÄ WHAT'S NEXT?\")\n    print(\"=\" * 15)\n    print(\"Now that you understand training, you can:\")\n    print(\"‚Ä¢ Scale up: Use bigger models and more data\")\n    print(\"‚Ä¢ Experiment: Try different architectures\")\n    print(\"‚Ä¢ Optimize: Improve training efficiency\")\n    print(\"‚Ä¢ Deploy: Use your trained model for tasks\")\n    \n    print(\"\\nüéâ YOU'RE NOW A TRANSFORMER EXPERT!\")\n    print(\"You understand the core of how language AI works.\")\n    print(\"The rest is just engineering and scaling!\")\n\nsummarize_what_we_learned()\n\n# Show one final learning visualization\ndef final_learning_demonstration():\n    \"\"\"One last simple demo to cement understanding.\"\"\"\n    \n    print(\"\\nüî¨ FINAL DEMO: LEARNING IN ACTION\")\n    print(\"=\" * 35)\n    \n    # Simple demo of loss going down\n    steps = [1, 10, 50, 100, 500]\n    losses = [10.5, 8.2, 4.1, 2.8, 1.9]\n    examples = [\n        \"ajsdh qwehjk asdf\",\n        \"The dog cat\", \n        \"The cat sat\",\n        \"The cat sat on the\",\n        \"The cat sat on the mat\"\n    ]\n    \n    print(\"Watch how loss drops as the model gets smarter:\")\n    print()\n    print(\"Step | Loss | Sample Output        | Model's Thoughts\")\n    print(\"-\" * 60)\n    \n    thoughts = [\n        \"üòµ What even is language??\",\n        \"ü§î These are probably words...\",\n        \"üí° I see patterns emerging!\",\n        \"üß† I'm understanding structure!\",\n        \"üéØ I've got this!\"\n    ]\n    \n    for step, loss, example, thought in zip(steps, losses, examples, thoughts):\n        print(f\"{step:4} | {loss:4.1f} | {example:20} | {thought}\")\n    \n    print(\"\\n‚ú® That's machine learning in action!\")\n    print(\"From confusion to competence through practice.\")\n\nfinal_learning_demonstration()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overfitting():\n",
    "    \"\"\"Show how overfitting manifests in language models.\"\"\"\n",
    "    \n",
    "    print(\"Overfitting in Language Models\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a tiny dataset to encourage overfitting\n",
    "    tiny_text = \"The cat sat on the mat. The cat sat on the hat.\"\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Encode the text\n",
    "    tokens = tokenizer.encode(tiny_text, add_special_tokens=False)\n",
    "    print(f\"Training text: '{tiny_text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SimpleTextDataset(tiny_text, tokenizer, block_size=8)\n",
    "    dataloader = create_dataloader(dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nTraining samples:\")\n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if i < 3:  # Show first 3 batches\n",
    "            print(f\"Batch {i}: input shape {input_ids.shape}\")\n",
    "            for j in range(input_ids.shape[0]):\n",
    "                input_text = tokenizer.decode(input_ids[j].tolist(), skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(target_ids[j].tolist(), skip_special_tokens=True)\n",
    "                print(f\"  Input:  '{input_text}'\")\n",
    "                print(f\"  Target: '{target_text}'\")\n",
    "    \n",
    "    print(\"\\nSigns of Overfitting:\")\n",
    "    print(\"‚Ä¢ Training loss goes to zero but validation loss increases\")\n",
    "    print(\"‚Ä¢ Model memorizes training data exactly\")\n",
    "    print(\"‚Ä¢ Poor generalization to new text\")\n",
    "    print(\"‚Ä¢ Generated text becomes repetitive or nonsensical\")\n",
    "    \n",
    "    return dataset, dataloader\n",
    "\n",
    "tiny_dataset, tiny_dataloader = demonstrate_overfitting()\n",
    "\n",
    "def demonstrate_regularization_techniques():\n",
    "    \"\"\"Show different regularization methods.\"\"\"\n",
    "    \n",
    "    print(\"\\nRegularization Techniques for Transformers\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    techniques = {\n",
    "        \"Dropout\": {\n",
    "            \"description\": \"Randomly zero out neurons during training\",\n",
    "            \"implementation\": \"nn.Dropout(p=0.1) in attention and FFN\",\n",
    "            \"effect\": \"Prevents co-adaptation of neurons\"\n",
    "        },\n",
    "        \"Weight Decay\": {\n",
    "            \"description\": \"Add L2 penalty to weights\",\n",
    "            \"implementation\": \"weight_decay=0.01 in optimizer\",\n",
    "            \"effect\": \"Keeps weights small, improves generalization\"\n",
    "        },\n",
    "        \"Gradient Clipping\": {\n",
    "            \"description\": \"Limit gradient magnitude\",\n",
    "            \"implementation\": \"clip_grad_norm_(params, max_norm=1.0)\",\n",
    "            \"effect\": \"Prevents exploding gradients\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"description\": \"Stop when validation loss stops improving\",\n",
    "            \"implementation\": \"Monitor validation loss, save best model\",\n",
    "            \"effect\": \"Prevents overfitting to training data\"\n",
    "        },\n",
    "        \"Data Augmentation\": {\n",
    "            \"description\": \"Increase effective dataset size\",\n",
    "            \"implementation\": \"Paraphrasing, back-translation, masking\",\n",
    "            \"effect\": \"More diverse training examples\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, info in techniques.items():\n",
    "        print(f\"\\n{technique}:\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Implementation: {info['implementation']}\")\n",
    "        print(f\"  Effect: {info['effect']}\")\n",
    "    \n",
    "    # Visualize dropout effect\n",
    "    print(\"\\nDropout Visualization:\")\n",
    "    \n",
    "    # Simulate dropout on a tensor\n",
    "    x = torch.ones(4, 8)  # 4x8 tensor of ones\n",
    "    dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(x, cmap='Blues')\n",
    "    axes[0].set_title('Original Activations')\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Example')\n",
    "    \n",
    "    # With dropout (training mode)\n",
    "    dropout.train()\n",
    "    x_dropout = dropout(x)\n",
    "    axes[1].imshow(x_dropout, cmap='Blues')\n",
    "    axes[1].set_title('With Dropout (Training)')\n",
    "    axes[1].set_xlabel('Feature')\n",
    "    \n",
    "    # Without dropout (eval mode)\n",
    "    dropout.eval()\n",
    "    x_eval = dropout(x)\n",
    "    axes[2].imshow(x_eval, cmap='Blues')\n",
    "    axes[2].set_title('Without Dropout (Evaluation)')\n",
    "    axes[2].set_xlabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice how dropout randomly zeros neurons during training but not evaluation!\")\n",
    "\n",
    "demonstrate_regularization_techniques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_training_monitoring():\n",
    "    \"\"\"Demonstrate comprehensive training monitoring.\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Training Monitoring\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a slightly larger model for realistic monitoring\n",
    "    config = create_model_config(\"small\")\n",
    "    config[\"vocab_size\"] = 200\n",
    "    config[\"n_layers\"] = 3  # Smaller for faster training\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    # More comprehensive training text\n",
    "    training_text = \"\"\"\n",
    "    The transformer architecture revolutionized natural language processing.\n",
    "    Attention mechanisms allow models to focus on relevant parts of the input.\n",
    "    Large language models demonstrate emergent capabilities at scale.\n",
    "    Training requires careful optimization and regularization techniques.\n",
    "    Deep learning continues to advance the field of artificial intelligence.\n",
    "    Neural networks learn complex patterns from vast amounts of data.\n",
    "    Machine learning algorithms can generalize to unseen examples.\n",
    "    The future of AI depends on responsible development and deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    dataset = SimpleTextDataset(training_text, tokenizer, block_size=24)\n",
    "    dataloader = create_dataloader(dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer with proper settings\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01, betas=(0.9, 0.95))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    scheduler = CosineWarmupScheduler(optimizer, warmup_steps=10, max_steps=total_steps, base_lr=3e-4)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"Dataset: {len(dataset)} samples\")\n",
    "    print(f\"Total training steps: {total_steps}\")\n",
    "    \n",
    "    # Training loop with comprehensive monitoring\n",
    "    metrics = {\n",
    "        'steps': [],\n",
    "        'losses': [],\n",
    "        'learning_rates': [],\n",
    "        'grad_norms': [],\n",
    "        'weight_norms': [],\n",
    "        'samples': []\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        print(f\"\\nEpoch {epoch + 1}/3\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Training step\n",
    "            model.train()\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(input_ids)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Monitor gradients\n",
    "            total_grad_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "            total_grad_norm = total_grad_norm ** 0.5\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            current_lr = scheduler.step()\n",
    "            \n",
    "            # Monitor weights\n",
    "            total_weight_norm = 0\n",
    "            for p in model.parameters():\n",
    "                total_weight_norm += p.data.norm(2).item() ** 2\n",
    "            total_weight_norm = total_weight_norm ** 0.5\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics['steps'].append(step)\n",
    "            metrics['losses'].append(loss.item())\n",
    "            metrics['learning_rates'].append(current_lr)\n",
    "            metrics['grad_norms'].append(total_grad_norm)\n",
    "            metrics['weight_norms'].append(total_weight_norm)\n",
    "            \n",
    "            # Generate sample every 10 steps\n",
    "            if step % 10 == 0:\n",
    "                sample = generate_sample(model, tokenizer, \"The\", max_length=10)\n",
    "                metrics['samples'].append((step, sample))\n",
    "                print(f\"Step {step:3d}: Loss={loss.item():.3f}, LR={current_lr:.2e}, Sample='{sample}'\")\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run comprehensive training\n",
    "metrics = comprehensive_training_monitoring()\n",
    "\n",
    "# Plot comprehensive metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(metrics['steps'], metrics['losses'], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[0, 1].plot(metrics['steps'], metrics['learning_rates'], 'r-', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Learning Rate')\n",
    "axes[0, 1].set_title('Learning Rate Schedule')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norms\n",
    "axes[1, 0].plot(metrics['steps'], metrics['grad_norms'], 'g-', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title('Gradient Norms')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight norms\n",
    "axes[1, 1].plot(metrics['steps'], metrics['weight_norms'], 'm-', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Weight Norm')\n",
    "axes[1, 1].set_title('Weight Norms')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Quality Indicators:\")\n",
    "print(f\"‚Ä¢ Loss decreased from {metrics['losses'][0]:.3f} to {metrics['losses'][-1]:.3f}\")\n",
    "print(f\"‚Ä¢ Gradient norms: {np.mean(metrics['grad_norms']):.3f} (should be stable, not too large)\")\n",
    "print(f\"‚Ä¢ Weight norms growing: {metrics['weight_norms'][-1] > metrics['weight_norms'][0]} (expected during training)\")\n",
    "print(f\"‚Ä¢ Learning rate properly scheduled: {metrics['learning_rates'][0]:.2e} ‚Üí {metrics['learning_rates'][-1]:.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}