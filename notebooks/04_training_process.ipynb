{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers: From Random Weights to Language Understanding\n",
    "\n",
    "How does a transformer learn to understand and generate language? In this notebook, we'll explore the training process step by step, from loss functions to optimization strategies.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Language Modeling Objective** - Next token prediction and cross-entropy loss\n",
    "2. **Training Loop** - Forward pass, backward pass, optimization\n",
    "3. **Monitoring Progress** - Loss curves, perplexity, and sample generation\n",
    "4. **Training Techniques** - Learning rates, warmup, gradient clipping\n",
    "5. **Overfitting & Regularization** - Dropout, weight decay, early stopping\n",
    "6. **Scaling Laws** - How performance scales with data and model size\n",
    "\n",
    "Let's train a transformer from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Modeling Objective\n",
    "\n",
    "Language models are trained to predict the next token given previous tokens. This simple objective forces the model to learn grammar, semantics, and even reasoning!\n",
    "\n",
    "Given a sequence $[x_1, x_2, ..., x_n]$, we want to maximize:\n",
    "$$P(x_2|x_1) \\cdot P(x_3|x_1, x_2) \\cdot ... \\cdot P(x_n|x_1, ..., x_{n-1})$$\n",
    "\n",
    "In practice, we use cross-entropy loss for each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_language_modeling_objective():\n",
    "    \"\"\"Show how the language modeling objective works.\"\"\"\n",
    "    \n",
    "    # Example sentence: \"The cat sat\"\n",
    "    sentence = \"The cat sat\"\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    print(\"Language Modeling: Predicting Next Token\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    print(\"Training examples created from 'The cat sat':\")\n",
    "    print()\n",
    "    print(\"Input → Target\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Show how we create input-target pairs\n",
    "    for i in range(len(tokens)):\n",
    "        input_seq = tokens[:i+1] if i > 0 else [\"<START>\"]\n",
    "        target = tokens[i] if i < len(tokens) else \"<END>\"\n",
    "        \n",
    "        input_str = \" \".join(input_seq)\n",
    "        print(f\"{input_str:<12} → {target}\")\n",
    "    \n",
    "    # Demonstrate with actual tensors\n",
    "    vocab = {\"<PAD>\": 0, \"<START>\": 1, \"The\": 2, \"cat\": 3, \"sat\": 4, \"<END>\": 5}\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # Convert to IDs\n",
    "    sequence = [vocab[\"<START>\"], vocab[\"The\"], vocab[\"cat\"], vocab[\"sat\"], vocab[\"<END>\"]]\n",
    "    \n",
    "    print(f\"\\nTokenized sequence: {sequence}\")\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    \n",
    "    # Create input and target tensors\n",
    "    input_ids = torch.tensor(sequence[:-1])  # All except last\n",
    "    target_ids = torch.tensor(sequence[1:])  # All except first\n",
    "    \n",
    "    print(f\"\\nInput IDs:  {input_ids.tolist()}\")\n",
    "    print(f\"Target IDs: {target_ids.tolist()}\")\n",
    "    \n",
    "    # Simulate model predictions (random for demonstration)\n",
    "    seq_len = len(input_ids)\n",
    "    fake_logits = torch.randn(seq_len, vocab_size)  # [seq_len, vocab_size]\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(fake_logits, target_ids)\n",
    "    \n",
    "    print(f\"\\nModel logits shape: {fake_logits.shape}\")\n",
    "    print(f\"Cross-entropy loss: {loss.item():.3f}\")\n",
    "    \n",
    "    # Show probabilities for first prediction\n",
    "    first_probs = F.softmax(fake_logits[0], dim=0)\n",
    "    print(f\"\\nPredicted probabilities for first position:\")\n",
    "    for word, idx in vocab.items():\n",
    "        prob = first_probs[idx].item()\n",
    "        marker = \" ← TARGET\" if idx == target_ids[0] else \"\"\n",
    "        print(f\"  {word:<8}: {prob:.3f}{marker}\")\n",
    "    \n",
    "    return input_ids, target_ids, fake_logits, loss\n",
    "\n",
    "input_ids, target_ids, logits, loss = demonstrate_language_modeling_objective()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Each position predicts the next token\")\n",
    "print(\"• Loss is calculated for all positions simultaneously\")\n",
    "print(\"• Model learns patterns by minimizing prediction errors\")\n",
    "print(\"• Same architecture can learn grammar, facts, reasoning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Loop Implementation\n",
    "\n",
    "Let's implement a complete training loop and see how a transformer learns step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "from src.data.dataset import SimpleTextDataset, create_dataloader\n",
    "\n",
    "class TrainingTracker:\n",
    "    \"\"\"Track training metrics and visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.learning_rates = []\n",
    "        self.steps = []\n",
    "        self.samples = []\n",
    "        \n",
    "    def log(self, step: int, loss: float, lr: float, sample: str = None):\n",
    "        self.steps.append(step)\n",
    "        self.losses.append(loss)\n",
    "        self.learning_rates.append(lr)\n",
    "        if sample:\n",
    "            self.samples.append((step, sample))\n",
    "    \n",
    "    def plot_progress(self):\n",
    "        \"\"\"Plot training progress.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss curve\n",
    "        axes[0].plot(self.steps, self.losses, 'b-', alpha=0.7)\n",
    "        axes[0].set_xlabel('Training Step')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training Loss')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[1].plot(self.steps, self.learning_rates, 'r-', alpha=0.7)\n",
    "        axes[1].set_xlabel('Training Step')\n",
    "        axes[1].set_ylabel('Learning Rate')\n",
    "        axes[1].set_title('Learning Rate Schedule')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def train_step(model, batch, optimizer, criterion, device):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    input_ids, target_ids = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    target_ids = target_ids.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(input_ids)\n",
    "    \n",
    "    # Calculate loss\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    logits_flat = logits.view(-1, vocab_size)\n",
    "    targets_flat = target_ids.view(-1)\n",
    "    \n",
    "    loss = criterion(logits_flat, targets_flat)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping (prevent exploding gradients)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt=\"The\", max_length=20, temperature=0.8):\n",
    "    \"\"\"Generate a sample during training.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode prompt\n",
    "        tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0)\n",
    "        \n",
    "        # Generate\n",
    "        generated = model.generate(input_ids, max_new_tokens=max_length, temperature=temperature)\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Create training setup\n",
    "def setup_training():\n",
    "    \"\"\"Setup model, data, and optimizer for training.\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    config = create_model_config(\"tiny\")\n",
    "    config[\"vocab_size\"] = 200  # Reduce for faster training\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    # Data\n",
    "    text = \"\"\"The quick brown fox jumps over the lazy dog. The cat sat on the mat. \n",
    "    A bird in the hand is worth two in the bush. Time flies like an arrow. \n",
    "    The early bird catches the worm. Actions speak louder than words.\n",
    "    The pen is mightier than the sword. All that glitters is not gold.\n",
    "    Rome was not built in a day. The grass is always greener on the other side.\"\"\"\n",
    "    \n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    dataset = SimpleTextDataset(text, tokenizer, block_size=32)\n",
    "    dataloader = create_dataloader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return model, dataloader, optimizer, criterion, tokenizer, device\n",
    "\n",
    "# Setup training\n",
    "model, dataloader, optimizer, criterion, tokenizer, device = setup_training()\n",
    "\n",
    "# Test initial generation (before training)\n",
    "print(\"\\nBefore training:\")\n",
    "initial_sample = generate_sample(model, tokenizer, \"The cat\")\n",
    "print(f\"Generated: '{initial_sample}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model\n",
    "\n",
    "Now let's train the model and watch it learn! We'll monitor the loss and see how text generation improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, tokenizer, device, num_epochs=5):\n",
    "    \"\"\"Train the model and track progress.\"\"\"\n",
    "    \n",
    "    tracker = TrainingTracker()\n",
    "    step = 0\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Progress bar for epoch\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            # Training step\n",
    "            loss = train_step(model, batch, optimizer, criterion, device)\n",
    "            epoch_losses.append(loss)\n",
    "            \n",
    "            # Log progress\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            tracker.log(step, loss, current_lr)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss:.3f}', 'lr': f'{current_lr:.2e}'})\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.3f}\")\n",
    "        \n",
    "        # Generate sample\n",
    "        sample = generate_sample(model, tokenizer, \"The cat\", max_length=15)\n",
    "        print(f\"Sample: '{sample}'\")\n",
    "        tracker.log(step, avg_loss, current_lr, sample)\n",
    "        print()\n",
    "    \n",
    "    return tracker\n",
    "\n",
    "# Train the model\n",
    "tracker = train_model(model, dataloader, optimizer, criterion, tokenizer, device, num_epochs=5)\n",
    "\n",
    "# Plot training progress\n",
    "print(\"Training completed! Here's the progress:\")\n",
    "tracker.plot_progress()\n",
    "\n",
    "# Test final generation\n",
    "print(\"\\nAfter training:\")\n",
    "final_sample = generate_sample(model, tokenizer, \"The cat\", max_length=20)\n",
    "print(f\"Generated: '{final_sample}'\")\n",
    "\n",
    "# Compare before and after\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"Before:  '{initial_sample}'\")\n",
    "print(f\"After:   '{final_sample}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Loss and Perplexity\n",
    "\n",
    "Loss tells us how well the model is learning, but perplexity is more interpretable. Perplexity roughly corresponds to \"how many choices the model thinks it has at each step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_loss_and_perplexity(tracker):\n",
    "    \"\"\"Analyze training loss and compute perplexity.\"\"\"\n",
    "    \n",
    "    # Convert loss to perplexity\n",
    "    perplexities = [math.exp(loss) for loss in tracker.losses]\n",
    "    \n",
    "    print(\"Loss and Perplexity Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Show progression\n",
    "    print(\"Training progression:\")\n",
    "    print(\"Step | Loss  | Perplexity | Interpretation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    checkpoints = [0, len(tracker.losses)//4, len(tracker.losses)//2, \n",
    "                   3*len(tracker.losses)//4, len(tracker.losses)-1]\n",
    "    \n",
    "    for i in checkpoints:\n",
    "        loss = tracker.losses[i]\n",
    "        perplexity = perplexities[i]\n",
    "        \n",
    "        if perplexity > 100:\n",
    "            interpretation = \"Very confused\"\n",
    "        elif perplexity > 50:\n",
    "            interpretation = \"Quite confused\"\n",
    "        elif perplexity > 20:\n",
    "            interpretation = \"Somewhat confused\"\n",
    "        elif perplexity > 10:\n",
    "            interpretation = \"Getting better\"\n",
    "        else:\n",
    "            interpretation = \"Fairly confident\"\n",
    "        \n",
    "        print(f\"{tracker.steps[i]:4} | {loss:5.2f} | {perplexity:10.1f} | {interpretation}\")\n",
    "    \n",
    "    # Plot loss and perplexity\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(tracker.steps, tracker.losses, 'b-', alpha=0.7)\n",
    "    axes[0].set_xlabel('Training Step')\n",
    "    axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perplexity\n",
    "    axes[1].plot(tracker.steps, perplexities, 'r-', alpha=0.7)\n",
    "    axes[1].set_xlabel('Training Step')\n",
    "    axes[1].set_ylabel('Perplexity')\n",
    "    axes[1].set_title('Perplexity (exp(loss))')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal metrics:\")\n",
    "    print(f\"Initial loss: {tracker.losses[0]:.3f}, perplexity: {perplexities[0]:.1f}\")\n",
    "    print(f\"Final loss: {tracker.losses[-1]:.3f}, perplexity: {perplexities[-1]:.1f}\")\n",
    "    print(f\"Improvement: {tracker.losses[0] - tracker.losses[-1]:.3f} loss units\")\n",
    "    \n",
    "    print(\"\\nPerplexity Interpretation:\")\n",
    "    print(\"• Perplexity ≈ 'How many choices does the model think it has?'\")\n",
    "    print(\"• Random guessing: perplexity = vocabulary size\")\n",
    "    print(\"• Perfect prediction: perplexity = 1\")\n",
    "    print(\"• Good language models: perplexity < 50\")\n",
    "    print(\"• Great language models: perplexity < 20\")\n",
    "\n",
    "analyze_loss_and_perplexity(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Rate Scheduling\n",
    "\n",
    "Learning rate is crucial for training. Let's explore different learning rate schedules and see their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_learning_rate_schedules():\n",
    "    \"\"\"Show different learning rate schedules.\"\"\"\n",
    "    \n",
    "    max_steps = 100\n",
    "    base_lr = 1e-3\n",
    "    \n",
    "    steps = np.arange(max_steps)\n",
    "    \n",
    "    # Different schedules\n",
    "    schedules = {}\n",
    "    \n",
    "    # Constant\n",
    "    schedules['Constant'] = np.full(max_steps, base_lr)\n",
    "    \n",
    "    # Linear decay\n",
    "    schedules['Linear Decay'] = base_lr * (1 - steps / max_steps)\n",
    "    \n",
    "    # Cosine decay\n",
    "    schedules['Cosine Decay'] = base_lr * 0.5 * (1 + np.cos(np.pi * steps / max_steps))\n",
    "    \n",
    "    # Warmup + cosine\n",
    "    warmup_steps = 10\n",
    "    warmup_cosine = np.zeros(max_steps)\n",
    "    for i in range(max_steps):\n",
    "        if i < warmup_steps:\n",
    "            warmup_cosine[i] = base_lr * i / warmup_steps\n",
    "        else:\n",
    "            progress = (i - warmup_steps) / (max_steps - warmup_steps)\n",
    "            warmup_cosine[i] = base_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    schedules['Warmup + Cosine'] = warmup_cosine\n",
    "    \n",
    "    # Exponential decay\n",
    "    gamma = 0.95\n",
    "    schedules['Exponential'] = base_lr * (gamma ** (steps / 10))\n",
    "    \n",
    "    # Plot schedules\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for name, schedule in schedules.items():\n",
    "        plt.plot(steps, schedule, label=name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedules')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Learning Rate Schedule Guidelines:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"• Constant: Simple but may not converge optimally\")\n",
    "    print(\"• Linear Decay: Smooth reduction, good baseline\")\n",
    "    print(\"• Cosine Decay: Popular for transformers, smooth curves\")\n",
    "    print(\"• Warmup + Cosine: Stabilizes early training, then smooth decay\")\n",
    "    print(\"• Exponential: Fast initial decay, then slow\")\n",
    "    print()\n",
    "    print(\"Best Practices:\")\n",
    "    print(\"• Use warmup for large models (prevents early instability)\")\n",
    "    print(\"• Cosine decay works well for most transformer training\")\n",
    "    print(\"• End with small LR for fine-tuning convergence\")\n",
    "    print(\"• Monitor loss curves to adjust schedule\")\n",
    "\n",
    "demonstrate_learning_rate_schedules()\n",
    "\n",
    "# Implement cosine warmup schedule\n",
    "class CosineWarmupScheduler:\n",
    "    \"\"\"Cosine learning rate schedule with warmup.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps: int, max_steps: int, base_lr: float, min_lr: float = 0):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_steps = max_steps\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.step_count < self.warmup_steps:\n",
    "            # Warmup phase\n",
    "            lr = self.base_lr * self.step_count / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine decay phase\n",
    "            progress = (self.step_count - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
    "            progress = min(progress, 1.0)\n",
    "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "\n",
    "print(\"\\n✅ Learning rate scheduler implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularization Techniques\n",
    "\n",
    "Transformers can easily overfit, especially on small datasets. Let's explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overfitting():\n",
    "    \"\"\"Show how overfitting manifests in language models.\"\"\"\n",
    "    \n",
    "    print(\"Overfitting in Language Models\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create a tiny dataset to encourage overfitting\n",
    "    tiny_text = \"The cat sat on the mat. The cat sat on the hat.\"\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Encode the text\n",
    "    tokens = tokenizer.encode(tiny_text, add_special_tokens=False)\n",
    "    print(f\"Training text: '{tiny_text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SimpleTextDataset(tiny_text, tokenizer, block_size=8)\n",
    "    dataloader = create_dataloader(dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nTraining samples:\")\n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if i < 3:  # Show first 3 batches\n",
    "            print(f\"Batch {i}: input shape {input_ids.shape}\")\n",
    "            for j in range(input_ids.shape[0]):\n",
    "                input_text = tokenizer.decode(input_ids[j].tolist(), skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(target_ids[j].tolist(), skip_special_tokens=True)\n",
    "                print(f\"  Input:  '{input_text}'\")\n",
    "                print(f\"  Target: '{target_text}'\")\n",
    "    \n",
    "    print(\"\\nSigns of Overfitting:\")\n",
    "    print(\"• Training loss goes to zero but validation loss increases\")\n",
    "    print(\"• Model memorizes training data exactly\")\n",
    "    print(\"• Poor generalization to new text\")\n",
    "    print(\"• Generated text becomes repetitive or nonsensical\")\n",
    "    \n",
    "    return dataset, dataloader\n",
    "\n",
    "tiny_dataset, tiny_dataloader = demonstrate_overfitting()\n",
    "\n",
    "def demonstrate_regularization_techniques():\n",
    "    \"\"\"Show different regularization methods.\"\"\"\n",
    "    \n",
    "    print(\"\\nRegularization Techniques for Transformers\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    techniques = {\n",
    "        \"Dropout\": {\n",
    "            \"description\": \"Randomly zero out neurons during training\",\n",
    "            \"implementation\": \"nn.Dropout(p=0.1) in attention and FFN\",\n",
    "            \"effect\": \"Prevents co-adaptation of neurons\"\n",
    "        },\n",
    "        \"Weight Decay\": {\n",
    "            \"description\": \"Add L2 penalty to weights\",\n",
    "            \"implementation\": \"weight_decay=0.01 in optimizer\",\n",
    "            \"effect\": \"Keeps weights small, improves generalization\"\n",
    "        },\n",
    "        \"Gradient Clipping\": {\n",
    "            \"description\": \"Limit gradient magnitude\",\n",
    "            \"implementation\": \"clip_grad_norm_(params, max_norm=1.0)\",\n",
    "            \"effect\": \"Prevents exploding gradients\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"description\": \"Stop when validation loss stops improving\",\n",
    "            \"implementation\": \"Monitor validation loss, save best model\",\n",
    "            \"effect\": \"Prevents overfitting to training data\"\n",
    "        },\n",
    "        \"Data Augmentation\": {\n",
    "            \"description\": \"Increase effective dataset size\",\n",
    "            \"implementation\": \"Paraphrasing, back-translation, masking\",\n",
    "            \"effect\": \"More diverse training examples\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, info in techniques.items():\n",
    "        print(f\"\\n{technique}:\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Implementation: {info['implementation']}\")\n",
    "        print(f\"  Effect: {info['effect']}\")\n",
    "    \n",
    "    # Visualize dropout effect\n",
    "    print(\"\\nDropout Visualization:\")\n",
    "    \n",
    "    # Simulate dropout on a tensor\n",
    "    x = torch.ones(4, 8)  # 4x8 tensor of ones\n",
    "    dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(x, cmap='Blues')\n",
    "    axes[0].set_title('Original Activations')\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Example')\n",
    "    \n",
    "    # With dropout (training mode)\n",
    "    dropout.train()\n",
    "    x_dropout = dropout(x)\n",
    "    axes[1].imshow(x_dropout, cmap='Blues')\n",
    "    axes[1].set_title('With Dropout (Training)')\n",
    "    axes[1].set_xlabel('Feature')\n",
    "    \n",
    "    # Without dropout (eval mode)\n",
    "    dropout.eval()\n",
    "    x_eval = dropout(x)\n",
    "    axes[2].imshow(x_eval, cmap='Blues')\n",
    "    axes[2].set_title('Without Dropout (Evaluation)')\n",
    "    axes[2].set_xlabel('Feature')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice how dropout randomly zeros neurons during training but not evaluation!\")\n",
    "\n",
    "demonstrate_regularization_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitoring Training Progress\n",
    "\n",
    "Good monitoring is essential for successful training. Let's implement comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_training_monitoring():\n",
    "    \"\"\"Demonstrate comprehensive training monitoring.\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Training Monitoring\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a slightly larger model for realistic monitoring\n",
    "    config = create_model_config(\"small\")\n",
    "    config[\"vocab_size\"] = 200\n",
    "    config[\"n_layers\"] = 3  # Smaller for faster training\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    # More comprehensive training text\n",
    "    training_text = \"\"\"\n",
    "    The transformer architecture revolutionized natural language processing.\n",
    "    Attention mechanisms allow models to focus on relevant parts of the input.\n",
    "    Large language models demonstrate emergent capabilities at scale.\n",
    "    Training requires careful optimization and regularization techniques.\n",
    "    Deep learning continues to advance the field of artificial intelligence.\n",
    "    Neural networks learn complex patterns from vast amounts of data.\n",
    "    Machine learning algorithms can generalize to unseen examples.\n",
    "    The future of AI depends on responsible development and deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    dataset = SimpleTextDataset(training_text, tokenizer, block_size=24)\n",
    "    dataloader = create_dataloader(dataset, batch_size=3, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer with proper settings\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01, betas=(0.9, 0.95))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    scheduler = CosineWarmupScheduler(optimizer, warmup_steps=10, max_steps=total_steps, base_lr=3e-4)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"Dataset: {len(dataset)} samples\")\n",
    "    print(f\"Total training steps: {total_steps}\")\n",
    "    \n",
    "    # Training loop with comprehensive monitoring\n",
    "    metrics = {\n",
    "        'steps': [],\n",
    "        'losses': [],\n",
    "        'learning_rates': [],\n",
    "        'grad_norms': [],\n",
    "        'weight_norms': [],\n",
    "        'samples': []\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        print(f\"\\nEpoch {epoch + 1}/3\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Training step\n",
    "            model.train()\n",
    "            input_ids, target_ids = batch\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(input_ids)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Monitor gradients\n",
    "            total_grad_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "            total_grad_norm = total_grad_norm ** 0.5\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            current_lr = scheduler.step()\n",
    "            \n",
    "            # Monitor weights\n",
    "            total_weight_norm = 0\n",
    "            for p in model.parameters():\n",
    "                total_weight_norm += p.data.norm(2).item() ** 2\n",
    "            total_weight_norm = total_weight_norm ** 0.5\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics['steps'].append(step)\n",
    "            metrics['losses'].append(loss.item())\n",
    "            metrics['learning_rates'].append(current_lr)\n",
    "            metrics['grad_norms'].append(total_grad_norm)\n",
    "            metrics['weight_norms'].append(total_weight_norm)\n",
    "            \n",
    "            # Generate sample every 10 steps\n",
    "            if step % 10 == 0:\n",
    "                sample = generate_sample(model, tokenizer, \"The\", max_length=10)\n",
    "                metrics['samples'].append((step, sample))\n",
    "                print(f\"Step {step:3d}: Loss={loss.item():.3f}, LR={current_lr:.2e}, Sample='{sample}'\")\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run comprehensive training\n",
    "metrics = comprehensive_training_monitoring()\n",
    "\n",
    "# Plot comprehensive metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(metrics['steps'], metrics['losses'], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[0, 1].plot(metrics['steps'], metrics['learning_rates'], 'r-', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Learning Rate')\n",
    "axes[0, 1].set_title('Learning Rate Schedule')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norms\n",
    "axes[1, 0].plot(metrics['steps'], metrics['grad_norms'], 'g-', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title('Gradient Norms')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight norms\n",
    "axes[1, 1].plot(metrics['steps'], metrics['weight_norms'], 'm-', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Weight Norm')\n",
    "axes[1, 1].set_title('Weight Norms')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Quality Indicators:\")\n",
    "print(f\"• Loss decreased from {metrics['losses'][0]:.3f} to {metrics['losses'][-1]:.3f}\")\n",
    "print(f\"• Gradient norms: {np.mean(metrics['grad_norms']):.3f} (should be stable, not too large)\")\n",
    "print(f\"• Weight norms growing: {metrics['weight_norms'][-1] > metrics['weight_norms'][0]} (expected during training)\")\n",
    "print(f\"• Learning rate properly scheduled: {metrics['learning_rates'][0]:.2e} → {metrics['learning_rates'][-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the complete transformer training process:\n",
    "\n",
    "1. **Language Modeling Objective** - Next token prediction with cross-entropy loss\n",
    "2. **Training Loop** - Forward pass, loss calculation, backpropagation, optimization\n",
    "3. **Progress Monitoring** - Loss curves, perplexity, sample generation\n",
    "4. **Learning Rate Scheduling** - Warmup, cosine decay, and their effects\n",
    "5. **Regularization** - Dropout, weight decay, gradient clipping\n",
    "6. **Comprehensive Monitoring** - Tracking gradients, weights, and training health\n",
    "\n",
    "### Key Training Insights:\n",
    "\n",
    "- **Loss is everything**: Cross-entropy loss drives all learning\n",
    "- **Perplexity matters**: More interpretable than raw loss\n",
    "- **Learning rate is critical**: Use warmup + cosine decay\n",
    "- **Regularization prevents overfitting**: Dropout, weight decay, clipping\n",
    "- **Monitor everything**: Loss, gradients, weights, samples\n",
    "- **Scaling laws**: More data + bigger models = better performance\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Start with a small model and overfit on a tiny dataset\n",
    "- Use proper learning rate scheduling (warmup + decay)\n",
    "- Monitor gradient norms (clip if > 1.0)\n",
    "- Generate samples regularly to check progress\n",
    "- Save checkpoints and implement early stopping\n",
    "- Use mixed precision training for efficiency\n",
    "\n",
    "### Training Stages:\n",
    "\n",
    "1. **Early**: Loss drops quickly, gradients large, samples nonsensical\n",
    "2. **Middle**: Steady improvement, learning meaningful patterns\n",
    "3. **Late**: Slow improvement, fine-tuning, risk of overfitting\n",
    "\n",
    "The magic of transformers is that this simple next-token prediction objective leads to emergence of complex language understanding, reasoning, and generation capabilities!\n",
    "\n",
    "Next, we'll explore the fascinating world of text generation strategies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}