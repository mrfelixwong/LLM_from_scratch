{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Considerations: Deploying Transformers at Scale\n",
    "\n",
    "Moving from research to production requires addressing efficiency, deployment, safety, and scale. This notebook covers the essential considerations for deploying transformer models in real-world applications.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Quantization** - Reducing model size and memory usage\n",
    "2. **Deployment Strategies** - Serving models efficiently \n",
    "3. **Distributed Training** - Training large models across multiple GPUs\n",
    "4. **Hardware Optimization** - Getting the most from your hardware\n",
    "5. **Safety and Monitoring** - Responsible AI deployment\n",
    "\n",
    "From research prototype to production powerhouse! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"Production toolkit loaded! üè≠\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Quantization\n",
    "\n",
    "Quantization reduces model size and memory usage by using lower precision numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    \"\"\"Tools for quantizing transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.original_state = None\n",
    "    \n",
    "    def get_model_size(self, model=None) -> Dict[str, float]:\n",
    "        \"\"\"Get model size in various units.\"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Calculate memory usage\n",
    "        param_memory = 0\n",
    "        for p in model.parameters():\n",
    "            param_memory += p.numel() * p.element_size()\n",
    "        \n",
    "        return {\n",
    "            'parameters': total_params,\n",
    "            'memory_bytes': param_memory,\n",
    "            'memory_mb': param_memory / (1024 * 1024),\n",
    "            'memory_gb': param_memory / (1024 * 1024 * 1024)\n",
    "        }\n",
    "    \n",
    "    def dynamic_quantization(self) -> nn.Module:\n",
    "        \"\"\"Apply dynamic quantization to the model.\"\"\"\n",
    "        # Save original state\n",
    "        self.original_state = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "        \n",
    "        # Apply dynamic quantization\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            self.model,\n",
    "            {nn.Linear},  # Quantize Linear layers\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "        \n",
    "        return quantized_model\n",
    "    \n",
    "    def simulate_fp16(self) -> nn.Module:\n",
    "        \"\"\"Simulate FP16 precision.\"\"\"\n",
    "        fp16_model = self.model.half()\n",
    "        return fp16_model\n",
    "    \n",
    "    def simulate_int8_quantization(self) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate INT8 quantization effects.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Get original model info\n",
    "        original_size = self.get_model_size()\n",
    "        \n",
    "        # Simulate quantization by reducing precision\n",
    "        with torch.no_grad():\n",
    "            quantized_params = {}\n",
    "            for name, param in self.model.named_parameters():\n",
    "                # Simulate 8-bit quantization\n",
    "                # Scale to use full int8 range\n",
    "                param_abs_max = param.abs().max()\n",
    "                scale = param_abs_max / 127.0  # Max value for int8\n",
    "                \n",
    "                # Quantize and dequantize\n",
    "                quantized = torch.round(param / scale).clamp(-128, 127)\n",
    "                dequantized = quantized * scale\n",
    "                \n",
    "                quantized_params[name] = dequantized\n",
    "                \n",
    "                # Calculate quantization error\n",
    "                error = (param - dequantized).abs().mean().item()\n",
    "                results[f'{name}_error'] = error\n",
    "        \n",
    "        # Estimate size reduction\n",
    "        int8_size = original_size['memory_bytes'] / 4  # 32-bit -> 8-bit = 4x smaller\n",
    "        \n",
    "        results.update({\n",
    "            'original_size_mb': original_size['memory_mb'],\n",
    "            'quantized_size_mb': int8_size / (1024 * 1024),\n",
    "            'compression_ratio': original_size['memory_bytes'] / int8_size,\n",
    "            'size_reduction_percent': (1 - int8_size / original_size['memory_bytes']) * 100\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_inference(self, model, input_ids, num_runs=100) -> Dict[str, float]:\n",
    "        \"\"\"Benchmark inference speed.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids)\n",
    "        \n",
    "        # Time inference\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / num_runs * 1000  # ms\n",
    "        \n",
    "        return {\n",
    "            'avg_inference_time_ms': avg_time,\n",
    "            'throughput_samples_per_sec': 1000 / avg_time,\n",
    "            'total_time_sec': end_time - start_time\n",
    "        }\n",
    "\n",
    "# Demonstrate quantization\n",
    "print(\"‚öñÔ∏è MODEL QUANTIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a model for testing\n",
    "config = create_model_config(\"small\")\n",
    "tokenizer = create_tokenizer(\"simple\")\n",
    "config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "\n",
    "model = GPTModel(**config).to(device)\n",
    "quantizer = ModelQuantizer(model)\n",
    "\n",
    "# Test input\n",
    "test_input = torch.randint(0, config[\"vocab_size\"], (1, 32)).to(device)\n",
    "\n",
    "# Get original model stats\n",
    "original_size = quantizer.get_model_size()\n",
    "original_perf = quantizer.benchmark_inference(model, test_input)\n",
    "\n",
    "print(f\"üìä ORIGINAL MODEL:\")\n",
    "print(f\"  Parameters: {original_size['parameters']:,}\")\n",
    "print(f\"  Memory: {original_size['memory_mb']:.1f} MB\")\n",
    "print(f\"  Inference: {original_perf['avg_inference_time_ms']:.2f} ms\")\n",
    "\n",
    "# Try FP16\n",
    "print(f\"\\nüîÑ FP16 QUANTIZATION:\")\n",
    "try:\n",
    "    fp16_model = model.half()\n",
    "    fp16_size = quantizer.get_model_size(fp16_model)\n",
    "    fp16_perf = quantizer.benchmark_inference(fp16_model, test_input.half() if test_input.dtype != torch.long else test_input)\n",
    "    \n",
    "    print(f\"  Memory: {fp16_size['memory_mb']:.1f} MB ({fp16_size['memory_mb']/original_size['memory_mb']:.1f}x smaller)\")\n",
    "    print(f\"  Inference: {fp16_perf['avg_inference_time_ms']:.2f} ms ({original_perf['avg_inference_time_ms']/fp16_perf['avg_inference_time_ms']:.1f}x faster)\")\n",
    "except Exception as e:\n",
    "    print(f\"  FP16 not supported: {e}\")\n",
    "\n",
    "# Simulate INT8 quantization\n",
    "print(f\"\\nüéØ INT8 QUANTIZATION SIMULATION:\")\n",
    "int8_results = quantizer.simulate_int8_quantization()\n",
    "print(f\"  Original: {int8_results['original_size_mb']:.1f} MB\")\n",
    "print(f\"  Quantized: {int8_results['quantized_size_mb']:.1f} MB\")\n",
    "print(f\"  Compression: {int8_results['compression_ratio']:.1f}x smaller\")\n",
    "print(f\"  Size reduction: {int8_results['size_reduction_percent']:.1f}%\")\n",
    "\n",
    "# Show quantization errors\n",
    "avg_error = np.mean([v for k, v in int8_results.items() if k.endswith('_error')])\n",
    "print(f\"  Average quantization error: {avg_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization trade-offs\n",
    "precision_types = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "memory_ratios = [1.0, 0.5, 0.25, 0.125]  # Relative to FP32\n",
    "speed_improvements = [1.0, 1.5, 2.5, 4.0]  # Approximate speedups\n",
    "quality_retention = [100, 99.5, 97, 90]  # Approximate quality retention %\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Memory usage\n",
    "bars1 = ax1.bar(precision_types, memory_ratios, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "ax1.set_ylabel('Relative Memory Usage')\n",
    "ax1.set_title('Memory Usage by Precision')\n",
    "ax1.set_ylim(0, 1.2)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, ratio in zip(bars1, memory_ratios):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{ratio:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Speed improvements\n",
    "bars2 = ax2.bar(precision_types, speed_improvements, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "ax2.set_ylabel('Relative Speed Improvement')\n",
    "ax2.set_title('Inference Speed by Precision')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, speed in zip(bars2, speed_improvements):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{speed:.1f}x', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Quality retention\n",
    "bars3 = ax3.bar(precision_types, quality_retention, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "ax3.set_ylabel('Model Quality Retention (%)')\n",
    "ax3.set_title('Quality Retention by Precision')\n",
    "ax3.set_ylim(80, 101)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, quality in zip(bars3, quality_retention):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "             f'{quality:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚öñÔ∏è QUANTIZATION TRADE-OFFS:\")\n",
    "print(\"‚Ä¢ FP32: Full precision, largest memory, slowest\")\n",
    "print(\"‚Ä¢ FP16: Half precision, 2x memory savings, ~1.5x speedup, minimal quality loss\")\n",
    "print(\"‚Ä¢ INT8: 4x memory savings, ~2.5x speedup, some quality loss\")\n",
    "print(\"‚Ä¢ INT4: 8x memory savings, ~4x speedup, significant quality loss\")\n",
    "print(\"\\nüéØ RECOMMENDATION: Start with FP16 for production deployments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deployment Strategies\n",
    "\n",
    "Different strategies for deploying transformer models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentStrategy:\n",
    "    \"\"\"Different deployment strategies for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache = {}\n",
    "        self.request_history = []\n",
    "    \n",
    "    def single_inference(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Single request inference.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                input_ids, \n",
    "                max_new_tokens=10,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'input': text,\n",
    "            'output': generated_text,\n",
    "            'latency_ms': (end_time - start_time) * 1000,\n",
    "            'input_tokens': len(tokens),\n",
    "            'output_tokens': output.shape[1]\n",
    "        }\n",
    "        \n",
    "        self.request_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def batched_inference(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Batched inference for multiple requests.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        all_tokens = []\n",
    "        max_len = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "            all_tokens.append(tokens)\n",
    "            max_len = max(max_len, len(tokens))\n",
    "        \n",
    "        # Pad to same length\n",
    "        padded_tokens = []\n",
    "        for tokens in all_tokens:\n",
    "            padded = tokens + [0] * (max_len - len(tokens))  # Pad with 0\n",
    "            padded_tokens.append(padded)\n",
    "        \n",
    "        input_ids = torch.tensor(padded_tokens).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_latency = (end_time - start_time) * 1000\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for i, (text, output) in enumerate(zip(texts, outputs)):\n",
    "            generated_text = self.tokenizer.decode(output.tolist(), skip_special_tokens=True)\n",
    "            \n",
    "            result = {\n",
    "                'input': text,\n",
    "                'output': generated_text,\n",
    "                'batch_latency_ms': total_latency,\n",
    "                'per_sample_latency_ms': total_latency / len(texts),\n",
    "                'input_tokens': len(all_tokens[i]),\n",
    "                'output_tokens': output.shape[0]\n",
    "            }\n",
    "            results.append(result)\n",
    "            self.request_history.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cached_inference(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Inference with caching for repeated requests.\"\"\"\n",
    "        cache_key = hash(text)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            result = self.cache[cache_key].copy()\n",
    "            result['cache_hit'] = True\n",
    "            result['latency_ms'] = 0.1  # Minimal cache lookup time\n",
    "            return result\n",
    "        \n",
    "        # Not in cache, compute normally\n",
    "        result = self.single_inference(text)\n",
    "        result['cache_hit'] = False\n",
    "        \n",
    "        # Store in cache\n",
    "        self.cache[cache_key] = result.copy()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def streaming_inference(self, text: str, callback=None) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate streaming inference (token by token).\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        generated_tokens = []\n",
    "        current_ids = input_ids\n",
    "        \n",
    "        # Generate token by token\n",
    "        for step in range(10):  # Generate 10 tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(current_ids)\n",
    "                logits = outputs[0]\n",
    "                \n",
    "                # Sample next token\n",
    "                next_token_logits = logits[0, -1, :] / 0.8  # temperature\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                generated_tokens.append(next_token.item())\n",
    "                \n",
    "                # Update input for next iteration\n",
    "                current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # Simulate streaming callback\n",
    "                if callback:\n",
    "                    partial_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                    callback(step, partial_text)\n",
    "                \n",
    "                # Small delay to simulate streaming\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        full_output = input_ids.tolist()[0] + generated_tokens\n",
    "        generated_text = self.tokenizer.decode(full_output, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'input': text,\n",
    "            'output': generated_text,\n",
    "            'latency_ms': (end_time - start_time) * 1000,\n",
    "            'streaming': True,\n",
    "            'tokens_generated': len(generated_tokens)\n",
    "        }\n",
    "    \n",
    "    def benchmark_strategies(self, test_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Benchmark different deployment strategies.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Single inference\n",
    "        single_times = []\n",
    "        for text in test_texts:\n",
    "            result = self.single_inference(text)\n",
    "            single_times.append(result['latency_ms'])\n",
    "        \n",
    "        results['single'] = {\n",
    "            'avg_latency_ms': np.mean(single_times),\n",
    "            'total_time_ms': sum(single_times),\n",
    "            'throughput_req_per_sec': len(test_texts) / (sum(single_times) / 1000)\n",
    "        }\n",
    "        \n",
    "        # Batched inference\n",
    "        batch_results = self.batched_inference(test_texts)\n",
    "        batch_total_time = batch_results[0]['batch_latency_ms']\n",
    "        \n",
    "        results['batched'] = {\n",
    "            'avg_latency_ms': batch_total_time / len(test_texts),\n",
    "            'total_time_ms': batch_total_time,\n",
    "            'throughput_req_per_sec': len(test_texts) / (batch_total_time / 1000)\n",
    "        }\n",
    "        \n",
    "        # Cached inference (simulate cache hits)\n",
    "        self.cache.clear()\n",
    "        cached_times = []\n",
    "        \n",
    "        # First pass populates cache\n",
    "        for text in test_texts:\n",
    "            self.cached_inference(text)\n",
    "        \n",
    "        # Second pass hits cache\n",
    "        for text in test_texts:\n",
    "            result = self.cached_inference(text)\n",
    "            cached_times.append(result['latency_ms'])\n",
    "        \n",
    "        results['cached'] = {\n",
    "            'avg_latency_ms': np.mean(cached_times),\n",
    "            'total_time_ms': sum(cached_times),\n",
    "            'throughput_req_per_sec': len(test_texts) / (sum(cached_times) / 1000)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demonstrate deployment strategies\n",
    "print(\"üöÄ DEPLOYMENT STRATEGIES COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deployer = DeploymentStrategy(model, tokenizer)\n",
    "\n",
    "# Test inputs\n",
    "test_texts = [\n",
    "    \"The future of AI is\",\n",
    "    \"Machine learning will\",\n",
    "    \"Transformers are\",\n",
    "    \"Deep learning enables\",\n",
    "    \"Neural networks can\"\n",
    "]\n",
    "\n",
    "# Benchmark strategies\n",
    "benchmark_results = deployer.benchmark_strategies(test_texts)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE COMPARISON ({len(test_texts)} requests):\")\n",
    "print(f\"{'Strategy':<15} {'Avg Latency (ms)':<18} {'Total Time (ms)':<17} {'Throughput (req/s)'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for strategy, metrics in benchmark_results.items():\n",
    "    print(f\"{strategy.capitalize():<15} {metrics['avg_latency_ms']:<18.1f} {metrics['total_time_ms']:<17.1f} {metrics['throughput_req_per_sec']:<.1f}\")\n",
    "\n",
    "# Demonstrate streaming\n",
    "print(f\"\\nüåä STREAMING INFERENCE DEMO:\")\n",
    "def streaming_callback(step, partial_text):\n",
    "    print(f\"  Step {step}: '{partial_text}'\")\n",
    "\n",
    "streaming_result = deployer.streaming_inference(\"The future of technology\", streaming_callback)\n",
    "print(f\"  Final: '{streaming_result['output']}'\")\n",
    "print(f\"  Total latency: {streaming_result['latency_ms']:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deployment strategy trade-offs\n",
    "strategies = list(benchmark_results.keys())\n",
    "latencies = [benchmark_results[s]['avg_latency_ms'] for s in strategies]\n",
    "throughputs = [benchmark_results[s]['throughput_req_per_sec'] for s in strategies]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Latency comparison\n",
    "colors = ['blue', 'green', 'orange']\n",
    "bars1 = ax1.bar(strategies, latencies, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Average Latency (ms)')\n",
    "ax1.set_title('Latency by Deployment Strategy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, latency in zip(bars1, latencies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + max(latencies) * 0.01,\n",
    "             f'{latency:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Throughput comparison\n",
    "bars2 = ax2.bar(strategies, throughputs, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Throughput (requests/sec)')\n",
    "ax2.set_title('Throughput by Deployment Strategy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, throughput in zip(bars2, throughputs):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + max(throughputs) * 0.01,\n",
    "             f'{throughput:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ DEPLOYMENT STRATEGY INSIGHTS:\")\n",
    "print(\"‚Ä¢ Single: Simple but low throughput\")\n",
    "print(\"‚Ä¢ Batched: Higher throughput, good for high load\")\n",
    "print(\"‚Ä¢ Cached: Extremely fast for repeated requests\")\n",
    "print(\"‚Ä¢ Streaming: Better user experience for long responses\")\n",
    "print(\"\\nüí° RECOMMENDATION: Use batching + caching for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Training\n",
    "\n",
    "Training large models across multiple GPUs and machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedTrainingAnalyzer:\n",
    "    \"\"\"Analyze distributed training strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.strategies = {\n",
    "            'Data Parallel': {\n",
    "                'description': 'Replicate model on each GPU, split data',\n",
    "                'memory_per_gpu': 'Full model + gradients',\n",
    "                'communication': 'All-reduce gradients',\n",
    "                'efficiency': 0.8,  # Communication overhead\n",
    "                'max_model_size': '1 GPU memory limit'\n",
    "            },\n",
    "            'Model Parallel': {\n",
    "                'description': 'Split model layers across GPUs',\n",
    "                'memory_per_gpu': 'Model subset',\n",
    "                'communication': 'Activations between layers',\n",
    "                'efficiency': 0.6,  # Pipeline bubbles\n",
    "                'max_model_size': 'Sum of all GPU memory'\n",
    "            },\n",
    "            'Pipeline Parallel': {\n",
    "                'description': 'Split model + pipeline micro-batches',\n",
    "                'memory_per_gpu': 'Model subset + micro-batches',\n",
    "                'communication': 'Activations pipeline',\n",
    "                'efficiency': 0.75,  # Reduced bubbles\n",
    "                'max_model_size': 'Sum of all GPU memory'\n",
    "            },\n",
    "            'Tensor Parallel': {\n",
    "                'description': 'Split individual layers across GPUs',\n",
    "                'memory_per_gpu': 'Layer subset',\n",
    "                'communication': 'All-reduce within layers',\n",
    "                'efficiency': 0.85,  # High bandwidth needed\n",
    "                'max_model_size': 'GPU count √ó single GPU memory'\n",
    "            },\n",
    "            '3D Parallel': {\n",
    "                'description': 'Combine data + model + tensor parallel',\n",
    "                'memory_per_gpu': 'Minimal',\n",
    "                'communication': 'Complex but optimized',\n",
    "                'efficiency': 0.9,  # Best for large scale\n",
    "                'max_model_size': 'Virtually unlimited'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def estimate_training_time(self, model_params: float, gpus: int, strategy: str, \n",
    "                             gpu_memory_gb: float = 80, gpu_tflops: float = 200) -> Dict[str, float]:\n",
    "        \"\"\"Estimate training time for different strategies.\"\"\"\n",
    "        \n",
    "        if strategy not in self.strategies:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        strategy_info = self.strategies[strategy]\n",
    "        efficiency = strategy_info['efficiency']\n",
    "        \n",
    "        # Estimate memory requirements\n",
    "        model_memory_gb = model_params * 4 / (1024**3)  # FP32 bytes to GB\n",
    "        \n",
    "        # Check if model fits\n",
    "        if strategy == 'Data Parallel':\n",
    "            memory_per_gpu = model_memory_gb * 2  # Model + gradients\n",
    "            max_model_params = gpu_memory_gb * 0.8 / 8  # 80% utilization, FP32\n",
    "        elif strategy in ['Model Parallel', 'Pipeline Parallel']:\n",
    "            memory_per_gpu = model_memory_gb / gpus\n",
    "            max_model_params = gpu_memory_gb * gpus * 0.8 / 4\n",
    "        elif strategy == 'Tensor Parallel':\n",
    "            memory_per_gpu = model_memory_gb / gpus\n",
    "            max_model_params = gpu_memory_gb * gpus * 0.8 / 4\n",
    "        else:  # 3D Parallel\n",
    "            memory_per_gpu = model_memory_gb / (gpus * 0.8)  # Optimistic\n",
    "            max_model_params = gpu_memory_gb * gpus * 0.8 / 2\n",
    "        \n",
    "        # Estimate compute time\n",
    "        # Rough estimate: 6 FLOPs per parameter per token\n",
    "        tokens_per_step = 1000  # Typical batch size\n",
    "        flops_per_step = 6 * model_params * tokens_per_step\n",
    "        \n",
    "        # Account for parallelization efficiency\n",
    "        effective_tflops = gpu_tflops * gpus * efficiency\n",
    "        time_per_step = flops_per_step / (effective_tflops * 1e12)  # seconds\n",
    "        \n",
    "        return {\n",
    "            'memory_per_gpu_gb': memory_per_gpu,\n",
    "            'fits_in_memory': memory_per_gpu <= gpu_memory_gb * 0.8,\n",
    "            'max_model_params': max_model_params,\n",
    "            'time_per_step_ms': time_per_step * 1000,\n",
    "            'effective_tflops': effective_tflops,\n",
    "            'efficiency': efficiency\n",
    "        }\n",
    "    \n",
    "    def compare_strategies(self, model_params: float, gpu_counts: List[int]) -> Dict:\n",
    "        \"\"\"Compare strategies across different GPU counts.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in self.strategies.keys():\n",
    "            results[strategy] = {}\n",
    "            \n",
    "            for gpu_count in gpu_counts:\n",
    "                try:\n",
    "                    estimate = self.estimate_training_time(model_params, gpu_count, strategy)\n",
    "                    results[strategy][gpu_count] = estimate\n",
    "                except Exception as e:\n",
    "                    results[strategy][gpu_count] = {'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_scaling_analysis(self, model_params: float):\n",
    "        \"\"\"Plot scaling analysis for different strategies.\"\"\"\n",
    "        gpu_counts = [1, 2, 4, 8, 16, 32, 64]\n",
    "        comparison = self.compare_strategies(model_params, gpu_counts)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Memory usage per GPU\n",
    "        for strategy in ['Data Parallel', 'Model Parallel', 'Tensor Parallel', '3D Parallel']:\n",
    "            memory_usage = []\n",
    "            valid_gpus = []\n",
    "            \n",
    "            for gpu_count in gpu_counts:\n",
    "                if gpu_count in comparison[strategy] and 'memory_per_gpu_gb' in comparison[strategy][gpu_count]:\n",
    "                    memory = comparison[strategy][gpu_count]['memory_per_gpu_gb']\n",
    "                    if memory > 0 and memory < 1000:  # Reasonable range\n",
    "                        memory_usage.append(memory)\n",
    "                        valid_gpus.append(gpu_count)\n",
    "            \n",
    "            if memory_usage:\n",
    "                axes[0, 0].plot(valid_gpus, memory_usage, 'o-', label=strategy, linewidth=2, markersize=6)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Number of GPUs')\n",
    "        axes[0, 0].set_ylabel('Memory per GPU (GB)')\n",
    "        axes[0, 0].set_title('Memory Usage vs GPU Count')\n",
    "        axes[0, 0].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='GPU Memory Limit')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_xscale('log', base=2)\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        \n",
    "        # Training speed\n",
    "        for strategy in ['Data Parallel', 'Model Parallel', 'Tensor Parallel', '3D Parallel']:\n",
    "            speeds = []\n",
    "            valid_gpus = []\n",
    "            \n",
    "            for gpu_count in gpu_counts:\n",
    "                if gpu_count in comparison[strategy] and 'time_per_step_ms' in comparison[strategy][gpu_count]:\n",
    "                    time_ms = comparison[strategy][gpu_count]['time_per_step_ms']\n",
    "                    if time_ms > 0 and time_ms < 10000:  # Reasonable range\n",
    "                        speeds.append(time_ms)\n",
    "                        valid_gpus.append(gpu_count)\n",
    "            \n",
    "            if speeds:\n",
    "                axes[0, 1].plot(valid_gpus, speeds, 'o-', label=strategy, linewidth=2, markersize=6)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Number of GPUs')\n",
    "        axes[0, 1].set_ylabel('Time per Step (ms)')\n",
    "        axes[0, 1].set_title('Training Speed vs GPU Count')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_xscale('log', base=2)\n",
    "        axes[0, 1].set_yscale('log')\n",
    "        \n",
    "        # Efficiency comparison\n",
    "        strategies = list(self.strategies.keys())\n",
    "        efficiencies = [self.strategies[s]['efficiency'] for s in strategies]\n",
    "        \n",
    "        bars = axes[1, 0].bar(strategies, efficiencies, alpha=0.7, color=plt.cm.viridis(np.linspace(0, 1, len(strategies))))\n",
    "        axes[1, 0].set_ylabel('Efficiency')\n",
    "        axes[1, 0].set_title('Strategy Efficiency Comparison')\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, eff in zip(bars, efficiencies):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                           f'{eff:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.setp(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Model size limits\n",
    "        gpu_memory = 80  # GB\n",
    "        max_sizes = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            if strategy == 'Data Parallel':\n",
    "                max_params = gpu_memory * 0.8 / 8 * 1e9  # Conservative estimate\n",
    "            elif strategy in ['Model Parallel', 'Pipeline Parallel', 'Tensor Parallel']:\n",
    "                max_params = gpu_memory * 64 * 0.8 / 4 * 1e9  # 64 GPUs\n",
    "            else:  # 3D Parallel\n",
    "                max_params = gpu_memory * 1000 * 0.8 / 2 * 1e9  # Very optimistic\n",
    "            \n",
    "            max_sizes.append(max_params / 1e9)  # Convert to billions\n",
    "        \n",
    "        bars2 = axes[1, 1].bar(strategies, max_sizes, alpha=0.7, color=plt.cm.plasma(np.linspace(0, 1, len(strategies))))\n",
    "        axes[1, 1].set_ylabel('Max Model Size (Billion Parameters)')\n",
    "        axes[1, 1].set_title('Maximum Model Size by Strategy')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate distributed training analysis\n",
    "print(\"üîÄ DISTRIBUTED TRAINING ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dist_analyzer = DistributedTrainingAnalyzer()\n",
    "\n",
    "# Analyze a 7B parameter model (like LLaMA-7B)\n",
    "model_size = 7e9  # 7 billion parameters\n",
    "print(f\"\\nAnalyzing {model_size/1e9:.0f}B parameter model:\")\n",
    "\n",
    "# Compare strategies for different GPU counts\n",
    "gpu_counts = [1, 8, 32]\n",
    "\n",
    "for gpu_count in gpu_counts:\n",
    "    print(f\"\\nüìä {gpu_count} GPUs:\")\n",
    "    print(f\"{'Strategy':<20} {'Memory/GPU (GB)':<15} {'Fits?':<8} {'Time/Step (ms)':<15} {'Efficiency'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for strategy in ['Data Parallel', 'Model Parallel', 'Tensor Parallel', '3D Parallel']:\n",
    "        try:\n",
    "            estimate = dist_analyzer.estimate_training_time(model_size, gpu_count, strategy)\n",
    "            memory = estimate['memory_per_gpu_gb']\n",
    "            fits = \"‚úÖ\" if estimate['fits_in_memory'] else \"‚ùå\"\n",
    "            time_ms = estimate['time_per_step_ms']\n",
    "            efficiency = estimate['efficiency']\n",
    "            \n",
    "            print(f\"{strategy:<20} {memory:<15.1f} {fits:<8} {time_ms:<15.1f} {efficiency:.1%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{strategy:<20} Error: {str(e)[:40]}...\")\n",
    "\n",
    "# Plot scaling analysis\n",
    "dist_analyzer.plot_scaling_analysis(model_size)\n",
    "\n",
    "print(\"\\nüéØ DISTRIBUTED TRAINING INSIGHTS:\")\n",
    "print(\"‚Ä¢ Data Parallel: Simple but limited by GPU memory\")\n",
    "print(\"‚Ä¢ Model Parallel: Enables larger models but has pipeline bubbles\")\n",
    "print(\"‚Ä¢ Tensor Parallel: Good balance, requires high-bandwidth interconnects\")\n",
    "print(\"‚Ä¢ 3D Parallel: Most scalable, used for largest models (GPT-3, PaLM)\")\n",
    "print(\"\\nüí° RECOMMENDATION: Use 3D parallel for models >10B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hardware Optimization\n",
    "\n",
    "Getting the most performance from your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwareOptimizer:\n",
    "    \"\"\"Hardware optimization tools and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_specs = {\n",
    "            'V100': {'memory_gb': 32, 'tflops_fp32': 15, 'tflops_fp16': 125, 'price_hourly': 3.0},\n",
    "            'A100': {'memory_gb': 80, 'tflops_fp32': 20, 'tflops_fp16': 312, 'price_hourly': 4.0},\n",
    "            'H100': {'memory_gb': 80, 'tflops_fp32': 30, 'tflops_fp16': 500, 'price_hourly': 8.0},\n",
    "            'RTX4090': {'memory_gb': 24, 'tflops_fp32': 35, 'tflops_fp16': 165, 'price_hourly': 1.5},\n",
    "        }\n",
    "    \n",
    "    def analyze_gpu_utilization(self, model_params: float, batch_size: int, seq_length: int) -> Dict:\n",
    "        \"\"\"Analyze GPU utilization for different hardware.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for gpu_name, specs in self.gpu_specs.items():\n",
    "            # Estimate memory usage\n",
    "            model_memory = model_params * 4 / (1024**3)  # FP32 in GB\n",
    "            activation_memory = batch_size * seq_length * model_params**(1/3) * 4 / (1024**3)  # Rough estimate\n",
    "            optimizer_memory = model_memory * 2  # Adam optimizer states\n",
    "            \n",
    "            total_memory = model_memory + activation_memory + optimizer_memory\n",
    "            memory_utilization = min(total_memory / specs['memory_gb'], 1.0)\n",
    "            \n",
    "            # Estimate compute utilization\n",
    "            # Rough estimate based on model FLOPS vs GPU capability\n",
    "            model_flops_per_token = 6 * model_params  # Forward + backward\n",
    "            total_flops = model_flops_per_token * batch_size * seq_length\n",
    "            \n",
    "            # Assume 50ms per step (reasonable for training)\n",
    "            required_tflops = total_flops / (0.05 * 1e12)\n",
    "            compute_utilization = min(required_tflops / specs['tflops_fp16'], 1.0)\n",
    "            \n",
    "            # Calculate efficiency\n",
    "            efficiency = min(memory_utilization, compute_utilization)\n",
    "            bottleneck = 'memory' if memory_utilization < compute_utilization else 'compute'\n",
    "            \n",
    "            results[gpu_name] = {\n",
    "                'memory_gb_used': total_memory,\n",
    "                'memory_utilization': memory_utilization,\n",
    "                'compute_utilization': compute_utilization,\n",
    "                'efficiency': efficiency,\n",
    "                'bottleneck': bottleneck,\n",
    "                'fits': total_memory <= specs['memory_gb'],\n",
    "                'cost_per_hour': specs['price_hourly']\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def recommend_hardware(self, model_params: float, budget_per_hour: float = 100) -> Dict:\n",
    "        \"\"\"Recommend hardware configuration for a given model and budget.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Analyze single GPU configurations\n",
    "        analysis = self.analyze_gpu_utilization(model_params, batch_size=32, seq_length=512)\n",
    "        \n",
    "        for gpu_name, metrics in analysis.items():\n",
    "            if metrics['fits']:\n",
    "                gpus_in_budget = int(budget_per_hour / self.gpu_specs[gpu_name]['price_hourly'])\n",
    "                \n",
    "                recommendation = {\n",
    "                    'gpu_type': gpu_name,\n",
    "                    'num_gpus': gpus_in_budget,\n",
    "                    'total_memory_gb': gpus_in_budget * self.gpu_specs[gpu_name]['memory_gb'],\n",
    "                    'total_tflops': gpus_in_budget * self.gpu_specs[gpu_name]['tflops_fp16'],\n",
    "                    'hourly_cost': gpus_in_budget * self.gpu_specs[gpu_name]['price_hourly'],\n",
    "                    'efficiency': metrics['efficiency'],\n",
    "                    'bottleneck': metrics['bottleneck']\n",
    "                }\n",
    "                recommendations.append(recommendation)\n",
    "        \n",
    "        # Sort by efficiency and cost\n",
    "        recommendations.sort(key=lambda x: (x['efficiency'], -x['hourly_cost']), reverse=True)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def optimization_checklist(self) -> List[str]:\n",
    "        \"\"\"Return hardware optimization checklist.\"\"\"\n",
    "        return [\n",
    "            \"üîß Use mixed precision (FP16) training\",\n",
    "            \"üìä Profile GPU utilization with nvidia-smi\",\n",
    "            \"üöÄ Enable Tensor Cores when available\",\n",
    "            \"üíæ Use gradient checkpointing for memory\",\n",
    "            \"‚ö° Optimize data loading pipeline\",\n",
    "            \"üîÑ Use gradient accumulation for large batches\",\n",
    "            \"üéØ Tune batch size for optimal throughput\",\n",
    "            \"üìà Monitor memory fragmentation\",\n",
    "            \"üåê Use fast interconnects (InfiniBand/NVLink)\",\n",
    "            \"‚ùÑÔ∏è Keep GPUs cool for sustained performance\",\n",
    "            \"‚öñÔ∏è Balance compute and memory workloads\",\n",
    "            \"üîç Use profiling tools (NSight, PyTorch Profiler)\"\n",
    "        ]\n",
    "    \n",
    "    def plot_hardware_comparison(self, model_params: float):\n",
    "        \"\"\"Plot hardware comparison for a given model size.\"\"\"\n",
    "        analysis = self.analyze_gpu_utilization(model_params, batch_size=32, seq_length=512)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        gpus = list(analysis.keys())\n",
    "        \n",
    "        # Memory utilization\n",
    "        memory_util = [analysis[gpu]['memory_utilization'] * 100 for gpu in gpus]\n",
    "        colors = ['red' if util > 90 else 'orange' if util > 70 else 'green' for util in memory_util]\n",
    "        \n",
    "        bars1 = axes[0, 0].bar(gpus, memory_util, color=colors, alpha=0.7)\n",
    "        axes[0, 0].set_ylabel('Memory Utilization (%)')\n",
    "        axes[0, 0].set_title('Memory Utilization by GPU Type')\n",
    "        axes[0, 0].axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='Warning (80%)')\n",
    "        axes[0, 0].axhline(y=95, color='red', linestyle='--', alpha=0.7, label='Critical (95%)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, util in zip(bars1, memory_util):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{util:.0f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Compute utilization\n",
    "        compute_util = [analysis[gpu]['compute_utilization'] * 100 for gpu in gpus]\n",
    "        \n",
    "        bars2 = axes[0, 1].bar(gpus, compute_util, alpha=0.7, color='blue')\n",
    "        axes[0, 1].set_ylabel('Compute Utilization (%)')\n",
    "        axes[0, 1].set_title('Compute Utilization by GPU Type')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, util in zip(bars2, compute_util):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{util:.0f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Cost efficiency (TFLOPS per dollar)\n",
    "        tflops_per_dollar = [self.gpu_specs[gpu]['tflops_fp16'] / self.gpu_specs[gpu]['price_hourly'] for gpu in gpus]\n",
    "        \n",
    "        bars3 = axes[1, 0].bar(gpus, tflops_per_dollar, alpha=0.7, color='green')\n",
    "        axes[1, 0].set_ylabel('TFLOPS per Dollar per Hour')\n",
    "        axes[1, 0].set_title('Cost Efficiency by GPU Type')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, eff in zip(bars3, tflops_per_dollar):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{eff:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Overall efficiency\n",
    "        efficiency = [analysis[gpu]['efficiency'] * 100 for gpu in gpus]\n",
    "        bottlenecks = [analysis[gpu]['bottleneck'] for gpu in gpus]\n",
    "        \n",
    "        colors_eff = ['red' if b == 'memory' else 'blue' for b in bottlenecks]\n",
    "        bars4 = axes[1, 1].bar(gpus, efficiency, color=colors_eff, alpha=0.7)\n",
    "        axes[1, 1].set_ylabel('Overall Efficiency (%)')\n",
    "        axes[1, 1].set_title('Overall Efficiency (Red=Memory Bound, Blue=Compute Bound)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, eff, bottleneck in zip(bars4, efficiency, bottlenecks):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{eff:.0f}%\\n({bottleneck})', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate hardware optimization\n",
    "print(\"‚öôÔ∏è HARDWARE OPTIMIZATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "optimizer = HardwareOptimizer()\n",
    "\n",
    "# Analyze different model sizes\n",
    "model_sizes = [1e9, 7e9, 70e9]  # 1B, 7B, 70B parameters\n",
    "budget = 50  # $50/hour\n",
    "\n",
    "for model_size in model_sizes:\n",
    "    print(f\"\\nüîç {model_size/1e9:.0f}B Parameter Model:\")\n",
    "    \n",
    "    recommendations = optimizer.recommend_hardware(model_size, budget)\n",
    "    \n",
    "    if recommendations:\n",
    "        best = recommendations[0]\n",
    "        print(f\"  Best option: {best['num_gpus']}x {best['gpu_type']}\")\n",
    "        print(f\"  Total memory: {best['total_memory_gb']:.0f} GB\")\n",
    "        print(f\"  Total compute: {best['total_tflops']:.0f} TFLOPS\")\n",
    "        print(f\"  Hourly cost: ${best['hourly_cost']:.0f}\")\n",
    "        print(f\"  Efficiency: {best['efficiency']:.1%}\")\n",
    "        print(f\"  Bottleneck: {best['bottleneck']}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Model too large for budget\")\n",
    "\n",
    "# Plot hardware comparison for 7B model\n",
    "print(f\"\\nüìä Hardware Analysis for 7B Parameter Model:\")\n",
    "optimizer.plot_hardware_comparison(7e9)\n",
    "\n",
    "# Show optimization checklist\n",
    "print(f\"\\n‚úÖ HARDWARE OPTIMIZATION CHECKLIST:\")\n",
    "checklist = optimizer.optimization_checklist()\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nüéØ HARDWARE INSIGHTS:\")\n",
    "print(f\"‚Ä¢ A100s offer best balance of memory and compute\")\n",
    "print(f\"‚Ä¢ H100s provide highest compute but are expensive\")\n",
    "print(f\"‚Ä¢ RTX4090s offer good cost efficiency for smaller models\")\n",
    "print(f\"‚Ä¢ Memory is often the bottleneck for large models\")\n",
    "print(f\"‚Ä¢ Mixed precision (FP16) can double effective memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Safety and Monitoring\n",
    "\n",
    "Responsible AI deployment with proper safety measures and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AISeasoningSystem:\n",
    "    \"\"\"AI Safety and Monitoring System.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.safety_checks = {\n",
    "            'content_filtering': {\n",
    "                'description': 'Filter harmful or inappropriate content',\n",
    "                'implementation': 'Pre/post-processing filters',\n",
    "                'priority': 'critical'\n",
    "            },\n",
    "            'bias_detection': {\n",
    "                'description': 'Monitor for biased outputs',\n",
    "                'implementation': 'Statistical bias metrics',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            'hallucination_detection': {\n",
    "                'description': 'Detect factually incorrect outputs',\n",
    "                'implementation': 'Knowledge base verification',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            'rate_limiting': {\n",
    "                'description': 'Prevent abuse through rate limits',\n",
    "                'implementation': 'Request throttling',\n",
    "                'priority': 'medium'\n",
    "            },\n",
    "            'output_monitoring': {\n",
    "                'description': 'Monitor output quality and safety',\n",
    "                'implementation': 'Real-time quality metrics',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            'adversarial_detection': {\n",
    "                'description': 'Detect adversarial inputs',\n",
    "                'implementation': 'Input analysis and flagging',\n",
    "                'priority': 'medium'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.monitoring_metrics = {\n",
    "            'requests_per_second': 0,\n",
    "            'average_latency': 0,\n",
    "            'error_rate': 0,\n",
    "            'safety_violations': 0,\n",
    "            'content_filtered': 0,\n",
    "            'bias_incidents': 0\n",
    "        }\n",
    "        \n",
    "        # Simulated safety violations for demo\n",
    "        self.violation_history = []\n",
    "    \n",
    "    def content_filter(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate content filtering.\"\"\"\n",
    "        # Simulated harmful content detection\n",
    "        harmful_keywords = ['violence', 'harmful', 'illegal', 'dangerous']\n",
    "        \n",
    "        violations = []\n",
    "        for keyword in harmful_keywords:\n",
    "            if keyword.lower() in text.lower():\n",
    "                violations.append(keyword)\n",
    "        \n",
    "        is_safe = len(violations) == 0\n",
    "        confidence = 0.95 if violations else 0.05\n",
    "        \n",
    "        return {\n",
    "            'is_safe': is_safe,\n",
    "            'confidence': confidence,\n",
    "            'violations': violations,\n",
    "            'filtered_text': text if is_safe else '[CONTENT FILTERED]'\n",
    "        }\n",
    "    \n",
    "    def bias_detector(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate bias detection.\"\"\"\n",
    "        # Simulated bias detection\n",
    "        bias_indicators = {\n",
    "            'gender': ['he is smart', 'she is emotional'],\n",
    "            'racial': ['certain groups', 'those people'],\n",
    "            'age': ['young and energetic', 'old and slow']\n",
    "        }\n",
    "        \n",
    "        detected_biases = []\n",
    "        for bias_type, indicators in bias_indicators.items():\n",
    "            for indicator in indicators:\n",
    "                if indicator.lower() in text.lower():\n",
    "                    detected_biases.append(bias_type)\n",
    "        \n",
    "        bias_score = len(detected_biases) / len(bias_indicators)\n",
    "        \n",
    "        return {\n",
    "            'bias_score': bias_score,\n",
    "            'detected_biases': list(set(detected_biases)),\n",
    "            'is_biased': bias_score > 0.3\n",
    "        }\n",
    "    \n",
    "    def hallucination_detector(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate hallucination detection.\"\"\"\n",
    "        # Simulated hallucination markers\n",
    "        hallucination_markers = [\n",
    "            'definitely', 'absolutely certain', 'I know for a fact',\n",
    "            'scientific studies prove', 'it is proven that'\n",
    "        ]\n",
    "        \n",
    "        confidence_markers = sum(1 for marker in hallucination_markers \n",
    "                               if marker.lower() in text.lower())\n",
    "        \n",
    "        # Simple heuristic: high confidence claims are suspicious\n",
    "        hallucination_risk = min(confidence_markers * 0.3, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'hallucination_risk': hallucination_risk,\n",
    "            'confidence_markers': confidence_markers,\n",
    "            'is_suspicious': hallucination_risk > 0.5\n",
    "        }\n",
    "    \n",
    "    def monitor_request(self, input_text: str, output_text: str, \n",
    "                       latency_ms: float) -> Dict[str, Any]:\n",
    "        \"\"\"Monitor a single request for safety and quality.\"\"\"\n",
    "        # Safety checks\n",
    "        content_result = self.content_filter(output_text)\n",
    "        bias_result = self.bias_detector(output_text)\n",
    "        hallucination_result = self.hallucination_detector(output_text)\n",
    "        \n",
    "        # Overall safety assessment\n",
    "        safety_violations = []\n",
    "        if not content_result['is_safe']:\n",
    "            safety_violations.append('content')\n",
    "        if bias_result['is_biased']:\n",
    "            safety_violations.append('bias')\n",
    "        if hallucination_result['is_suspicious']:\n",
    "            safety_violations.append('hallucination')\n",
    "        \n",
    "        is_safe = len(safety_violations) == 0\n",
    "        \n",
    "        # Update metrics\n",
    "        self.monitoring_metrics['requests_per_second'] += 1\n",
    "        self.monitoring_metrics['average_latency'] = latency_ms\n",
    "        \n",
    "        if not is_safe:\n",
    "            self.monitoring_metrics['safety_violations'] += 1\n",
    "            self.violation_history.append({\n",
    "                'timestamp': time.time(),\n",
    "                'violations': safety_violations,\n",
    "                'input': input_text[:100] + '...',\n",
    "                'output': output_text[:100] + '...'\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'is_safe': is_safe,\n",
    "            'safety_violations': safety_violations,\n",
    "            'content_filter': content_result,\n",
    "            'bias_detection': bias_result,\n",
    "            'hallucination_detection': hallucination_result,\n",
    "            'latency_ms': latency_ms,\n",
    "            'action': 'allow' if is_safe else 'block'\n",
    "        }\n",
    "    \n",
    "    def generate_safety_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a safety monitoring report.\"\"\"\n",
    "        total_requests = self.monitoring_metrics['requests_per_second']\n",
    "        violations = self.monitoring_metrics['safety_violations']\n",
    "        \n",
    "        return {\n",
    "            'total_requests': total_requests,\n",
    "            'safety_violations': violations,\n",
    "            'violation_rate': violations / max(total_requests, 1),\n",
    "            'average_latency': self.monitoring_metrics['average_latency'],\n",
    "            'recent_violations': self.violation_history[-5:],  # Last 5 violations\n",
    "            'safety_score': max(0, 1 - violations / max(total_requests, 1))\n",
    "        }\n",
    "    \n",
    "    def safety_checklist(self) -> List[str]:\n",
    "        \"\"\"Return production safety checklist.\"\"\"\n",
    "        return [\n",
    "            \"üõ°Ô∏è Implement content filtering for harmful outputs\",\n",
    "            \"‚öñÔ∏è Monitor for bias in model outputs\",\n",
    "            \"üîç Detect and flag hallucinations\",\n",
    "            \"üö¶ Implement rate limiting and abuse detection\",\n",
    "            \"üìä Set up comprehensive logging and monitoring\",\n",
    "            \"üîí Secure API endpoints and authentication\",\n",
    "            \"üë• Establish human review processes\",\n",
    "            \"üìã Create incident response procedures\",\n",
    "            \"üîÑ Regular safety audits and red-teaming\",\n",
    "            \"üìö User education and guidelines\",\n",
    "            \"‚õî Implement kill switches for emergencies\",\n",
    "            \"üìà Track safety metrics and trends\"\n",
    "        ]\n",
    "\n",
    "# Demonstrate safety monitoring\n",
    "print(\"üõ°Ô∏è AI SAFETY AND MONITORING DEMO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "safety_system = AISeasoningSystem()\n",
    "\n",
    "# Test different types of content\n",
    "test_cases = [\n",
    "    {\n",
    "        'input': 'Tell me about renewable energy',\n",
    "        'output': 'Renewable energy sources like solar and wind are important for sustainability.',\n",
    "        'latency': 45.2\n",
    "    },\n",
    "    {\n",
    "        'input': 'How to make something dangerous',\n",
    "        'output': 'I cannot provide information on creating dangerous or harmful items.',\n",
    "        'latency': 23.1\n",
    "    },\n",
    "    {\n",
    "        'input': 'What are some facts about history?',\n",
    "        'output': 'I absolutely know for a fact that historical events definitely happened exactly as I describe.',\n",
    "        'latency': 67.8\n",
    "    },\n",
    "    {\n",
    "        'input': 'Describe programming abilities',\n",
    "        'output': 'He is smart at programming while she is emotional about code reviews.',\n",
    "        'latency': 34.5\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüîç MONITORING TEST CASES:\")\n",
    "print(f\"{'Case':<6} {'Safety':<8} {'Violations':<20} {'Action':<8} {'Latency (ms)'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    result = safety_system.monitor_request(\n",
    "        case['input'], case['output'], case['latency']\n",
    "    )\n",
    "    \n",
    "    safety_status = \"‚úÖ Safe\" if result['is_safe'] else \"‚ùå Unsafe\"\n",
    "    violations = ', '.join(result['safety_violations']) if result['safety_violations'] else 'None'\n",
    "    action = result['action']\n",
    "    latency = case['latency']\n",
    "    \n",
    "    print(f\"{i:<6} {safety_status:<8} {violations:<20} {action:<8} {latency}\")\n",
    "\n",
    "# Generate safety report\n",
    "report = safety_system.generate_safety_report()\n",
    "\n",
    "print(f\"\\nüìä SAFETY MONITORING REPORT:\")\n",
    "print(f\"  Total requests: {report['total_requests']}\")\n",
    "print(f\"  Safety violations: {report['safety_violations']}\")\n",
    "print(f\"  Violation rate: {report['violation_rate']:.1%}\")\n",
    "print(f\"  Safety score: {report['safety_score']:.1%}\")\n",
    "print(f\"  Average latency: {report['average_latency']:.1f} ms\")\n",
    "\n",
    "if report['recent_violations']:\n",
    "    print(f\"\\n‚ö†Ô∏è Recent violations:\")\n",
    "    for violation in report['recent_violations']:\n",
    "        print(f\"    {violation['violations']}: {violation['output'][:50]}...\")\n",
    "\n",
    "# Show safety checklist\n",
    "print(f\"\\n‚úÖ PRODUCTION SAFETY CHECKLIST:\")\n",
    "checklist = safety_system.safety_checklist()\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nüéØ SAFETY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Content filtering catches obvious harmful content\")\n",
    "print(f\"‚Ä¢ Bias detection requires ongoing monitoring and adjustment\")\n",
    "print(f\"‚Ä¢ Hallucination detection is challenging but critical\")\n",
    "print(f\"‚Ä¢ Human oversight remains essential for edge cases\")\n",
    "print(f\"‚Ä¢ Regular audits and red-teaming are necessary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Production-Ready Transformer Deployment üè≠\n",
    "\n",
    "You now have the complete toolkit for deploying transformers in production!\n",
    "\n",
    "### üîß Production Optimizations\n",
    "\n",
    "**1. Quantization**\n",
    "- **FP16**: 2x memory savings, minimal quality loss\n",
    "- **INT8**: 4x memory savings, some quality degradation\n",
    "- **INT4**: 8x memory savings, significant quality loss\n",
    "- **Recommendation**: Start with FP16, consider INT8 for deployment\n",
    "\n",
    "**2. Deployment Strategies**\n",
    "- **Single inference**: Simple but low throughput\n",
    "- **Batched inference**: Higher throughput for multiple requests\n",
    "- **Cached inference**: Ultra-fast for repeated queries\n",
    "- **Streaming**: Better UX for long responses\n",
    "- **Recommendation**: Use batching + caching for production\n",
    "\n",
    "**3. Distributed Training**\n",
    "- **Data Parallel**: Simple, limited by GPU memory\n",
    "- **Model Parallel**: Enables larger models, pipeline bubbles\n",
    "- **Tensor Parallel**: Good balance, needs high bandwidth\n",
    "- **3D Parallel**: Most scalable, used for largest models\n",
    "- **Recommendation**: 3D parallel for models >10B parameters\n",
    "\n",
    "### ‚öôÔ∏è Hardware Optimization\n",
    "\n",
    "**GPU Selection:**\n",
    "- **A100**: Best balance of memory and compute\n",
    "- **H100**: Highest performance but expensive\n",
    "- **RTX4090**: Cost-effective for smaller models\n",
    "\n",
    "**Optimization Checklist:**\n",
    "- ‚úÖ Use mixed precision (FP16) training\n",
    "- ‚úÖ Profile GPU utilization regularly\n",
    "- ‚úÖ Optimize data loading pipelines\n",
    "- ‚úÖ Tune batch sizes for throughput\n",
    "- ‚úÖ Monitor memory fragmentation\n",
    "\n",
    "### üõ°Ô∏è Safety and Monitoring\n",
    "\n",
    "**Critical Safety Measures:**\n",
    "- **Content filtering**: Block harmful outputs\n",
    "- **Bias detection**: Monitor for unfair outputs\n",
    "- **Hallucination detection**: Flag suspicious claims\n",
    "- **Rate limiting**: Prevent abuse\n",
    "- **Human oversight**: Essential for edge cases\n",
    "\n",
    "**Monitoring Metrics:**\n",
    "- Request throughput and latency\n",
    "- Safety violation rates\n",
    "- Model quality scores\n",
    "- Hardware utilization\n",
    "- Cost per request\n",
    "\n",
    "### üí∞ Cost Optimization\n",
    "\n",
    "**Key Cost Drivers:**\n",
    "1. **Compute**: GPU hours for training/inference\n",
    "2. **Memory**: Model size and batch processing\n",
    "3. **Storage**: Model checkpoints and data\n",
    "4. **Bandwidth**: Data transfer and API calls\n",
    "\n",
    "**Cost Reduction Strategies:**\n",
    "- Use quantization to reduce memory needs\n",
    "- Implement efficient caching strategies\n",
    "- Optimize batch sizes for throughput\n",
    "- Use spot instances for training\n",
    "- Monitor and optimize utilization\n",
    "\n",
    "### üöÄ Deployment Pipeline\n",
    "\n",
    "**Recommended Production Pipeline:**\n",
    "1. **Development**: Train with FP32, small scale\n",
    "2. **Optimization**: Apply quantization, profiling\n",
    "3. **Testing**: Validate safety, performance, quality\n",
    "4. **Staging**: Full-scale testing with monitoring\n",
    "5. **Production**: Deploy with all safety measures\n",
    "6. **Monitoring**: Continuous safety and performance tracking\n",
    "\n",
    "### üìä Key Performance Indicators\n",
    "\n",
    "**Technical KPIs:**\n",
    "- Latency: <100ms for real-time applications\n",
    "- Throughput: >100 requests/second\n",
    "- GPU utilization: >80%\n",
    "- Safety violation rate: <0.1%\n",
    "\n",
    "**Business KPIs:**\n",
    "- Cost per request: <$0.01\n",
    "- User satisfaction: >95%\n",
    "- Uptime: >99.9%\n",
    "- Time to deployment: <2 weeks\n",
    "\n",
    "### üéØ Production Best Practices\n",
    "\n",
    "1. **Start small**: Begin with proven architectures and scale\n",
    "2. **Monitor everything**: Comprehensive logging and alerting\n",
    "3. **Automate testing**: Continuous integration for safety/quality\n",
    "4. **Plan for scale**: Design for 10x current load\n",
    "5. **Stay updated**: Keep up with latest optimization techniques\n",
    "\n",
    "### üîÆ Future Considerations\n",
    "\n",
    "**Emerging Trends:**\n",
    "- **Edge deployment**: Models running on mobile/edge devices\n",
    "- **Specialized chips**: TPUs, neuromorphic processors\n",
    "- **Advanced quantization**: Sub-8-bit, dynamic precision\n",
    "- **Model compression**: Pruning, distillation, architecture search\n",
    "\n",
    "You now have the knowledge to deploy transformer models at scale, safely and efficiently! This toolkit covers everything from optimization to monitoring, ensuring your AI systems are production-ready. üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}