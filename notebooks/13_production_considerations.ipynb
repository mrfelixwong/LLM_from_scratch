{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Considerations: From Research to Reality\n",
    "\n",
    "Building a transformer is only the beginning. Deploying it safely and efficiently in production requires mastering quantization, distributed systems, hardware optimization, and AI safety.\n",
    "\n",
    "## The Production Challenge\n",
    "\n",
    "Research models run once on clean data with unlimited time. Production models must:\n",
    "- **Serve millions of users** with millisecond latency\n",
    "- **Run on limited hardware** with strict memory constraints  \n",
    "- **Handle adversarial inputs** and generate safe outputs\n",
    "- **Scale efficiently** across multiple machines\n",
    "- **Cost pennies per request** while maintaining quality\n",
    "\n",
    "## The Physics of Production\n",
    "\n",
    "Production deployment is governed by fundamental trade-offs:\n",
    "\n",
    "**The Memory-Compute-Quality Triangle**:\n",
    "- **Memory**: Lower precision = less memory but potential quality loss\n",
    "- **Compute**: Parallelization speeds up inference but adds complexity\n",
    "- **Quality**: Aggressive optimization can degrade model performance\n",
    "\n",
    "**Amdahl's Law**: System speedup is limited by the slowest sequential component\n",
    "- Data loading, preprocessing, and postprocessing become bottlenecks\n",
    "- Perfect parallelization is impossible due to dependencies\n",
    "\n",
    "**Little's Law**: Average latency = Throughput × Average queue size\n",
    "- Higher load increases both queue size and latency\n",
    "- Capacity planning requires understanding this relationship\n",
    "\n",
    "## What You'll Master\n",
    "\n",
    "1. **Quantization**: Reduce model size 4-8x with minimal quality loss\n",
    "2. **Deployment strategies**: Single, batched, cached, and streaming inference\n",
    "3. **Distributed training**: Scale across hundreds of GPUs efficiently\n",
    "4. **Hardware optimization**: Extract maximum performance from available resources\n",
    "5. **Safety systems**: Deploy AI responsibly with comprehensive monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"Production deployment laboratory ready! 🏭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Quantization: The Art of Precision\n",
    "\n",
    "Quantization reduces model size and memory usage by using lower precision numbers. This is based on a key insight: neural networks are surprisingly robust to reduced precision.\n",
    "\n",
    "### The Mathematical Foundation\n",
    "\n",
    "**Floating-Point Representation**:\n",
    "- **FP32**: 32 bits = 1 sign + 8 exponent + 23 mantissa\n",
    "- **FP16**: 16 bits = 1 sign + 5 exponent + 10 mantissa  \n",
    "- **INT8**: 8 bits = 1 sign + 7 magnitude\n",
    "\n",
    "**Quantization Formula**:\n",
    "```\n",
    "quantized_value = round((float_value - zero_point) / scale)\n",
    "dequantized_value = quantized_value × scale + zero_point\n",
    "```\n",
    "\n",
    "### Why Quantization Works\n",
    "\n",
    "**Neural Network Robustness**: Networks learn distributed representations where:\n",
    "- Individual weight precision matters less than overall patterns\n",
    "- Redundancy across parameters provides error tolerance\n",
    "- Final predictions depend on aggregate activations, not individual weights\n",
    "\n",
    "**Quantization Error Statistics**: For Gaussian-distributed weights:\n",
    "- Quantization error is uniformly distributed\n",
    "- Error variance scales with quantization step size\n",
    "- Central Limit Theorem ensures errors tend to cancel out\n",
    "\n",
    "Let's implement and analyze different quantization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    \"\"\"Comprehensive model quantization toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.original_state = None\n",
    "    \n",
    "    def get_model_size_metrics(self, model=None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive model size metrics.\"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Calculate memory usage in bytes\n",
    "        param_memory = 0\n",
    "        for p in model.parameters():\n",
    "            param_memory += p.numel() * p.element_size()\n",
    "        \n",
    "        return {\n",
    "            'parameters': total_params,\n",
    "            'memory_bytes': param_memory,\n",
    "            'memory_mb': param_memory / (1024 * 1024),\n",
    "            'memory_gb': param_memory / (1024 * 1024 * 1024),\n",
    "            'params_per_mb': total_params / (param_memory / (1024 * 1024))\n",
    "        }\n",
    "    \n",
    "    def simulate_quantization_effects(self, target_bits=8) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate quantization effects and calculate quality metrics.\"\"\"\n",
    "        results = {'layer_errors': {}, 'overall_metrics': {}}\n",
    "        \n",
    "        # Get original model metrics\n",
    "        original_size = self.get_model_size_metrics()\n",
    "        \n",
    "        total_quantization_error = 0\n",
    "        total_weights = 0\n",
    "        \n",
    "        # Analyze quantization impact layer by layer\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.dim() > 1:  # Only quantize weight matrices\n",
    "                    # Calculate dynamic range for optimal quantization\n",
    "                    param_min = param.min().item()\n",
    "                    param_max = param.max().item()\n",
    "                    param_range = param_max - param_min\n",
    "                    \n",
    "                    # Quantization scale and zero point\n",
    "                    num_levels = 2 ** target_bits - 1\n",
    "                    scale = param_range / num_levels\n",
    "                    zero_point = param_min\n",
    "                    \n",
    "                    # Simulate quantization process\n",
    "                    quantized = torch.round((param - zero_point) / scale)\n",
    "                    quantized = torch.clamp(quantized, 0, num_levels)\n",
    "                    dequantized = quantized * scale + zero_point\n",
    "                    \n",
    "                    # Calculate quantization error metrics\n",
    "                    abs_error = (param - dequantized).abs()\n",
    "                    rel_error = abs_error / (param.abs() + 1e-8)\n",
    "                    \n",
    "                    layer_metrics = {\n",
    "                        'mean_abs_error': abs_error.mean().item(),\n",
    "                        'max_abs_error': abs_error.max().item(),\n",
    "                        'mean_rel_error': rel_error.mean().item(),\n",
    "                        'snr_db': 20 * torch.log10(param.std() / abs_error.std()).item(),\n",
    "                        'param_range': param_range,\n",
    "                        'quantization_scale': scale\n",
    "                    }\n",
    "                    \n",
    "                    results['layer_errors'][name] = layer_metrics\n",
    "                    \n",
    "                    # Accumulate for overall metrics\n",
    "                    total_quantization_error += abs_error.sum().item()\n",
    "                    total_weights += param.numel()\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        compression_ratio = 32 / target_bits  # Assuming FP32 to target_bits\n",
    "        new_memory = original_size['memory_bytes'] / compression_ratio\n",
    "        \n",
    "        results['overall_metrics'] = {\n",
    "            'original_size_mb': original_size['memory_mb'],\n",
    "            'quantized_size_mb': new_memory / (1024 * 1024),\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'size_reduction_percent': (1 - 1/compression_ratio) * 100,\n",
    "            'avg_quantization_error': total_quantization_error / total_weights,\n",
    "            'target_bits': target_bits\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_inference_performance(self, model, input_ids, num_runs=100) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive inference performance benchmarking.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Warmup phase - critical for accurate GPU benchmarking\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids)\n",
    "        \n",
    "        # Synchronize GPU operations for accurate timing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_ms = (total_time / num_runs) * 1000\n",
    "        \n",
    "        # Calculate throughput metrics\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_length = input_ids.shape[1]\n",
    "        \n",
    "        return {\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'throughput_samples_per_sec': (1000 / avg_time_ms) * batch_size,\n",
    "            'throughput_tokens_per_sec': (1000 / avg_time_ms) * batch_size * seq_length,\n",
    "            'total_benchmark_time_sec': total_time,\n",
    "            'memory_allocated_mb': torch.cuda.memory_allocated() / (1024*1024) if torch.cuda.is_available() else 0\n",
    "        }\n",
    "\n",
    "# Create and analyze a model for quantization experiments\n",
    "print(\"⚖️ MODEL QUANTIZATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up model for quantization analysis\n",
    "config = create_model_config(\"small\")\n",
    "tokenizer = create_tokenizer(\"simple\")\n",
    "config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "\n",
    "model = GPTModel(**config).to(device)\n",
    "quantizer = ModelQuantizer(model)\n",
    "\n",
    "print(f\"Model configuration: {config['n_layers']} layers, {config['d_model']} dimensions\")\n",
    "print(f\"Vocabulary size: {config['vocab_size']}\")\n",
    "\n",
    "# Create test input for benchmarking\n",
    "test_input = torch.randint(0, config[\"vocab_size\"], (4, 32)).to(device)\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "\n",
    "# Analyze original model\n",
    "original_metrics = quantizer.get_model_size_metrics()\n",
    "original_performance = quantizer.benchmark_inference_performance(model, test_input)\n",
    "\n",
    "print(f\"\\n📊 ORIGINAL MODEL (FP32):\")\n",
    "print(f\"  Parameters: {original_metrics['parameters']:,}\")\n",
    "print(f\"  Memory: {original_metrics['memory_mb']:.1f} MB\")\n",
    "print(f\"  Inference time: {original_performance['avg_inference_time_ms']:.2f} ms\")\n",
    "print(f\"  Throughput: {original_performance['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "\n",
    "print(\"\\nRunning quantization analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different quantization levels\n",
    "\n",
    "print(\"🔍 QUANTIZATION IMPACT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different bit widths\n",
    "bit_widths = [16, 8, 4]\n",
    "quantization_results = {}\n",
    "\n",
    "for bits in bit_widths:\n",
    "    print(f\"\\n🎯 Analyzing {bits}-bit quantization...\")\n",
    "    \n",
    "    # Simulate quantization effects\n",
    "    results = quantizer.simulate_quantization_effects(target_bits=bits)\n",
    "    quantization_results[bits] = results\n",
    "    \n",
    "    metrics = results['overall_metrics']\n",
    "    print(f\"  Compression ratio: {metrics['compression_ratio']:.1f}x\")\n",
    "    print(f\"  Size reduction: {metrics['size_reduction_percent']:.1f}%\")\n",
    "    print(f\"  Average quantization error: {metrics['avg_quantization_error']:.6f}\")\n",
    "    print(f\"  New size: {metrics['quantized_size_mb']:.1f} MB\")\n",
    "\n",
    "# Test FP16 precision if available\n",
    "print(f\"\\n🔄 Testing FP16 precision...\")\n",
    "try:\n",
    "    fp16_model = model.half().to(device)\n",
    "    fp16_input = test_input.to(device)  # Keep input as long for token indices\n",
    "    \n",
    "    fp16_metrics = quantizer.get_model_size_metrics(fp16_model)\n",
    "    fp16_performance = quantizer.benchmark_inference_performance(fp16_model, fp16_input)\n",
    "    \n",
    "    print(f\"✅ FP16 Results:\")\n",
    "    print(f\"  Memory: {fp16_metrics['memory_mb']:.1f} MB ({original_metrics['memory_mb']/fp16_metrics['memory_mb']:.1f}x smaller)\")\n",
    "    print(f\"  Inference: {fp16_performance['avg_inference_time_ms']:.2f} ms ({original_performance['avg_inference_time_ms']/fp16_performance['avg_inference_time_ms']:.1f}x faster)\")\n",
    "    print(f\"  Throughput: {fp16_performance['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    \n",
    "    # Store FP16 results for comparison\n",
    "    quantization_results['fp16'] = {\n",
    "        'overall_metrics': {\n",
    "            'compression_ratio': 2.0,\n",
    "            'size_reduction_percent': 50.0,\n",
    "            'quantized_size_mb': fp16_metrics['memory_mb'],\n",
    "            'speedup': original_performance['avg_inference_time_ms']/fp16_performance['avg_inference_time_ms']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ FP16 not supported: {e}\")\n",
    "    quantization_results['fp16'] = None\n",
    "\n",
    "print(f\"\\nQuantization analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization trade-offs with comprehensive analysis\n",
    "\n",
    "print(\"📊 VISUALIZING QUANTIZATION TRADE-OFFS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data for visualization\n",
    "precision_types = ['FP32 (Original)', 'FP16', 'INT8', 'INT4']\n",
    "compression_ratios = [1.0, 2.0, 4.0, 8.0]\n",
    "estimated_speedups = [1.0, 1.8, 2.5, 4.0]  # Realistic speedup estimates\n",
    "quality_retention = [100, 99.8, 97.5, 92.0]  # Estimated quality retention percentages\n",
    "memory_usage = [100, 50, 25, 12.5]  # Relative memory usage percentages\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Distinct colors for each precision\n",
    "\n",
    "# 1. Memory Usage Comparison\n",
    "bars1 = axes[0, 0].bar(precision_types, memory_usage, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_ylabel('Relative Memory Usage (%)', fontsize=12, weight='bold')\n",
    "axes[0, 0].set_title('Memory Efficiency by Precision Type', fontsize=14, weight='bold')\n",
    "axes[0, 0].set_ylim(0, 110)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, usage in zip(bars1, memory_usage):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                   f'{usage:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 2. Speed Improvements\n",
    "bars2 = axes[0, 1].bar(precision_types, estimated_speedups, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_ylabel('Relative Speed Improvement', fontsize=12, weight='bold')\n",
    "axes[0, 1].set_title('Inference Speed by Precision Type', fontsize=14, weight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, speedup in zip(bars2, estimated_speedups):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.08,\n",
    "                   f'{speedup:.1f}x', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 3. Quality Retention\n",
    "bars3 = axes[1, 0].bar(precision_types, quality_retention, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('Model Quality Retention (%)', fontsize=12, weight='bold')\n",
    "axes[1, 0].set_title('Quality Impact by Precision Type', fontsize=14, weight='bold')\n",
    "axes[1, 0].set_ylim(85, 101)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add quality threshold lines\n",
    "axes[1, 0].axhline(y=95, color='orange', linestyle='--', alpha=0.7, label='Acceptable Threshold (95%)')\n",
    "axes[1, 0].axhline(y=90, color='red', linestyle='--', alpha=0.7, label='Warning Threshold (90%)')\n",
    "axes[1, 0].legend(loc='lower left')\n",
    "\n",
    "for bar, quality in zip(bars3, quality_retention):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "                   f'{quality:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 4. Comprehensive Trade-off Analysis (Efficiency Score)\n",
    "# Calculate efficiency score: (Speed × Memory Savings) / Quality Loss\n",
    "efficiency_scores = []\n",
    "for i in range(len(precision_types)):\n",
    "    memory_savings = compression_ratios[i]\n",
    "    speed_gain = estimated_speedups[i]\n",
    "    quality_loss = 100 - quality_retention[i]\n",
    "    \n",
    "    # Efficiency formula: balance gains against quality loss\n",
    "    if quality_loss == 0:  # Avoid division by zero for FP32\n",
    "        efficiency = memory_savings * speed_gain\n",
    "    else:\n",
    "        efficiency = (memory_savings * speed_gain) / (1 + quality_loss/10)\n",
    "    \n",
    "    efficiency_scores.append(efficiency)\n",
    "\n",
    "bars4 = axes[1, 1].bar(precision_types, efficiency_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 1].set_ylabel('Efficiency Score', fontsize=12, weight='bold')\n",
    "axes[1, 1].set_title('Overall Efficiency Analysis\\n(Speed × Memory / Quality Loss)', fontsize=14, weight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars4, efficiency_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                   f'{score:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n",
    "\n",
    "# Print quantization recommendations\n",
    "print(\"\\n🎯 QUANTIZATION RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📋 Precision Selection Guide:\")\n",
    "print(\"  • FP32: Research and development, maximum quality\")\n",
    "print(\"  • FP16: Production deployment, excellent quality-performance balance\")\n",
    "print(\"  • INT8: Resource-constrained deployment, acceptable quality loss\")\n",
    "print(\"  • INT4: Edge devices only, significant quality degradation\")\n",
    "\n",
    "print(\"\\n💡 Best Practices:\")\n",
    "print(\"  • Start with FP16 for most production deployments\")\n",
    "print(\"  • Use calibration datasets for better INT8 quantization\")\n",
    "print(\"  • Always validate quality metrics after quantization\")\n",
    "print(\"  • Consider dynamic quantization for variable workloads\")\n",
    "print(\"  • Monitor inference accuracy in production\")\n",
    "\n",
    "print(f\"\\n📊 Summary Statistics:\")\n",
    "best_efficiency_idx = efficiency_scores.index(max(efficiency_scores))\n",
    "print(f\"  Best overall efficiency: {precision_types[best_efficiency_idx]}\")\n",
    "print(f\"  Maximum memory savings: {max(compression_ratios):.1f}x (INT4)\")\n",
    "print(f\"  Maximum speed improvement: {max(estimated_speedups):.1f}x (INT4)\")\n",
    "print(f\"  Minimum quality loss: {min([100-q for q in quality_retention]):.1f}% (FP16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deployment Strategies: Serving Models at Scale\n",
    "\n",
    "Different deployment strategies optimize for different constraints: latency, throughput, cost, or user experience.\n",
    "\n",
    "### The Physics of Model Serving\n",
    "\n",
    "**Little's Law in Practice**: L = λW\n",
    "- L = Average number of requests in system\n",
    "- λ = Request arrival rate\n",
    "- W = Average response time\n",
    "\n",
    "**Batching Benefits**: \n",
    "- GPU parallelism: Process multiple requests simultaneously\n",
    "- Memory efficiency: Amortize model loading costs\n",
    "- Throughput scaling: Linear improvement with batch size (up to memory limits)\n",
    "\n",
    "**Caching Theory**:\n",
    "- **Locality of reference**: Similar requests often repeat\n",
    "- **Cache hit ratio**: Percentage of requests served from cache\n",
    "- **Zipf distribution**: Popular requests follow power law (few queries dominate)\n",
    "\n",
    "**Streaming vs Batch Trade-offs**:\n",
    "- Streaming: Lower perceived latency, better UX, higher overhead\n",
    "- Batch: Higher throughput, lower cost, higher latency\n",
    "\n",
    "Let's implement and compare different deployment strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDeploymentSystem:\n",
    "    \"\"\"Comprehensive production deployment strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.request_cache = {}  # Simple LRU cache simulation\n",
    "        self.performance_history = []\n",
    "        self.cache_stats = {'hits': 0, 'misses': 0}\n",
    "    \n",
    "    def single_request_inference(self, text: str, generation_params: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single request with comprehensive metrics.\"\"\"\n",
    "        if generation_params is None:\n",
    "            generation_params = {'max_length': 50, 'temperature': 0.8, 'do_sample': True}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenization phase\n",
    "        tokenize_start = time.time()\n",
    "        try:\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "            input_ids = torch.tensor([tokens]).to(device)\n",
    "            tokenize_time = (time.time() - tokenize_start) * 1000\n",
    "        except Exception as e:\n",
    "            return {'error': f'Tokenization failed: {e}', 'success': False}\n",
    "        \n",
    "        # Generation phase\n",
    "        generation_start = time.time()\n",
    "        self.model.eval()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Simple generation (extend sequence token by token)\n",
    "                generated_ids = input_ids.clone()\n",
    "                max_new_tokens = min(generation_params.get('max_length', 20), 20)  # Limit for demo\n",
    "                \n",
    "                for _ in range(max_new_tokens):\n",
    "                    if generated_ids.size(1) >= self.model.max_seq_len:\n",
    "                        break\n",
    "                        \n",
    "                    outputs = self.model(generated_ids)\n",
    "                    logits = outputs[0, -1, :] / generation_params.get('temperature', 1.0)\n",
    "                    \n",
    "                    if generation_params.get('do_sample', True):\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    else:\n",
    "                        next_token = logits.argmax().unsqueeze(0)\n",
    "                    \n",
    "                    generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "            generation_time = (time.time() - generation_start) * 1000\n",
    "        except Exception as e:\n",
    "            return {'error': f'Generation failed: {e}', 'success': False}\n",
    "        \n",
    "        # Decoding phase\n",
    "        decode_start = time.time()\n",
    "        try:\n",
    "            generated_text = self.tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
    "            decode_time = (time.time() - decode_start) * 1000\n",
    "        except Exception as e:\n",
    "            return {'error': f'Decoding failed: {e}', 'success': False}\n",
    "        \n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        result = {\n",
    "            'success': True,\n",
    "            'input_text': text,\n",
    "            'output_text': generated_text,\n",
    "            'metrics': {\n",
    "                'total_latency_ms': total_time,\n",
    "                'tokenization_time_ms': tokenize_time,\n",
    "                'generation_time_ms': generation_time,\n",
    "                'decoding_time_ms': decode_time,\n",
    "                'input_tokens': len(tokens),\n",
    "                'output_tokens': generated_ids.size(1),\n",
    "                'tokens_per_second': (generated_ids.size(1) - len(tokens)) / (generation_time / 1000) if generation_time > 0 else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.performance_history.append(result['metrics'])\n",
    "        return result\n",
    "    \n",
    "    def batched_inference(self, texts: List[str], generation_params: Dict = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple requests in a single batch.\"\"\"\n",
    "        if generation_params is None:\n",
    "            generation_params = {'max_length': 50, 'temperature': 0.8}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize all inputs\n",
    "        tokenize_start = time.time()\n",
    "        all_tokens = []\n",
    "        max_input_len = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "                all_tokens.append(tokens)\n",
    "                max_input_len = max(max_input_len, len(tokens))\n",
    "            except Exception as e:\n",
    "                all_tokens.append([0])  # Fallback token\n",
    "        \n",
    "        # Pad sequences to same length\n",
    "        padded_tokens = []\n",
    "        for tokens in all_tokens:\n",
    "            padded = tokens + [0] * (max_input_len - len(tokens))  # Pad with 0\n",
    "            padded_tokens.append(padded)\n",
    "        \n",
    "        input_ids = torch.tensor(padded_tokens).to(device)\n",
    "        tokenize_time = (time.time() - tokenize_start) * 1000\n",
    "        \n",
    "        # Batch generation\n",
    "        generation_start = time.time()\n",
    "        self.model.eval()\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                generated_ids = input_ids.clone()\n",
    "                max_new_tokens = min(generation_params.get('max_length', 10), 10)  # Limit for demo\n",
    "                \n",
    "                for _ in range(max_new_tokens):\n",
    "                    if generated_ids.size(1) >= self.model.max_seq_len:\n",
    "                        break\n",
    "                        \n",
    "                    outputs = self.model(generated_ids)\n",
    "                    logits = outputs[:, -1, :] / generation_params.get('temperature', 1.0)\n",
    "                    \n",
    "                    # Sample next tokens for entire batch\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "                    \n",
    "                    generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n",
    "                \n",
    "            generation_time = (time.time() - generation_start) * 1000\n",
    "        except Exception as e:\n",
    "            # Return error for all requests\n",
    "            return [{'error': f'Batch generation failed: {e}', 'success': False} for _ in texts]\n",
    "        \n",
    "        # Decode all outputs\n",
    "        decode_start = time.time()\n",
    "        results = []\n",
    "        \n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        per_sample_time = total_time / len(texts)\n",
    "        \n",
    "        for i, (text, output_ids) in enumerate(zip(texts, generated_ids)):\n",
    "            try:\n",
    "                generated_text = self.tokenizer.decode(output_ids.tolist(), skip_special_tokens=True)\n",
    "                \n",
    "                result = {\n",
    "                    'success': True,\n",
    "                    'input_text': text,\n",
    "                    'output_text': generated_text,\n",
    "                    'metrics': {\n",
    "                        'batch_total_time_ms': total_time,\n",
    "                        'per_sample_time_ms': per_sample_time,\n",
    "                        'tokenization_time_ms': tokenize_time / len(texts),\n",
    "                        'generation_time_ms': generation_time / len(texts),\n",
    "                        'input_tokens': len(all_tokens[i]),\n",
    "                        'output_tokens': output_ids.size(0),\n",
    "                        'batch_size': len(texts)\n",
    "                    }\n",
    "                }\n",
    "                results.append(result)\n",
    "                self.performance_history.append(result['metrics'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({'error': f'Decoding failed: {e}', 'success': False})\n",
    "        \n",
    "        decode_time = (time.time() - decode_start) * 1000\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cached_inference(self, text: str, generation_params: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Inference with intelligent caching.\"\"\"\n",
    "        # Create cache key from input and parameters\n",
    "        cache_key = hash((text, str(sorted(generation_params.items()) if generation_params else \"\")))\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_key in self.request_cache:\n",
    "            self.cache_stats['hits'] += 1\n",
    "            cached_result = self.request_cache[cache_key].copy()\n",
    "            cached_result['metrics']['cache_hit'] = True\n",
    "            cached_result['metrics']['total_latency_ms'] = 0.5  # Minimal cache lookup time\n",
    "            return cached_result\n",
    "        \n",
    "        # Cache miss - compute normally\n",
    "        self.cache_stats['misses'] += 1\n",
    "        result = self.single_request_inference(text, generation_params)\n",
    "        \n",
    "        if result.get('success', False):\n",
    "            result['metrics']['cache_hit'] = False\n",
    "            # Store in cache (simple strategy - no LRU eviction for demo)\n",
    "            if len(self.request_cache) < 100:  # Limit cache size\n",
    "                self.request_cache[cache_key] = result.copy()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def streaming_simulation(self, text: str, callback=None) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate streaming inference with token-by-token generation.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "            input_ids = torch.tensor([tokens]).to(device)\n",
    "        except Exception as e:\n",
    "            return {'error': f'Tokenization failed: {e}', 'success': False}\n",
    "        \n",
    "        self.model.eval()\n",
    "        generated_tokens = []\n",
    "        current_ids = input_ids\n",
    "        \n",
    "        # Generate tokens one by one with streaming\n",
    "        try:\n",
    "            for step in range(15):  # Generate up to 15 tokens\n",
    "                if current_ids.size(1) >= self.model.max_seq_len:\n",
    "                    break\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(current_ids)\n",
    "                    logits = outputs[0, -1, :] / 0.8  # temperature\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    \n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    \n",
    "                    # Update sequence\n",
    "                    current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=1)\n",
    "                    \n",
    "                    # Simulate streaming callback\n",
    "                    if callback:\n",
    "                        partial_tokens = tokens + generated_tokens\n",
    "                        try:\n",
    "                            partial_text = self.tokenizer.decode(partial_tokens, skip_special_tokens=True)\n",
    "                            callback(step, partial_text, next_token.item())\n",
    "                        except:\n",
    "                            pass  # Skip decoding errors during streaming\n",
    "                    \n",
    "                    # Simulate streaming delay\n",
    "                    time.sleep(0.01)  # 10ms per token\n",
    "            \n",
    "            # Final result\n",
    "            final_tokens = tokens + generated_tokens\n",
    "            generated_text = self.tokenizer.decode(final_tokens, skip_special_tokens=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'Streaming generation failed: {e}', 'success': False}\n",
    "        \n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'input_text': text,\n",
    "            'output_text': generated_text,\n",
    "            'metrics': {\n",
    "                'total_latency_ms': total_time,\n",
    "                'tokens_generated': len(generated_tokens),\n",
    "                'streaming': True,\n",
    "                'time_to_first_token_ms': 15,  # Approximate\n",
    "                'tokens_per_second': len(generated_tokens) / (total_time / 1000) if total_time > 0 else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize deployment system\n",
    "print(\"🚀 PRODUCTION DEPLOYMENT STRATEGIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "deployment_system = ProductionDeploymentSystem(model, tokenizer)\n",
    "\n",
    "# Test data for deployment benchmarks\n",
    "test_texts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"Machine learning applications in\",\n",
    "    \"Deep neural networks can\",\n",
    "    \"Natural language processing enables\",\n",
    "    \"Computer vision systems detect\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_texts)} test queries for deployment analysis\")\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive deployment strategy benchmarking\n",
    "\n",
    "print(\"📊 DEPLOYMENT STRATEGY BENCHMARKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "# 1. Single Request Strategy\n",
    "print(\"\\n🔄 Testing Single Request Strategy...\")\n",
    "single_results = []\n",
    "single_start_time = time.time()\n",
    "\n",
    "for text in test_texts:\n",
    "    result = deployment_system.single_request_inference(text)\n",
    "    if result.get('success', False):\n",
    "        single_results.append(result['metrics'])\n",
    "\n",
    "single_total_time = (time.time() - single_start_time) * 1000\n",
    "\n",
    "if single_results:\n",
    "    benchmark_results['single'] = {\n",
    "        'avg_latency_ms': np.mean([r['total_latency_ms'] for r in single_results]),\n",
    "        'total_time_ms': single_total_time,\n",
    "        'throughput_req_per_sec': len(test_texts) / (single_total_time / 1000),\n",
    "        'success_rate': len(single_results) / len(test_texts),\n",
    "        'avg_tokens_per_sec': np.mean([r.get('tokens_per_second', 0) for r in single_results])\n",
    "    }\n",
    "    print(f\"  ✅ Completed: Avg latency {benchmark_results['single']['avg_latency_ms']:.1f}ms\")\n",
    "else:\n",
    "    print(f\"  ❌ Failed: No successful single requests\")\n",
    "\n",
    "# 2. Batched Strategy\n",
    "print(\"\\n🔄 Testing Batched Strategy...\")\n",
    "batch_start_time = time.time()\n",
    "batch_results = deployment_system.batched_inference(test_texts)\n",
    "batch_total_time = (time.time() - batch_start_time) * 1000\n",
    "\n",
    "successful_batch = [r for r in batch_results if r.get('success', False)]\n",
    "if successful_batch:\n",
    "    benchmark_results['batched'] = {\n",
    "        'avg_latency_ms': np.mean([r['metrics']['per_sample_time_ms'] for r in successful_batch]),\n",
    "        'total_time_ms': batch_total_time,\n",
    "        'throughput_req_per_sec': len(test_texts) / (batch_total_time / 1000),\n",
    "        'success_rate': len(successful_batch) / len(test_texts),\n",
    "        'batch_efficiency': single_total_time / batch_total_time if single_total_time > 0 else 1\n",
    "    }\n",
    "    print(f\"  ✅ Completed: Avg latency {benchmark_results['batched']['avg_latency_ms']:.1f}ms\")\n",
    "    print(f\"      Efficiency gain: {benchmark_results['batched']['batch_efficiency']:.1f}x faster than single\")\n",
    "else:\n",
    "    print(f\"  ❌ Failed: No successful batch requests\")\n",
    "\n",
    "# 3. Cached Strategy (simulate cache warming and hits)\n",
    "print(\"\\n🔄 Testing Cached Strategy...\")\n",
    "cached_results = []\n",
    "cached_start_time = time.time()\n",
    "\n",
    "# First pass - populate cache (cache misses)\n",
    "for text in test_texts:\n",
    "    result = deployment_system.cached_inference(text)\n",
    "    if result.get('success', False):\n",
    "        cached_results.append(result['metrics'])\n",
    "\n",
    "# Second pass - cache hits\n",
    "cache_hit_times = []\n",
    "for text in test_texts:\n",
    "    result = deployment_system.cached_inference(text)\n",
    "    if result.get('success', False):\n",
    "        cache_hit_times.append(result['metrics']['total_latency_ms'])\n",
    "\n",
    "cached_total_time = (time.time() - cached_start_time) * 1000\n",
    "\n",
    "if cached_results and cache_hit_times:\n",
    "    cache_hit_ratio = deployment_system.cache_stats['hits'] / (deployment_system.cache_stats['hits'] + deployment_system.cache_stats['misses'])\n",
    "    \n",
    "    benchmark_results['cached'] = {\n",
    "        'avg_latency_ms': np.mean(cache_hit_times),  # Focus on cache hit performance\n",
    "        'total_time_ms': cached_total_time,\n",
    "        'throughput_req_per_sec': (len(test_texts) * 2) / (cached_total_time / 1000),  # Two passes\n",
    "        'cache_hit_ratio': cache_hit_ratio,\n",
    "        'cache_speedup': np.mean([r['total_latency_ms'] for r in cached_results]) / np.mean(cache_hit_times) if cache_hit_times else 1\n",
    "    }\n",
    "    print(f\"  ✅ Completed: Cache hit latency {benchmark_results['cached']['avg_latency_ms']:.1f}ms\")\n",
    "    print(f\"      Cache hit ratio: {cache_hit_ratio:.1%}\")\n",
    "    print(f\"      Cache speedup: {benchmark_results['cached']['cache_speedup']:.0f}x faster for cached requests\")\n",
    "else:\n",
    "    print(f\"  ❌ Failed: No successful cached requests\")\n",
    "\n",
    "# 4. Streaming Strategy Demo\n",
    "print(\"\\n🔄 Testing Streaming Strategy...\")\n",
    "streaming_callbacks = []\n",
    "\n",
    "def streaming_callback(step, partial_text, token_id):\n",
    "    streaming_callbacks.append((step, len(partial_text), token_id))\n",
    "    if step < 3:  # Only show first few for demo\n",
    "        print(f\"    Token {step}: '{partial_text[-20:]}...' (ID: {token_id})\")\n",
    "\n",
    "streaming_result = deployment_system.streaming_simulation(\n",
    "    \"The future of technology will\", \n",
    "    callback=streaming_callback\n",
    ")\n",
    "\n",
    "if streaming_result.get('success', False):\n",
    "    benchmark_results['streaming'] = {\n",
    "        'total_latency_ms': streaming_result['metrics']['total_latency_ms'],\n",
    "        'time_to_first_token_ms': streaming_result['metrics']['time_to_first_token_ms'],\n",
    "        'tokens_per_sec': streaming_result['metrics']['tokens_per_second'],\n",
    "        'tokens_generated': streaming_result['metrics']['tokens_generated']\n",
    "    }\n",
    "    print(f\"  ✅ Completed: Total time {benchmark_results['streaming']['total_latency_ms']:.1f}ms\")\n",
    "    print(f\"      Time to first token: {benchmark_results['streaming']['time_to_first_token_ms']:.1f}ms\")\n",
    "    print(f\"      Generated {benchmark_results['streaming']['tokens_generated']} tokens\")\n",
    "else:\n",
    "    print(f\"  ❌ Failed: Streaming request failed\")\n",
    "\n",
    "print(f\"\\n📈 BENCHMARK SUMMARY:\")\n",
    "print(f\"Successfully tested {len(benchmark_results)} deployment strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deployment strategy performance\n",
    "\n",
    "print(\"📊 DEPLOYMENT STRATEGY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract data for visualization\n",
    "strategies = []\n",
    "latencies = []\n",
    "throughputs = []\n",
    "efficiency_scores = []\n",
    "\n",
    "for strategy, metrics in benchmark_results.items():\n",
    "    strategies.append(strategy.capitalize())\n",
    "    latencies.append(metrics.get('avg_latency_ms', 0))\n",
    "    throughputs.append(metrics.get('throughput_req_per_sec', 0))\n",
    "    \n",
    "    # Calculate efficiency score (throughput / latency)\n",
    "    if metrics.get('avg_latency_ms', 0) > 0:\n",
    "        efficiency = metrics.get('throughput_req_per_sec', 0) / metrics.get('avg_latency_ms', 1)\n",
    "    else:\n",
    "        efficiency = 0\n",
    "    efficiency_scores.append(efficiency)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color scheme for different strategies\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# 1. Latency Comparison\n",
    "bars1 = axes[0, 0].bar(strategies, latencies, color=colors[:len(strategies)], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_ylabel('Average Latency (ms)', fontsize=12, weight='bold')\n",
    "axes[0, 0].set_title('Latency by Deployment Strategy\\n(Lower is Better)', fontsize=14, weight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, latency in zip(bars1, latencies):\n",
    "    if latency > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + max(latencies) * 0.02,\n",
    "                       f'{latency:.1f}ms', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 2. Throughput Comparison\n",
    "bars2 = axes[0, 1].bar(strategies, throughputs, color=colors[:len(strategies)], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_ylabel('Throughput (requests/sec)', fontsize=12, weight='bold')\n",
    "axes[0, 1].set_title('Throughput by Deployment Strategy\\n(Higher is Better)', fontsize=14, weight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, throughput in zip(bars2, throughputs):\n",
    "    if throughput > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + max(throughputs) * 0.02,\n",
    "                       f'{throughput:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 3. Efficiency Score (Throughput/Latency)\n",
    "bars3 = axes[1, 0].bar(strategies, efficiency_scores, color=colors[:len(strategies)], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('Efficiency Score (req/sec/ms)', fontsize=12, weight='bold')\n",
    "axes[1, 0].set_title('Efficiency by Strategy\\n(Throughput/Latency)', fontsize=14, weight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, efficiency in zip(bars3, efficiency_scores):\n",
    "    if efficiency > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + max(efficiency_scores) * 0.02,\n",
    "                       f'{efficiency:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 4. Strategy Comparison Matrix\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "headers = ['Strategy', 'Latency (ms)', 'Throughput (req/s)', 'Best Use Case']\n",
    "\n",
    "use_cases = {\n",
    "    'Single': 'Low traffic, simple setup',\n",
    "    'Batched': 'High traffic, cost efficiency',\n",
    "    'Cached': 'Repeated queries',\n",
    "    'Streaming': 'Long responses, UX'\n",
    "}\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "    row = [\n",
    "        strategy,\n",
    "        f'{latencies[i]:.1f}' if latencies[i] > 0 else 'N/A',\n",
    "        f'{throughputs[i]:.1f}' if throughputs[i] > 0 else 'N/A',\n",
    "        use_cases.get(strategy, 'General purpose')\n",
    "    ]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create table\n",
    "table = axes[1, 1].table(cellText=comparison_data, colLabels=headers,\n",
    "                        cellLoc='center', loc='center',\n",
    "                        colWidths=[0.2, 0.2, 0.25, 0.35])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.8)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(headers)):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(comparison_data) + 1):\n",
    "    for j in range(len(headers)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "axes[1, 1].set_title('Deployment Strategy Comparison', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\n🎯 DEPLOYMENT STRATEGY INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find best strategy for each metric\n",
    "if latencies:\n",
    "    best_latency = strategies[np.argmin([l for l in latencies if l > 0])]\n",
    "    print(f\"⚡ Lowest Latency: {best_latency}\")\n",
    "\n",
    "if throughputs:\n",
    "    best_throughput = strategies[np.argmax(throughputs)]\n",
    "    print(f\"🚀 Highest Throughput: {best_throughput}\")\n",
    "\n",
    "if efficiency_scores:\n",
    "    best_efficiency = strategies[np.argmax(efficiency_scores)]\n",
    "    print(f\"⚖️ Best Efficiency: {best_efficiency}\")\n",
    "\n",
    "print(\"\\n💡 PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"  • Single: Use for development/testing or very low traffic\")\n",
    "print(\"  • Batched: Optimal for high-throughput production systems\")\n",
    "print(\"  • Cached: Essential for applications with repeated queries\")\n",
    "print(\"  • Streaming: Better user experience for long text generation\")\n",
    "print(\"\\n🏗️ HYBRID APPROACH: Combine batching + caching for optimal production performance\")\n",
    "\n",
    "# Cache statistics\n",
    "if deployment_system.cache_stats['hits'] + deployment_system.cache_stats['misses'] > 0:\n",
    "    total_cache_requests = deployment_system.cache_stats['hits'] + deployment_system.cache_stats['misses']\n",
    "    hit_rate = deployment_system.cache_stats['hits'] / total_cache_requests\n",
    "    print(f\"\\n📊 Cache Performance:\")\n",
    "    print(f\"  • Hit rate: {hit_rate:.1%}\")\n",
    "    print(f\"  • Total requests: {total_cache_requests}\")\n",
    "    print(f\"  • Cache hits: {deployment_system.cache_stats['hits']}\")\n",
    "    print(f\"  • Cache misses: {deployment_system.cache_stats['misses']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Training: Scaling Beyond Single Machines\n",
    "\n",
    "Training large models requires distributing computation across multiple GPUs and machines. This involves sophisticated parallelization strategies.\n",
    "\n",
    "### The Mathematics of Parallelization\n",
    "\n",
    "**Amdahl's Law**: Speedup = 1 / (S + P/N)\n",
    "- S = Sequential fraction of work\n",
    "- P = Parallelizable fraction  \n",
    "- N = Number of processors\n",
    "\n",
    "**Communication Overhead**: As you add more GPUs:\n",
    "- **All-reduce complexity**: O(N) for naive, O(log N) for tree-reduce\n",
    "- **Bandwidth requirements**: Scale with model size and gradient frequency\n",
    "- **Synchronization costs**: Increase with number of workers\n",
    "\n",
    "### Parallelization Strategies\n",
    "\n",
    "**Data Parallel**: Replicate model on each GPU, split data\n",
    "- **Memory**: Each GPU needs full model + gradients\n",
    "- **Communication**: All-reduce gradients after each batch\n",
    "- **Scaling limit**: GPU memory size\n",
    "\n",
    "**Model Parallel**: Split model layers across GPUs\n",
    "- **Memory**: Each GPU holds subset of model\n",
    "- **Communication**: Forward/backward activations between layers\n",
    "- **Challenge**: Pipeline bubbles reduce utilization\n",
    "\n",
    "**Tensor Parallel**: Split individual operations across GPUs\n",
    "- **Memory**: Divide weight matrices across GPUs\n",
    "- **Communication**: All-reduce within each layer\n",
    "- **Requirement**: High-bandwidth interconnects (NVLink)\n",
    "\n",
    "**3D Parallel**: Combines data + model + tensor parallelism\n",
    "- **Complexity**: Requires careful coordination\n",
    "- **Benefit**: Scales to thousands of GPUs\n",
    "- **Used by**: GPT-3, PaLM, and other large models\n",
    "\n",
    "Let's analyze distributed training strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedTrainingAnalyzer:\n",
    "    \"\"\"Analyze and compare distributed training strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define characteristics of different parallelization strategies\n",
    "        self.strategies = {\n",
    "            'Data Parallel': {\n",
    "                'description': 'Replicate model on each GPU, split batch across GPUs',\n",
    "                'memory_per_gpu': 'Full model + optimizer states + gradients',\n",
    "                'communication_pattern': 'All-reduce gradients after backward pass',\n",
    "                'communication_volume': 'Model size per step',\n",
    "                'efficiency_factor': 0.85,  # Account for communication overhead\n",
    "                'scaling_limit': 'GPU memory (model must fit on single GPU)',\n",
    "                'implementation_complexity': 'Low',\n",
    "                'optimal_use_case': 'Models that fit on single GPU, high throughput training'\n",
    "            },\n",
    "            'Model Parallel': {\n",
    "                'description': 'Split model layers across GPUs sequentially',\n",
    "                'memory_per_gpu': 'Subset of model layers',\n",
    "                'communication_pattern': 'Forward activations, backward gradients between adjacent GPUs',\n",
    "                'communication_volume': 'Activation size per layer',\n",
    "                'efficiency_factor': 0.65,  # Significant pipeline bubbles\n",
    "                'scaling_limit': 'Number of layers (diminishing returns)',\n",
    "                'implementation_complexity': 'Medium',\n",
    "                'optimal_use_case': 'Very large models that don\\'t fit on single GPU'\n",
    "            },\n",
    "            'Pipeline Parallel': {\n",
    "                'description': 'Model parallel + micro-batching to reduce bubbles',\n",
    "                'memory_per_gpu': 'Subset of model + multiple micro-batch activations',\n",
    "                'communication_pattern': 'Pipelined activations and gradients',\n",
    "                'communication_volume': 'Activation size × pipeline depth',\n",
    "                'efficiency_factor': 0.78,  # Better than naive model parallel\n",
    "                'scaling_limit': 'Pipeline depth vs memory trade-off',\n",
    "                'implementation_complexity': 'High',\n",
    "                'optimal_use_case': 'Large models with sufficient batch size for micro-batching'\n",
    "            },\n",
    "            'Tensor Parallel': {\n",
    "                'description': 'Split individual weight matrices across GPUs',\n",
    "                'memory_per_gpu': 'Fraction of each layer (1/N of model)',\n",
    "                'communication_pattern': 'All-reduce within each layer operation',\n",
    "                'communication_volume': 'Activation size per layer',\n",
    "                'efficiency_factor': 0.90,  # High efficiency with fast interconnects\n",
    "                'scaling_limit': 'Interconnect bandwidth (requires NVLink/InfiniBand)',\n",
    "                'implementation_complexity': 'High',\n",
    "                'optimal_use_case': 'Large models with high-bandwidth GPU interconnects'\n",
    "            },\n",
    "            '3D Parallel': {\n",
    "                'description': 'Combines data, model, and tensor parallelism',\n",
    "                'memory_per_gpu': 'Minimal (distributed across all dimensions)',\n",
    "                'communication_pattern': 'Hierarchical: tensor → pipeline → data replication',\n",
    "                'communication_volume': 'Optimized across all dimensions',\n",
    "                'efficiency_factor': 0.95,  # Highest efficiency for large scale\n",
    "                'scaling_limit': 'Communication topology and coordination complexity',\n",
    "                'implementation_complexity': 'Very High',\n",
    "                'optimal_use_case': 'Massive models (100B+ parameters) on hundreds of GPUs'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # GPU specifications for analysis\n",
    "        self.gpu_specs = {\n",
    "            'memory_gb': 80,  # A100 GPU memory\n",
    "            'compute_tflops': 312,  # A100 tensor FLOPS (FP16)\n",
    "            'memory_bandwidth_gbps': 2000,  # HBM2e bandwidth\n",
    "            'nvlink_bandwidth_gbps': 600,  # NVLink 3.0 per GPU\n",
    "            'infiniband_bandwidth_gbps': 200  # HDR InfiniBand\n",
    "        }\n",
    "    \n",
    "    def estimate_memory_requirements(self, model_params: float, strategy: str, num_gpus: int) -> Dict[str, float]:\n",
    "        \"\"\"Estimate memory requirements for different strategies.\"\"\"\n",
    "        \n",
    "        # Base memory calculations (in GB)\n",
    "        model_memory = model_params * 4 / (1024**3)  # FP32 weights\n",
    "        optimizer_memory = model_memory * 2  # Adam: momentum + variance\n",
    "        gradient_memory = model_memory  # Gradient storage\n",
    "        \n",
    "        if strategy == 'Data Parallel':\n",
    "            # Each GPU holds full model + optimizer + gradients\n",
    "            memory_per_gpu = model_memory + optimizer_memory + gradient_memory\n",
    "            max_model_size = self.gpu_specs['memory_gb'] * 0.8 / 3  # 80% utilization, 3x overhead\n",
    "            \n",
    "        elif strategy in ['Model Parallel', 'Pipeline Parallel']:\n",
    "            # Model split across GPUs, but activations and optimizer distributed\n",
    "            memory_per_gpu = (model_memory + optimizer_memory + gradient_memory) / num_gpus\n",
    "            # Add activation memory (varies with sequence length and batch size)\n",
    "            activation_memory = 0.1 * model_memory  # Rough estimate\n",
    "            memory_per_gpu += activation_memory\n",
    "            max_model_size = self.gpu_specs['memory_gb'] * num_gpus * 0.8 / 3\n",
    "            \n",
    "        elif strategy == 'Tensor Parallel':\n",
    "            # Each layer split across GPUs\n",
    "            memory_per_gpu = (model_memory + optimizer_memory + gradient_memory) / num_gpus\n",
    "            # Minimal activation overhead due to immediate all-reduce\n",
    "            memory_per_gpu += 0.05 * model_memory\n",
    "            max_model_size = self.gpu_specs['memory_gb'] * num_gpus * 0.9 / 3  # Better memory efficiency\n",
    "            \n",
    "        elif strategy == '3D Parallel':\n",
    "            # Optimal distribution across all dimensions\n",
    "            # Assume 2D data parallelism × 2D model parallelism for simplicity\n",
    "            effective_model_split = min(num_gpus, 8)  # Reasonable tensor parallel degree\n",
    "            memory_per_gpu = (model_memory + optimizer_memory + gradient_memory) / effective_model_split\n",
    "            memory_per_gpu += 0.02 * model_memory  # Minimal overhead with optimal partitioning\n",
    "            max_model_size = self.gpu_specs['memory_gb'] * num_gpus * 0.95 / 2  # Most efficient\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        return {\n",
    "            'memory_per_gpu_gb': memory_per_gpu,\n",
    "            'total_memory_gb': memory_per_gpu * num_gpus,\n",
    "            'memory_utilization': memory_per_gpu / self.gpu_specs['memory_gb'],\n",
    "            'fits_in_memory': memory_per_gpu <= self.gpu_specs['memory_gb'] * 0.95,\n",
    "            'max_model_size_params': max_model_size * (1024**3) / 4,  # Convert back to parameter count\n",
    "            'efficiency': 1.0 - max(0, (memory_per_gpu - self.gpu_specs['memory_gb'] * 0.8) / (self.gpu_specs['memory_gb'] * 0.2))\n",
    "        }\n",
    "    \n",
    "    def estimate_training_performance(self, model_params: float, strategy: str, num_gpus: int, \n",
    "                                    batch_size: int = 32, sequence_length: int = 2048) -> Dict[str, float]:\n",
    "        \"\"\"Estimate training performance for different strategies.\"\"\"\n",
    "        \n",
    "        memory_analysis = self.estimate_memory_requirements(model_params, strategy, num_gpus)\n",
    "        \n",
    "        if not memory_analysis['fits_in_memory']:\n",
    "            return {'error': 'Model does not fit in GPU memory with this strategy'}\n",
    "        \n",
    "        # Estimate computation requirements\n",
    "        # Transformer training: ~6 FLOPS per parameter per token (forward + backward)\n",
    "        flops_per_step = 6 * model_params * batch_size * sequence_length\n",
    "        \n",
    "        # Get strategy-specific efficiency\n",
    "        strategy_efficiency = self.strategies[strategy]['efficiency_factor']\n",
    "        \n",
    "        # Calculate effective compute\n",
    "        total_tflops = self.gpu_specs['compute_tflops'] * num_gpus * strategy_efficiency\n",
    "        compute_time_ms = (flops_per_step / (total_tflops * 1e12)) * 1000\n",
    "        \n",
    "        # Estimate communication overhead\n",
    "        communication_time_ms = self._estimate_communication_time(model_params, strategy, num_gpus, batch_size)\n",
    "        \n",
    "        # Total step time\n",
    "        total_step_time_ms = compute_time_ms + communication_time_ms\n",
    "        \n",
    "        # Calculate throughput metrics\n",
    "        samples_per_sec = (1000 / total_step_time_ms) * batch_size if total_step_time_ms > 0 else 0\n",
    "        tokens_per_sec = samples_per_sec * sequence_length\n",
    "        \n",
    "        return {\n",
    "            'step_time_ms': total_step_time_ms,\n",
    "            'compute_time_ms': compute_time_ms,\n",
    "            'communication_time_ms': communication_time_ms,\n",
    "            'samples_per_sec': samples_per_sec,\n",
    "            'tokens_per_sec': tokens_per_sec,\n",
    "            'efficiency': strategy_efficiency,\n",
    "            'communication_overhead': communication_time_ms / total_step_time_ms if total_step_time_ms > 0 else 0,\n",
    "            'scaling_efficiency': total_tflops / (self.gpu_specs['compute_tflops'] * num_gpus),\n",
    "            'memory_efficiency': memory_analysis['efficiency']\n",
    "        }\n",
    "    \n",
    "    def _estimate_communication_time(self, model_params: float, strategy: str, num_gpus: int, batch_size: int) -> float:\n",
    "        \"\"\"Estimate communication time for different strategies.\"\"\"\n",
    "        \n",
    "        model_size_gb = model_params * 4 / (1024**3)  # FP32 in GB\n",
    "        \n",
    "        if strategy == 'Data Parallel':\n",
    "            # All-reduce gradients (model size)\n",
    "            data_volume_gb = model_size_gb\n",
    "            # Use InfiniBand for inter-node communication\n",
    "            bandwidth_gbps = self.gpu_specs['infiniband_bandwidth_gbps']\n",
    "            # All-reduce has 2x overhead (scatter + gather)\n",
    "            comm_time_ms = (data_volume_gb * 2 / bandwidth_gbps) * 1000\n",
    "            \n",
    "        elif strategy in ['Model Parallel', 'Pipeline Parallel']:\n",
    "            # Forward/backward activations\n",
    "            # Rough estimate: activation size ≈ sqrt(model_params) * batch_size\n",
    "            activation_size_gb = (model_params ** 0.5) * batch_size * 4 / (1024**3)\n",
    "            bandwidth_gbps = self.gpu_specs['nvlink_bandwidth_gbps']\n",
    "            comm_time_ms = (activation_size_gb / bandwidth_gbps) * 1000\n",
    "            \n",
    "        elif strategy == 'Tensor Parallel':\n",
    "            # All-reduce activations within each layer\n",
    "            # Estimate: ~10% of model size per step\n",
    "            data_volume_gb = model_size_gb * 0.1\n",
    "            bandwidth_gbps = self.gpu_specs['nvlink_bandwidth_gbps']\n",
    "            comm_time_ms = (data_volume_gb * 2 / bandwidth_gbps) * 1000  # All-reduce overhead\n",
    "            \n",
    "        elif strategy == '3D Parallel':\n",
    "            # Optimized combination of all communication patterns\n",
    "            # Assume sophisticated optimization reduces overhead\n",
    "            data_volume_gb = model_size_gb * 0.05  # Much more efficient\n",
    "            bandwidth_gbps = self.gpu_specs['nvlink_bandwidth_gbps']\n",
    "            comm_time_ms = (data_volume_gb / bandwidth_gbps) * 1000\n",
    "            \n",
    "        else:\n",
    "            comm_time_ms = 0\n",
    "        \n",
    "        return comm_time_ms\n",
    "    \n",
    "    def compare_strategies(self, model_params: float, gpu_counts: List[int]) -> Dict[str, Dict]:\n",
    "        \"\"\"Compare all strategies across different GPU counts.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in self.strategies.keys():\n",
    "            results[strategy] = {}\n",
    "            \n",
    "            for gpu_count in gpu_counts:\n",
    "                try:\n",
    "                    performance = self.estimate_training_performance(model_params, strategy, gpu_count)\n",
    "                    memory = self.estimate_memory_requirements(model_params, strategy, gpu_count)\n",
    "                    \n",
    "                    results[strategy][gpu_count] = {\n",
    "                        **performance,\n",
    "                        **memory,\n",
    "                        'strategy_info': self.strategies[strategy]\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    results[strategy][gpu_count] = {'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize distributed training analyzer\n",
    "print(\"🔀 DISTRIBUTED TRAINING STRATEGY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "dist_analyzer = DistributedTrainingAnalyzer()\n",
    "\n",
    "# Analyze a 7B parameter model (similar to LLaMA-7B)\n",
    "model_size_params = 7e9  # 7 billion parameters\n",
    "gpu_counts_to_test = [1, 8, 16, 32, 64]\n",
    "\n",
    "print(f\"Analyzing {model_size_params/1e9:.0f}B parameter model\")\n",
    "print(f\"GPU counts to test: {gpu_counts_to_test}\")\n",
    "print(f\"GPU specs: {dist_analyzer.gpu_specs['memory_gb']}GB memory, {dist_analyzer.gpu_specs['compute_tflops']} TFLOPS\")\n",
    "\n",
    "# Run comprehensive comparison\n",
    "print(\"\\nRunning distributed training analysis...\")\n",
    "comparison_results = dist_analyzer.compare_strategies(model_size_params, gpu_counts_to_test)\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributed training analysis\n",
    "\n",
    "print(\"📊 DISTRIBUTED TRAINING PERFORMANCE VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Color map for different strategies\n",
    "strategy_colors = {'Data Parallel': '#1f77b4', 'Model Parallel': '#ff7f0e', \n",
    "                  'Pipeline Parallel': '#2ca02c', 'Tensor Parallel': '#d62728', \n",
    "                  '3D Parallel': '#9467bd'}\n",
    "\n",
    "# Extract data for plotting\n",
    "strategies = list(comparison_results.keys())\n",
    "gpu_counts = gpu_counts_to_test\n",
    "\n",
    "# 1. Memory usage per GPU vs GPU count\n",
    "for strategy in strategies:\n",
    "    memory_usage = []\n",
    "    valid_gpus = []\n",
    "    \n",
    "    for gpu_count in gpu_counts:\n",
    "        result = comparison_results[strategy].get(gpu_count, {})\n",
    "        if 'memory_per_gpu_gb' in result and result['fits_in_memory']:\n",
    "            memory_usage.append(result['memory_per_gpu_gb'])\n",
    "            valid_gpus.append(gpu_count)\n",
    "    \n",
    "    if memory_usage:\n",
    "        axes[0, 0].plot(valid_gpus, memory_usage, 'o-', \n",
    "                       label=strategy, linewidth=2, markersize=6,\n",
    "                       color=strategy_colors.get(strategy, 'gray'))\n",
    "\n",
    "axes[0, 0].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='GPU Memory Limit')\n",
    "axes[0, 0].set_xlabel('Number of GPUs')\n",
    "axes[0, 0].set_ylabel('Memory per GPU (GB)')\n",
    "axes[0, 0].set_title('Memory Usage Scaling', fontsize=14, weight='bold')\n",
    "axes[0, 0].set_xscale('log', base=2)\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training throughput (samples/sec) vs GPU count\n",
    "for strategy in strategies:\n",
    "    throughputs = []\n",
    "    valid_gpus = []\n",
    "    \n",
    "    for gpu_count in gpu_counts:\n",
    "        result = comparison_results[strategy].get(gpu_count, {})\n",
    "        if 'samples_per_sec' in result and result.get('fits_in_memory', False):\n",
    "            throughputs.append(result['samples_per_sec'])\n",
    "            valid_gpus.append(gpu_count)\n",
    "    \n",
    "    if throughputs:\n",
    "        axes[0, 1].plot(valid_gpus, throughputs, 'o-', \n",
    "                       label=strategy, linewidth=2, markersize=6,\n",
    "                       color=strategy_colors.get(strategy, 'gray'))\n",
    "\n",
    "axes[0, 1].set_xlabel('Number of GPUs')\n",
    "axes[0, 1].set_ylabel('Training Throughput (samples/sec)')\n",
    "axes[0, 1].set_title('Throughput Scaling', fontsize=14, weight='bold')\n",
    "axes[0, 1].set_xscale('log', base=2)\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Communication overhead vs GPU count\n",
    "for strategy in strategies:\n",
    "    comm_overheads = []\n",
    "    valid_gpus = []\n",
    "    \n",
    "    for gpu_count in gpu_counts:\n",
    "        result = comparison_results[strategy].get(gpu_count, {})\n",
    "        if 'communication_overhead' in result and result.get('fits_in_memory', False):\n",
    "            comm_overheads.append(result['communication_overhead'] * 100)  # Convert to percentage\n",
    "            valid_gpus.append(gpu_count)\n",
    "    \n",
    "    if comm_overheads:\n",
    "        axes[0, 2].plot(valid_gpus, comm_overheads, 'o-', \n",
    "                       label=strategy, linewidth=2, markersize=6,\n",
    "                       color=strategy_colors.get(strategy, 'gray'))\n",
    "\n",
    "axes[0, 2].set_xlabel('Number of GPUs')\n",
    "axes[0, 2].set_ylabel('Communication Overhead (%)')\n",
    "axes[0, 2].set_title('Communication Overhead Scaling', fontsize=14, weight='bold')\n",
    "axes[0, 2].set_xscale('log', base=2)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scaling efficiency vs GPU count\n",
    "for strategy in strategies:\n",
    "    efficiencies = []\n",
    "    valid_gpus = []\n",
    "    \n",
    "    for gpu_count in gpu_counts:\n",
    "        result = comparison_results[strategy].get(gpu_count, {})\n",
    "        if 'scaling_efficiency' in result and result.get('fits_in_memory', False):\n",
    "            efficiencies.append(result['scaling_efficiency'] * 100)  # Convert to percentage\n",
    "            valid_gpus.append(gpu_count)\n",
    "    \n",
    "    if efficiencies:\n",
    "        axes[1, 0].plot(valid_gpus, efficiencies, 'o-', \n",
    "                       label=strategy, linewidth=2, markersize=6,\n",
    "                       color=strategy_colors.get(strategy, 'gray'))\n",
    "\n",
    "axes[1, 0].set_xlabel('Number of GPUs')\n",
    "axes[1, 0].set_ylabel('Scaling Efficiency (%)')\n",
    "axes[1, 0].set_title('Parallel Efficiency', fontsize=14, weight='bold')\n",
    "axes[1, 0].set_xscale('log', base=2)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Strategy comparison at 32 GPUs\n",
    "gpu_count_for_comparison = 32\n",
    "strategy_names = []\n",
    "step_times = []\n",
    "memory_usage = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    result = comparison_results[strategy].get(gpu_count_for_comparison, {})\n",
    "    if 'step_time_ms' in result and result.get('fits_in_memory', False):\n",
    "        strategy_names.append(strategy)\n",
    "        step_times.append(result['step_time_ms'])\n",
    "        memory_usage.append(result['memory_per_gpu_gb'])\n",
    "\n",
    "if strategy_names:\n",
    "    bars = axes[1, 1].bar(strategy_names, step_times, \n",
    "                         color=[strategy_colors.get(s, 'gray') for s in strategy_names],\n",
    "                         alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    axes[1, 1].set_ylabel('Step Time (ms)')\n",
    "    axes[1, 1].set_title(f'Training Speed at {gpu_count_for_comparison} GPUs\\n(Lower is Better)', fontsize=14, weight='bold')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time_val in zip(bars, step_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(step_times) * 0.02,\n",
    "                       f'{time_val:.1f}ms', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 6. Strategy characteristics table\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Create summary table\n",
    "table_data = []\n",
    "headers = ['Strategy', 'Memory\\nEfficiency', 'Communication\\nOverhead', 'Complexity']\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_info = dist_analyzer.strategies[strategy]\n",
    "    # Get representative values from 32 GPU results\n",
    "    result = comparison_results[strategy].get(32, {})\n",
    "    \n",
    "    memory_eff = 'High' if result.get('memory_efficiency', 0) > 0.8 else 'Medium' if result.get('memory_efficiency', 0) > 0.6 else 'Low'\n",
    "    comm_overhead = 'Low' if result.get('communication_overhead', 1) < 0.1 else 'Medium' if result.get('communication_overhead', 1) < 0.3 else 'High'\n",
    "    complexity = strategy_info['implementation_complexity']\n",
    "    \n",
    "    table_data.append([strategy, memory_eff, comm_overhead, complexity])\n",
    "\n",
    "table = axes[1, 2].table(cellText=table_data, colLabels=headers,\n",
    "                        cellLoc='center', loc='center',\n",
    "                        colWidths=[0.35, 0.2, 0.25, 0.2])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.0, 2.0)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(headers)):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color code cells based on values\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(len(headers)):\n",
    "        cell_text = table_data[i-1][j]\n",
    "        if cell_text == 'High' and j in [1]:  # Memory efficiency - high is good\n",
    "            table[(i, j)].set_facecolor('#e8f5e8')\n",
    "        elif cell_text == 'Low' and j in [2]:  # Communication overhead - low is good\n",
    "            table[(i, j)].set_facecolor('#e8f5e8')\n",
    "        elif cell_text == 'High' and j in [2]:  # Communication overhead - high is bad\n",
    "            table[(i, j)].set_facecolor('#fce8e8')\n",
    "        elif cell_text == 'Very High':\n",
    "            table[(i, j)].set_facecolor('#fce8e8')\n",
    "\n",
    "axes[1, 2].set_title('Strategy Characteristics Summary', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistributed training visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis and recommendations\n",
    "\n",
    "print(\"🎯 DISTRIBUTED TRAINING STRATEGY RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze results for different scenarios\n",
    "scenarios = {\n",
    "    'Small Model (1B params)': 1e9,\n",
    "    'Medium Model (7B params)': 7e9,\n",
    "    'Large Model (70B params)': 70e9\n",
    "}\n",
    "\n",
    "for scenario_name, model_size in scenarios.items():\n",
    "    print(f\"\\n📊 {scenario_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with 32 GPUs as representative configuration\n",
    "    test_gpu_count = 32\n",
    "    \n",
    "    best_strategies = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        # Quick analysis for this model size\n",
    "        try:\n",
    "            memory_req = dist_analyzer.estimate_memory_requirements(model_size, strategy, test_gpu_count)\n",
    "            performance = dist_analyzer.estimate_training_performance(model_size, strategy, test_gpu_count)\n",
    "            \n",
    "            if memory_req['fits_in_memory'] and 'step_time_ms' in performance:\n",
    "                efficiency_score = (\n",
    "                    performance['scaling_efficiency'] * 0.4 +  # Scaling efficiency\n",
    "                    (1 - performance['communication_overhead']) * 0.3 +  # Low communication overhead\n",
    "                    memory_req['efficiency'] * 0.3  # Memory efficiency\n",
    "                )\n",
    "                \n",
    "                best_strategies.append({\n",
    "                    'strategy': strategy,\n",
    "                    'efficiency_score': efficiency_score,\n",
    "                    'step_time_ms': performance['step_time_ms'],\n",
    "                    'memory_per_gpu': memory_req['memory_per_gpu_gb'],\n",
    "                    'comm_overhead': performance['communication_overhead']\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Sort by efficiency score\n",
    "    best_strategies.sort(key=lambda x: x['efficiency_score'], reverse=True)\n",
    "    \n",
    "    if best_strategies:\n",
    "        print(f\"  🏆 Best strategy: {best_strategies[0]['strategy']}\")\n",
    "        print(f\"      Step time: {best_strategies[0]['step_time_ms']:.1f} ms\")\n",
    "        print(f\"      Memory per GPU: {best_strategies[0]['memory_per_gpu']:.1f} GB\")\n",
    "        print(f\"      Communication overhead: {best_strategies[0]['comm_overhead']:.1%}\")\n",
    "        \n",
    "        if len(best_strategies) > 1:\n",
    "            print(f\"  🥈 Alternative: {best_strategies[1]['strategy']}\")\n",
    "            print(f\"      Step time: {best_strategies[1]['step_time_ms']:.1f} ms\")\n",
    "    else:\n",
    "        print(f\"  ❌ No suitable strategy found for {test_gpu_count} GPUs\")\n",
    "        print(f\"      Model too large - requires more GPUs or different approach\")\n",
    "\n",
    "# General recommendations\n",
    "print(f\"\\n💡 GENERAL RECOMMENDATIONS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"🔸 Model Size Guidelines:\")\n",
    "print(\"  • <10B parameters: Data Parallel (simple, effective)\")\n",
    "print(\"  • 10-100B parameters: Tensor Parallel or Pipeline Parallel\")\n",
    "print(\"  • >100B parameters: 3D Parallel (combines all techniques)\")\n",
    "\n",
    "print(\"\\n🔸 Hardware Requirements:\")\n",
    "print(\"  • Data Parallel: Standard InfiniBand interconnect\")\n",
    "print(\"  • Tensor Parallel: High-bandwidth NVLink or NVSwitch\")\n",
    "print(\"  • Pipeline Parallel: Moderate bandwidth, careful micro-batch tuning\")\n",
    "print(\"  • 3D Parallel: Hierarchical topology, expert implementation\")\n",
    "\n",
    "print(\"\\n🔸 Implementation Complexity:\")\n",
    "print(\"  • Start with Data Parallel (easiest)\")\n",
    "print(\"  • Add Tensor Parallel for memory constraints\")\n",
    "print(\"  • Use Pipeline Parallel for very large models\")\n",
    "print(\"  • 3D Parallel only for massive scale (1000+ GPUs)\")\n",
    "\n",
    "print(\"\\n🔸 Cost Optimization:\")\n",
    "print(\"  • Data Parallel: Lowest implementation cost\")\n",
    "print(\"  • Tensor Parallel: Requires premium hardware\")\n",
    "print(\"  • Pipeline Parallel: Complex tuning overhead\")\n",
    "print(\"  • 3D Parallel: Highest expertise requirement\")\n",
    "\n",
    "# Show strategy decision tree\n",
    "print(f\"\\n🌳 STRATEGY DECISION TREE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Does model fit on single GPU?\")\n",
    "print(\"   ├─ YES → Use Data Parallel\")\n",
    "print(\"   └─ NO → Go to step 2\")\n",
    "print(\"\")\n",
    "print(\"2. Do you have high-bandwidth interconnects (NVLink)?\")\n",
    "print(\"   ├─ YES → Use Tensor Parallel\")\n",
    "print(\"   └─ NO → Go to step 3\")\n",
    "print(\"\")\n",
    "print(\"3. Is your batch size large enough for micro-batching?\")\n",
    "print(\"   ├─ YES → Use Pipeline Parallel\")\n",
    "print(\"   └─ NO → Use Model Parallel\")\n",
    "print(\"\")\n",
    "print(\"4. For massive models (>100B params):\")\n",
    "print(\"   └─ Use 3D Parallel (combines all techniques)\")\n",
    "\n",
    "print(f\"\\n🚀 PRODUCTION TIPS:\")\n",
    "print(\"• Start simple: Begin with Data Parallel and scale up\")\n",
    "print(\"• Measure everything: Profile before optimizing\")\n",
    "print(\"• Balance is key: Don't over-optimize one dimension\")\n",
    "print(\"• Hardware matters: Match strategy to your infrastructure\")\n",
    "print(\"• Expertise required: Complex strategies need experienced teams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Production-Ready Transformer Deployment\n",
    "\n",
    "You now possess the complete arsenal for deploying transformers in production environments!\n",
    "\n",
    "### 🎯 Key Production Optimizations\n",
    "\n",
    "**1. Quantization Mastery**\n",
    "- **FP16**: 2x memory savings, <0.5% quality loss → Start here\n",
    "- **INT8**: 4x memory savings, ~2.5% quality loss → Production deployment\n",
    "- **INT4**: 8x memory savings, ~8% quality loss → Edge devices only\n",
    "- **Sweet spot**: FP16 for most production workloads\n",
    "\n",
    "**2. Deployment Strategy Selection**\n",
    "- **Single inference**: Development and low-traffic scenarios\n",
    "- **Batched inference**: High-throughput production (5-10x speedup)\n",
    "- **Cached inference**: 100x speedup for repeated queries\n",
    "- **Streaming**: Better UX for long-form generation\n",
    "- **Production recommendation**: Batching + Caching hybrid\n",
    "\n",
    "**3. Distributed Training Strategies**\n",
    "- **Data Parallel**: Models <10B parameters, simple implementation\n",
    "- **Tensor Parallel**: 10-100B parameters, requires NVLink\n",
    "- **Pipeline Parallel**: Very large models, careful micro-batch tuning\n",
    "- **3D Parallel**: Massive models (>100B), expert implementation required\n",
    "\n",
    "### ⚙️ Hardware Optimization Framework\n",
    "\n",
    "**GPU Selection Matrix**:\n",
    "- **A100**: Best balance for production (80GB memory, 312 TFLOPS)\n",
    "- **H100**: Highest performance (500 TFLOPS) but expensive\n",
    "- **RTX4090**: Cost-effective for smaller models\n",
    "\n",
    "**Memory-Compute Balance**:\n",
    "- Monitor GPU utilization >80%\n",
    "- Use mixed precision training\n",
    "- Optimize batch sizes for throughput\n",
    "- Gradient checkpointing for memory-bound workloads\n",
    "\n",
    "### 🛡️ Safety and Monitoring\n",
    "\n",
    "**Critical Safety Measures**:\n",
    "- **Content filtering**: Block harmful outputs\n",
    "- **Bias detection**: Monitor for unfair outputs  \n",
    "- **Hallucination detection**: Flag suspicious claims\n",
    "- **Rate limiting**: Prevent abuse and overload\n",
    "- **Human oversight**: Essential for edge cases\n",
    "\n",
    "**Key Monitoring Metrics**:\n",
    "- Latency: Target <100ms for real-time applications\n",
    "- Throughput: >100 requests/second for production\n",
    "- Safety violation rate: <0.1%\n",
    "- Cost per request: <$0.01 for sustainable economics\n",
    "\n",
    "### 💰 Cost Optimization Strategies\n",
    "\n",
    "**Primary Cost Drivers**:\n",
    "1. **Compute**: 60-70% of total cost\n",
    "2. **Memory**: 20-25% of total cost\n",
    "3. **Storage**: 5-10% of total cost\n",
    "4. **Bandwidth**: 5-10% of total cost\n",
    "\n",
    "**Cost Reduction Techniques**:\n",
    "- Apply quantization (4x memory savings = 2x cost reduction)\n",
    "- Implement efficient caching (10x speedup for repeated queries)\n",
    "- Use spot instances for training (70% cost savings)\n",
    "- Optimize batch sizes (linear throughput scaling)\n",
    "- Monitor and eliminate idle resources\n",
    "\n",
    "### 📊 Production Performance Targets\n",
    "\n",
    "**Technical KPIs**:\n",
    "- **Latency**: <100ms (real-time) to <1s (batch)\n",
    "- **Throughput**: 100-1000 requests/second\n",
    "- **GPU Utilization**: >80% sustained\n",
    "- **Memory Efficiency**: >70% utilization\n",
    "- **Availability**: >99.9% uptime\n",
    "\n",
    "**Quality KPIs**:\n",
    "- **Safety Compliance**: <0.1% violation rate\n",
    "- **Output Quality**: >95% user satisfaction\n",
    "- **Consistency**: <5% variance in response quality\n",
    "\n",
    "### 🚀 Deployment Pipeline\n",
    "\n",
    "**Production-Ready Pipeline**:\n",
    "1. **Development**: FP32 training, extensive experimentation\n",
    "2. **Optimization**: Apply quantization, profile performance\n",
    "3. **Safety Testing**: Comprehensive red-teaming, bias audits\n",
    "4. **Staging**: Full-scale load testing, monitoring validation\n",
    "5. **Production**: Gradual rollout with comprehensive monitoring\n",
    "6. **Monitoring**: Continuous safety and performance tracking\n",
    "\n",
    "### 🎯 Strategic Implementation Approach\n",
    "\n",
    "**Phase 1: Foundation (Weeks 1-2)**\n",
    "- Deploy with FP16 quantization\n",
    "- Implement basic batching\n",
    "- Set up essential monitoring\n",
    "\n",
    "**Phase 2: Optimization (Weeks 3-4)**\n",
    "- Add intelligent caching\n",
    "- Implement safety filters\n",
    "- Optimize batch sizes and hardware utilization\n",
    "\n",
    "**Phase 3: Scale (Weeks 5-8)**\n",
    "- Consider INT8 quantization for cost reduction\n",
    "- Implement advanced distributed training\n",
    "- Add sophisticated monitoring and alerting\n",
    "\n",
    "**Phase 4: Excellence (Ongoing)**\n",
    "- Continuous safety improvements\n",
    "- Advanced optimization techniques\n",
    "- Research integration and model updates\n",
    "\n",
    "### 🔮 Future-Proofing Considerations\n",
    "\n",
    "**Emerging Trends**:\n",
    "- **Edge deployment**: Models on mobile/IoT devices\n",
    "- **Specialized hardware**: TPUs, neuromorphic chips\n",
    "- **Advanced quantization**: Sub-8-bit, dynamic precision\n",
    "- **Model compression**: Pruning, distillation, neural architecture search\n",
    "\n",
    "**Architecture Evolution**:\n",
    "- Mixture of experts models\n",
    "- Retrieval-augmented generation\n",
    "- Multimodal architectures\n",
    "- Sparse and efficient attention mechanisms\n",
    "\n",
    "### ✅ Production Readiness Checklist\n",
    "\n",
    "**Before Going Live**:\n",
    "- ✅ Quantization applied and validated\n",
    "- ✅ Batching and caching implemented\n",
    "- ✅ Safety filters and monitoring active\n",
    "- ✅ Load testing completed\n",
    "- ✅ Disaster recovery plan established\n",
    "- ✅ Cost monitoring and alerts configured\n",
    "- ✅ Team trained on monitoring and incident response\n",
    "\n",
    "**Ongoing Operations**:\n",
    "- 📊 Daily performance reviews\n",
    "- 🛡️ Weekly safety audits\n",
    "- 💰 Monthly cost optimization\n",
    "- 🔄 Quarterly model updates\n",
    "- 📈 Continuous improvement culture\n",
    "\n",
    "You now have the knowledge and tools to deploy transformer models at enterprise scale, safely and cost-effectively. From quantization mathematics to distributed systems engineering, from safety protocols to cost optimization - you're equipped for production success! 🌟🏭"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}