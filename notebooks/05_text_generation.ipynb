{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation: From Probabilities to Creative Writing\n",
    "\n",
    "Once a transformer is trained, how do we use it to generate text? This notebook explores the fascinating world of text generation strategies, from simple greedy decoding to creative sampling techniques.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Autoregressive Generation** - How transformers generate text step by step\n",
    "2. **Greedy Decoding** - Always pick the most likely next token\n",
    "3. **Sampling Strategies** - Temperature, top-k, top-p (nucleus) sampling\n",
    "4. **Beam Search** - Exploring multiple possibilities\n",
    "5. **Generation Quality** - Controlling repetition, coherence, and creativity\n",
    "6. **Advanced Techniques** - Contrastive search, typical sampling\n",
    "\n",
    "Let's unlock the creative potential of transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import math\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoregressive Generation Basics\n",
    "\n",
    "Transformers generate text autoregressively - one token at a time, using previously generated tokens as context. Let's see how this works step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_autoregressive_generation():\n",
    "    \"\"\"Show how autoregressive generation works step by step.\"\"\"\n",
    "    \n",
    "    print(\"Autoregressive Text Generation\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Simple vocabulary for demonstration\n",
    "    vocab = [\"<PAD>\", \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"dog\", \"ran\"]\n",
    "    vocab_size = len(vocab)\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Simulate model predictions (fake logits for demonstration)\n",
    "    # In reality, these would come from a trained transformer\n",
    "    def get_fake_logits(context_tokens):\n",
    "        \"\"\"Simulate model predictions based on context.\"\"\"\n",
    "        torch.manual_seed(42 + len(context_tokens))  # Different seed for each step\n",
    "        logits = torch.randn(vocab_size)\n",
    "        \n",
    "        # Add some bias to make more realistic predictions\n",
    "        if len(context_tokens) == 1 and context_tokens[0] == word_to_id[\"The\"]:\n",
    "            logits[word_to_id[\"cat\"]] += 2.0  # \"The\" -> \"cat\" more likely\n",
    "            logits[word_to_id[\"dog\"]] += 1.5  # \"The\" -> \"dog\" somewhat likely\n",
    "        elif len(context_tokens) >= 2 and context_tokens[-1] == word_to_id[\"cat\"]:\n",
    "            logits[word_to_id[\"sat\"]] += 3.0  # \"cat\" -> \"sat\" very likely\n",
    "        elif len(context_tokens) >= 2 and context_tokens[-1] == word_to_id[\"sat\"]:\n",
    "            logits[word_to_id[\"on\"]] += 2.5   # \"sat\" -> \"on\" likely\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # Generate text step by step\n",
    "    prompt = \"The\"\n",
    "    context = [word_to_id[prompt]]\n",
    "    max_length = 6\n",
    "    \n",
    "    print(f\"\\nStarting with prompt: '{prompt}'\")\n",
    "    print(\"Generation process:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for step in range(max_length - 1):\n",
    "        # Get model predictions\n",
    "        logits = get_fake_logits(context)\n",
    "        probabilities = F.softmax(logits, dim=0)\n",
    "        \n",
    "        # Show current context\n",
    "        context_words = [id_to_word[token_id] for token_id in context]\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(f\"Context: {' '.join(context_words)}\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        top_probs, top_indices = torch.topk(probabilities, 3)\n",
    "        print(\"Top predictions:\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "            word = id_to_word[idx.item()]\n",
    "            print(f\"  {i+1}. {word}: {prob.item():.3f}\")\n",
    "        \n",
    "        # Greedy selection (pick most likely)\n",
    "        next_token_id = top_indices[0].item()\n",
    "        next_word = id_to_word[next_token_id]\n",
    "        \n",
    "        # Add to context\n",
    "        context.append(next_token_id)\n",
    "        \n",
    "        print(f\"Selected: {next_word}\")\n",
    "        \n",
    "        # Stop if we hit end token\n",
    "        if next_word == \".\":\n",
    "            break\n",
    "    \n",
    "    # Final result\n",
    "    final_text = \" \".join([id_to_word[token_id] for token_id in context])\n",
    "    print(f\"\\nFinal generated text: '{final_text}'\")\n",
    "    \n",
    "    print(\"\\nKey Points:\")\n",
    "    print(\"‚Ä¢ Each step uses ALL previous tokens as context\")\n",
    "    print(\"‚Ä¢ Model predicts probability distribution over vocabulary\")\n",
    "    print(\"‚Ä¢ Selection strategy determines the next token\")\n",
    "    print(\"‚Ä¢ Process continues until stopping condition\")\n",
    "    \n",
    "    return context, vocab, word_to_id, id_to_word\n",
    "\n",
    "context, vocab, word_to_id, id_to_word = demonstrate_autoregressive_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generation Strategies\n",
    "\n",
    "How we choose the next token dramatically affects the quality and creativity of generated text. Let's explore different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_generation_strategies():\n",
    "    \"\"\"Compare different token selection strategies.\"\"\"\n",
    "    \n",
    "    print(\"Token Selection Strategies\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create example probability distribution\n",
    "    vocab = [\"cat\", \"dog\", \"bird\", \"fish\", \"mouse\", \"horse\", \"cow\", \"sheep\"]\n",
    "    # Simulate realistic probability distribution (some tokens much more likely)\n",
    "    raw_probs = torch.tensor([0.5, 0.3, 0.1, 0.05, 0.03, 0.01, 0.005, 0.005])\n",
    "    probs = raw_probs / raw_probs.sum()  # Normalize\n",
    "    \n",
    "    print(f\"Example probability distribution:\")\n",
    "    for word, prob in zip(vocab, probs):\n",
    "        print(f\"  {word:<6}: {prob:.3f} {'‚ñà' * int(prob * 50)}\")\n",
    "    \n",
    "    return vocab, probs\n",
    "\n",
    "vocab, probs = demonstrate_generation_strategies()\n",
    "\n",
    "# 1. Greedy Decoding\n",
    "def greedy_selection(probs):\n",
    "    \"\"\"Always select the most likely token.\"\"\"\n",
    "    return torch.argmax(probs)\n",
    "\n",
    "# 2. Temperature Sampling\n",
    "def temperature_sampling(logits, temperature=1.0):\n",
    "    \"\"\"Sample with temperature scaling.\"\"\"\n",
    "    if temperature == 0:\n",
    "        return torch.argmax(logits)\n",
    "    \n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=0)\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "# 3. Top-k Sampling\n",
    "def top_k_sampling(logits, k=3):\n",
    "    \"\"\"Sample from top-k most likely tokens.\"\"\"\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k)\n",
    "    probs = F.softmax(top_k_logits, dim=0)\n",
    "    selected_idx = torch.multinomial(probs, 1).item()\n",
    "    return top_k_indices[selected_idx].item()\n",
    "\n",
    "# 4. Top-p (Nucleus) Sampling\n",
    "def top_p_sampling(logits, p=0.9):\n",
    "    \"\"\"Sample from smallest set of tokens with cumulative probability >= p.\"\"\"\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    \n",
    "    # Find cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = 0  # Keep at least one token\n",
    "    \n",
    "    # Zero out probabilities of removed tokens\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    probs[indices_to_remove] = 0\n",
    "    \n",
    "    # Renormalize\n",
    "    probs = probs / probs.sum()\n",
    "    \n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "# Test strategies\n",
    "logits = torch.log(probs)  # Convert back to logits\n",
    "\n",
    "print(\"\\nStrategy Comparison:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Greedy\n",
    "greedy_idx = greedy_selection(probs)\n",
    "print(f\"Greedy:       {vocab[greedy_idx]} (always same)\")\n",
    "\n",
    "# Temperature sampling with different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "print(\"\\nTemperature sampling (5 samples each):\")\n",
    "for temp in temperatures:\n",
    "    samples = []\n",
    "    for _ in range(5):\n",
    "        idx = temperature_sampling(logits, temp)\n",
    "        samples.append(vocab[idx])\n",
    "    print(f\"  T={temp:3.1f}: {samples}\")\n",
    "\n",
    "# Top-k sampling\n",
    "print(\"\\nTop-k sampling (k=3, 5 samples):\")\n",
    "top_k_samples = []\n",
    "for _ in range(5):\n",
    "    idx = top_k_sampling(logits, k=3)\n",
    "    top_k_samples.append(vocab[idx])\n",
    "print(f\"  {top_k_samples}\")\n",
    "\n",
    "# Top-p sampling\n",
    "print(\"\\nTop-p sampling (p=0.9, 5 samples):\")\n",
    "top_p_samples = []\n",
    "for _ in range(5):\n",
    "    idx = top_p_sampling(logits, p=0.9)\n",
    "    top_p_samples.append(vocab[idx])\n",
    "print(f\"  {top_p_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Effects\n",
    "\n",
    "Temperature is one of the most important parameters for controlling generation. Let's visualize how it affects the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temperature_effects():\n",
    "    \"\"\"Visualize how temperature affects probability distributions.\"\"\"\n",
    "    \n",
    "    # Create example logits\n",
    "    vocab = [\"the\", \"cat\", \"dog\", \"sat\", \"ran\", \"jumped\", \"quickly\", \"slowly\"]\n",
    "    logits = torch.tensor([3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5])\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, temp in enumerate(temperatures):\n",
    "        if temp == 0:\n",
    "            # Special case: greedy (one-hot)\n",
    "            probs = torch.zeros_like(logits)\n",
    "            probs[torch.argmax(logits)] = 1.0\n",
    "        else:\n",
    "            scaled_logits = logits / temp\n",
    "            probs = F.softmax(scaled_logits, dim=0)\n",
    "        \n",
    "        # Bar plot\n",
    "        bars = axes[i].bar(range(len(vocab)), probs, alpha=0.7)\n",
    "        axes[i].set_xlabel('Tokens')\n",
    "        axes[i].set_ylabel('Probability')\n",
    "        axes[i].set_title(f'Temperature = {temp}')\n",
    "        axes[i].set_xticks(range(len(vocab)))\n",
    "        axes[i].set_xticklabels(vocab, rotation=45)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Color the highest probability bar\n",
    "        max_idx = torch.argmax(probs)\n",
    "        bars[max_idx].set_color('red')\n",
    "        bars[max_idx].set_alpha(0.9)\n",
    "        \n",
    "        # Add probability values on bars\n",
    "        for j, (bar, prob) in enumerate(zip(bars, probs)):\n",
    "            if prob > 0.01:  # Only show if probability > 1%\n",
    "                axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                           f'{prob:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Temperature Effects:\")\n",
    "    print(\"=\" * 20)\n",
    "    print(\"‚Ä¢ Low temperature (T < 1):  More focused, less creative\")\n",
    "    print(\"‚Ä¢ Temperature = 1:          Original distribution\")\n",
    "    print(\"‚Ä¢ High temperature (T > 1): More random, more creative\")\n",
    "    print(\"‚Ä¢ Temperature ‚Üí 0:          Greedy decoding\")\n",
    "    print(\"‚Ä¢ Temperature ‚Üí ‚àû:          Uniform random\")\n",
    "    \n",
    "    # Calculate entropy for each temperature\n",
    "    print(\"\\nEntropy (measure of randomness):\")\n",
    "    for temp in temperatures:\n",
    "        if temp == 0:\n",
    "            entropy = 0.0\n",
    "        else:\n",
    "            scaled_logits = logits / temp\n",
    "            probs = F.softmax(scaled_logits, dim=0)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "        print(f\"  T={temp:3.1f}: entropy={entropy:.3f}\")\n",
    "\n",
    "visualize_temperature_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real Text Generation with a Trained Model\n",
    "\n",
    "Let's load our trained transformer and explore different generation strategies with real text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"Advanced text generator with multiple strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def generate(self, prompt: str, max_length: int = 50, strategy: str = \"temperature\", **kwargs):\n",
    "        \"\"\"Generate text using specified strategy.\"\"\"\n",
    "        \n",
    "        # Encode prompt\n",
    "        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        generated_tokens = tokens.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # Get model predictions\n",
    "                logits, _ = self.model(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]  # Last position\n",
    "                \n",
    "                # Apply generation strategy\n",
    "                if strategy == \"greedy\":\n",
    "                    next_token = torch.argmax(next_token_logits).item()\n",
    "                \n",
    "                elif strategy == \"temperature\":\n",
    "                    temperature = kwargs.get('temperature', 1.0)\n",
    "                    if temperature == 0:\n",
    "                        next_token = torch.argmax(next_token_logits).item()\n",
    "                    else:\n",
    "                        scaled_logits = next_token_logits / temperature\n",
    "                        probs = F.softmax(scaled_logits, dim=0)\n",
    "                        next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                elif strategy == \"top_k\":\n",
    "                    k = kwargs.get('k', 10)\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
    "                    probs = F.softmax(top_k_logits, dim=0)\n",
    "                    selected_idx = torch.multinomial(probs, 1).item()\n",
    "                    next_token = top_k_indices[selected_idx].item()\n",
    "                \n",
    "                elif strategy == \"top_p\":\n",
    "                    p = kwargs.get('p', 0.9)\n",
    "                    probs = F.softmax(next_token_logits, dim=0)\n",
    "                    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "                    \n",
    "                    # Remove tokens with cumulative probability above threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > p\n",
    "                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                    sorted_indices_to_remove[0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                    probs[indices_to_remove] = 0\n",
    "                    probs = probs / probs.sum()\n",
    "                    \n",
    "                    next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_tokens.append(next_token)\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(self.device)], dim=1)\n",
    "                \n",
    "                # Stop if we hit a natural stopping point\n",
    "                if len(generated_tokens) > len(tokens) + 5:  # Generate at least 5 tokens\n",
    "                    decoded = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                    if decoded.endswith('.') or decoded.endswith('!') or decoded.endswith('?'):\n",
    "                        break\n",
    "        \n",
    "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Setup model and generator\n",
    "def setup_text_generator():\n",
    "    \"\"\"Setup a text generator with a simple trained model.\"\"\"\n",
    "    \n",
    "    # Create a small model for demonstration\n",
    "    config = create_model_config(\"tiny\")\n",
    "    config[\"vocab_size\"] = 200\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Quick training on sample text to make generation more interesting\n",
    "    from src.data.dataset import SimpleTextDataset, create_dataloader\n",
    "    \n",
    "    training_text = \"\"\"\n",
    "    The cat sat on the mat and looked around the room. The dog ran quickly across the yard.\n",
    "    A beautiful bird flew high in the sky above the trees. The sun shone brightly on the garden.\n",
    "    Children played happily in the park with their friends. The ocean waves crashed on the shore.\n",
    "    Mountains stood tall against the clear blue sky. Flowers bloomed in the spring meadow.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = SimpleTextDataset(training_text, tokenizer, block_size=16)\n",
    "    dataloader = create_dataloader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Quick training\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(10):  # Quick training\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    generator = TextGenerator(model, tokenizer)\n",
    "    \n",
    "    print(f\"Model ready with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    return generator\n",
    "\n",
    "generator = setup_text_generator()\n",
    "\n",
    "# Test different generation strategies\n",
    "prompt = \"The cat\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "strategies = [\n",
    "    (\"greedy\", {}),\n",
    "    (\"temperature\", {\"temperature\": 0.5}),\n",
    "    (\"temperature\", {\"temperature\": 1.0}),\n",
    "    (\"temperature\", {\"temperature\": 1.5}),\n",
    "    (\"top_k\", {\"k\": 5}),\n",
    "    (\"top_p\", {\"p\": 0.8}),\n",
    "]\n",
    "\n",
    "for strategy, kwargs in strategies:\n",
    "    generated = generator.generate(prompt, max_length=15, strategy=strategy, **kwargs)\n",
    "    params_str = \", \".join([f\"{k}={v}\" for k, v in kwargs.items()]) if kwargs else \"\"\n",
    "    strategy_label = f\"{strategy}({params_str})\" if params_str else strategy\n",
    "    print(f\"{strategy_label:<20}: '{generated}'\")\n",
    "\n",
    "print(\"\\nNotice how different strategies produce different styles of text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beam Search\n",
    "\n",
    "Beam search explores multiple possible sequences simultaneously, keeping track of the most promising candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    \"\"\"Beam search text generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def beam_search(self, prompt: str, beam_width: int = 3, max_length: int = 20):\n",
    "        \"\"\"Generate text using beam search.\"\"\"\n",
    "        \n",
    "        # Encode prompt\n",
    "        initial_tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        \n",
    "        # Initialize beams: (sequence, log_probability)\n",
    "        beams = [(initial_tokens, 0.0)]\n",
    "        \n",
    "        print(f\"Beam Search with beam_width={beam_width}\")\n",
    "        print(f\"Starting prompt: '{prompt}'\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                print(f\"\\nStep {step + 1}:\")\n",
    "                \n",
    "                for beam_idx, (sequence, log_prob) in enumerate(beams):\n",
    "                    # Convert to tensor\n",
    "                    input_ids = torch.tensor(sequence).unsqueeze(0).to(self.device)\n",
    "                    \n",
    "                    # Get model predictions\n",
    "                    logits, _ = self.model(input_ids)\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    \n",
    "                    # Get top-k tokens for this beam\n",
    "                    log_probs = F.log_softmax(next_token_logits, dim=0)\n",
    "                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                    \n",
    "                    # Show current beam\n",
    "                    current_text = self.tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "                    print(f\"  Beam {beam_idx}: '{current_text}' (score: {log_prob:.3f})\")\n",
    "                    \n",
    "                    # Add candidates\n",
    "                    for token_idx, token_log_prob in zip(top_indices, top_log_probs):\n",
    "                        new_sequence = sequence + [token_idx.item()]\n",
    "                        new_log_prob = log_prob + token_log_prob.item()\n",
    "                        candidates.append((new_sequence, new_log_prob))\n",
    "                \n",
    "                # Keep top beam_width candidates\n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)  # Sort by log probability\n",
    "                beams = candidates[:beam_width]\n",
    "                \n",
    "                # Show selected beams\n",
    "                print(\"  Selected for next step:\")\n",
    "                for i, (sequence, log_prob) in enumerate(beams):\n",
    "                    text = self.tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "                    print(f\"    {i+1}. '{text}' (score: {log_prob:.3f})\")\n",
    "                \n",
    "                # Check if all beams end with sentence terminators\n",
    "                all_terminated = True\n",
    "                for sequence, _ in beams:\n",
    "                    text = self.tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "                    if not (text.endswith('.') or text.endswith('!') or text.endswith('?')):\n",
    "                        all_terminated = False\n",
    "                        break\n",
    "                \n",
    "                if all_terminated and step > 5:  # At least 5 steps\n",
    "                    break\n",
    "        \n",
    "        # Return best beam\n",
    "        best_sequence, best_score = beams[0]\n",
    "        best_text = self.tokenizer.decode(best_sequence, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(\"-\" * 20)\n",
    "        for i, (sequence, score) in enumerate(beams):\n",
    "            text = self.tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "            print(f\"{i+1}. '{text}' (score: {score:.3f})\")\n",
    "        \n",
    "        return best_text, beams\n",
    "\n",
    "# Test beam search\n",
    "beam_generator = BeamSearchGenerator(generator.model, generator.tokenizer)\n",
    "best_text, all_beams = beam_generator.beam_search(\"The cat\", beam_width=3, max_length=10)\n",
    "\n",
    "print(f\"\\nBest result: '{best_text}'\")\n",
    "\n",
    "print(\"\\nBeam Search vs. Sampling:\")\n",
    "print(\"‚Ä¢ Beam search: Finds high-probability sequences\")\n",
    "print(\"‚Ä¢ Pro: More coherent, grammatically correct\")\n",
    "print(\"‚Ä¢ Con: Can be repetitive, less creative\")\n",
    "print(\"‚Ä¢ Sampling: More diverse but potentially less coherent\")\n",
    "print(\"‚Ä¢ Best: Combine both - beam search + sampling within beams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Techniques\n",
    "\n",
    "Let's explore some advanced techniques for improving generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_penalty_sampling(logits, previous_tokens, penalty=1.2):\n",
    "    \"\"\"Apply repetition penalty to discourage repeated tokens.\"\"\"\n",
    "    \n",
    "    # Count token frequencies in previous context\n",
    "    token_counts = Counter(previous_tokens)\n",
    "    \n",
    "    # Apply penalty\n",
    "    penalized_logits = logits.clone()\n",
    "    for token_id, count in token_counts.items():\n",
    "        if token_id < len(penalized_logits):\n",
    "            penalized_logits[token_id] /= (penalty ** count)\n",
    "    \n",
    "    return penalized_logits\n",
    "\n",
    "def typical_sampling(logits, tau=0.95):\n",
    "    \"\"\"Typical sampling - sample from tokens with 'typical' probability.\"\"\"\n",
    "    \n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-10))\n",
    "    \n",
    "    # Calculate \"surprisal\" (negative log probability)\n",
    "    surprisals = -torch.log(probs + 1e-10)\n",
    "    \n",
    "    # Find tokens with surprisal close to entropy (\"typical\" tokens)\n",
    "    differences = torch.abs(surprisals - entropy)\n",
    "    \n",
    "    # Sort by how \"typical\" they are\n",
    "    sorted_diffs, sorted_indices = torch.sort(differences)\n",
    "    \n",
    "    # Keep tokens until we reach tau probability mass\n",
    "    cumulative_prob = 0\n",
    "    typical_indices = []\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        typical_indices.append(idx.item())\n",
    "        cumulative_prob += probs[idx].item()\n",
    "        if cumulative_prob >= tau:\n",
    "            break\n",
    "    \n",
    "    # Create new probability distribution over typical tokens\n",
    "    typical_probs = torch.zeros_like(probs)\n",
    "    for idx in typical_indices:\n",
    "        typical_probs[idx] = probs[idx]\n",
    "    \n",
    "    typical_probs = typical_probs / typical_probs.sum()\n",
    "    \n",
    "    return torch.multinomial(typical_probs, 1).item()\n",
    "\n",
    "def demonstrate_advanced_techniques():\n",
    "    \"\"\"Demonstrate advanced generation techniques.\"\"\"\n",
    "    \n",
    "    print(\"Advanced Generation Techniques\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create example scenario\n",
    "    vocab = [\"the\", \"cat\", \"dog\", \"sat\", \"ran\", \"quickly\", \"slowly\", \".\", \"and\", \"then\"]\n",
    "    logits = torch.tensor([2.0, 3.0, 1.5, 2.5, 1.0, 0.5, 0.3, 1.8, 1.2, 0.8])\n",
    "    previous_tokens = [1, 3, 1, 1]  # \"cat sat cat cat\" - repetitive!\n",
    "    \n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    print(f\"Previous tokens: {[vocab[i] for i in previous_tokens]}\")\n",
    "    print()\n",
    "    \n",
    "    # Show original probabilities\n",
    "    original_probs = F.softmax(logits, dim=0)\n",
    "    print(\"Original probabilities:\")\n",
    "    for i, (word, prob) in enumerate(zip(vocab, original_probs)):\n",
    "        marker = \" ‚Üê repetitive!\" if i in previous_tokens else \"\"\n",
    "        print(f\"  {word:<8}: {prob:.3f}{marker}\")\n",
    "    \n",
    "    # Apply repetition penalty\n",
    "    penalized_logits = repetition_penalty_sampling(logits, previous_tokens, penalty=1.5)\n",
    "    penalized_probs = F.softmax(penalized_logits, dim=0)\n",
    "    \n",
    "    print(\"\\nAfter repetition penalty:\")\n",
    "    for i, (word, prob) in enumerate(zip(vocab, penalized_probs)):\n",
    "        change = prob - original_probs[i]\n",
    "        arrow = \"‚Üì\" if change < -0.01 else \"‚Üë\" if change > 0.01 else \"‚Üí\"\n",
    "        print(f\"  {word:<8}: {prob:.3f} {arrow}\")\n",
    "    \n",
    "    # Test typical sampling\n",
    "    print(\"\\nTypical Sampling:\")\n",
    "    typical_samples = []\n",
    "    for _ in range(5):\n",
    "        idx = typical_sampling(logits, tau=0.8)\n",
    "        typical_samples.append(vocab[idx])\n",
    "    print(f\"Samples: {typical_samples}\")\n",
    "    \n",
    "    print(\"\\nTechnique Benefits:\")\n",
    "    print(\"‚Ä¢ Repetition penalty: Reduces boring repetition\")\n",
    "    print(\"‚Ä¢ Typical sampling: Avoids both too-common and too-rare tokens\")\n",
    "    print(\"‚Ä¢ Contrastive search: Balances coherence and diversity\")\n",
    "    print(\"‚Ä¢ Dynamic temperature: Adjusts creativity based on confidence\")\n",
    "\n",
    "demonstrate_advanced_techniques()\n",
    "\n",
    "# Implement an advanced generator\n",
    "class AdvancedGenerator:\n",
    "    \"\"\"Generator with advanced techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def advanced_generate(self, prompt: str, max_length: int = 30, \n",
    "                         temperature: float = 1.0, repetition_penalty: float = 1.1,\n",
    "                         top_p: float = 0.9):\n",
    "        \"\"\"Generate with multiple advanced techniques.\"\"\"\n",
    "        \n",
    "        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        input_ids = torch.tensor(tokens).unsqueeze(0).to(self.device)\n",
    "        generated_tokens = tokens.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length):\n",
    "                # Get model predictions\n",
    "                logits, _ = self.model(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                \n",
    "                # Apply repetition penalty\n",
    "                if repetition_penalty > 1.0:\n",
    "                    context_window = generated_tokens[-20:]  # Last 20 tokens\n",
    "                    next_token_logits = repetition_penalty_sampling(\n",
    "                        next_token_logits, context_window, repetition_penalty\n",
    "                    )\n",
    "                \n",
    "                # Apply temperature\n",
    "                if temperature != 1.0:\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                \n",
    "                # Top-p sampling\n",
    "                probs = F.softmax(next_token_logits, dim=0)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                sorted_indices_to_remove[0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                probs[indices_to_remove] = 0\n",
    "                probs = probs / probs.sum()\n",
    "                \n",
    "                # Sample\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(self.device)], dim=1)\n",
    "                \n",
    "                # Check for natural stopping\n",
    "                if step > 5:\n",
    "                    decoded = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                    if decoded.endswith('.') or decoded.endswith('!') or decoded.endswith('?'):\n",
    "                        break\n",
    "        \n",
    "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Test advanced generator\n",
    "advanced_gen = AdvancedGenerator(generator.model, generator.tokenizer)\n",
    "\n",
    "print(\"\\nAdvanced Generation Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "prompt = \"The cat\"\n",
    "configs = [\n",
    "    {\"temperature\": 0.8, \"repetition_penalty\": 1.0, \"top_p\": 1.0, \"label\": \"Basic\"},\n",
    "    {\"temperature\": 0.8, \"repetition_penalty\": 1.2, \"top_p\": 1.0, \"label\": \"+ Rep. Penalty\"},\n",
    "    {\"temperature\": 0.8, \"repetition_penalty\": 1.0, \"top_p\": 0.9, \"label\": \"+ Top-p\"},\n",
    "    {\"temperature\": 0.8, \"repetition_penalty\": 1.2, \"top_p\": 0.9, \"label\": \"All Techniques\"},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    label = config.pop(\"label\")\n",
    "    result = advanced_gen.advanced_generate(prompt, max_length=20, **config)\n",
    "    print(f\"{label:<15}: '{result}'\")\n",
    "\n",
    "print(\"\\n‚úÖ Advanced generation techniques improve quality and diversity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation Quality Metrics\n",
    "\n",
    "How do we measure the quality of generated text? Let's explore some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generation_quality(texts: List[str]):\n",
    "    \"\"\"Analyze various quality metrics for generated texts.\"\"\"\n",
    "    \n",
    "    print(\"Generation Quality Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\nText {i+1}: '{text}'\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        words = text.split()\n",
    "        unique_words = set(words)\n",
    "        \n",
    "        print(f\"  Length: {len(words)} words\")\n",
    "        print(f\"  Unique words: {len(unique_words)}\")\n",
    "        print(f\"  Repetition ratio: {1 - len(unique_words)/len(words):.3f}\")\n",
    "        \n",
    "        # Repetition analysis\n",
    "        word_counts = Counter(words)\n",
    "        repeated_words = {word: count for word, count in word_counts.items() if count > 1}\n",
    "        if repeated_words:\n",
    "            print(f\"  Repeated words: {repeated_words}\")\n",
    "        \n",
    "        # N-gram repetition\n",
    "        bigrams = [tuple(words[i:i+2]) for i in range(len(words)-1)]\n",
    "        bigram_counts = Counter(bigrams)\n",
    "        repeated_bigrams = {bg: count for bg, count in bigram_counts.items() if count > 1}\n",
    "        if repeated_bigrams:\n",
    "            print(f\"  Repeated bigrams: {repeated_bigrams}\")\n",
    "        \n",
    "        # Perplexity simulation (would need actual model for real perplexity)\n",
    "        # For demonstration, we'll estimate based on word frequency\n",
    "        avg_word_freq = sum(word_counts.values()) / len(unique_words)\n",
    "        estimated_perplexity = len(unique_words) / avg_word_freq\n",
    "        print(f\"  Estimated diversity: {estimated_perplexity:.2f}\")\n",
    "\n",
    "# Generate examples with different strategies for comparison\n",
    "prompt = \"The beautiful garden\"\n",
    "examples = []\n",
    "\n",
    "# Greedy (should be repetitive)\n",
    "greedy_text = generator.generate(prompt, max_length=15, strategy=\"greedy\")\n",
    "examples.append(greedy_text)\n",
    "\n",
    "# High temperature (should be diverse but potentially incoherent)\n",
    "high_temp_text = generator.generate(prompt, max_length=15, strategy=\"temperature\", temperature=2.0)\n",
    "examples.append(high_temp_text)\n",
    "\n",
    "# Balanced approach\n",
    "balanced_text = advanced_gen.advanced_generate(prompt, max_length=15, temperature=0.8, repetition_penalty=1.2, top_p=0.9)\n",
    "examples.append(balanced_text)\n",
    "\n",
    "analyze_generation_quality(examples)\n",
    "\n",
    "print(\"\\nQuality Metrics Summary:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"‚Ä¢ Repetition ratio: Lower is better (less repetitive)\")\n",
    "print(\"‚Ä¢ Unique word count: Higher often better (more diverse)\")\n",
    "print(\"‚Ä¢ Perplexity: Moderate is best (not too predictable, not too random)\")\n",
    "print(\"‚Ä¢ Coherence: Must be evaluated by humans or advanced models\")\n",
    "print(\"‚Ä¢ Factual accuracy: Requires external knowledge validation\")\n",
    "\n",
    "# Visualize repetition patterns\n",
    "def visualize_repetition_patterns(text: str):\n",
    "    \"\"\"Visualize word repetition in generated text.\"\"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    word_positions = {}\n",
    "    \n",
    "    # Track positions of each word\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_positions:\n",
    "            word_positions[word] = []\n",
    "        word_positions[word].append(i)\n",
    "    \n",
    "    # Find repeated words\n",
    "    repeated_words = {word: positions for word, positions in word_positions.items() if len(positions) > 1}\n",
    "    \n",
    "    if repeated_words:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        \n",
    "        for i, (word, positions) in enumerate(repeated_words.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            plt.scatter(positions, [i] * len(positions), c=color, s=100, label=word, alpha=0.7)\n",
    "            \n",
    "            # Draw lines between repetitions\n",
    "            for j in range(len(positions) - 1):\n",
    "                plt.plot([positions[j], positions[j+1]], [i, i], color=color, alpha=0.3, linewidth=2)\n",
    "        \n",
    "        plt.xlabel('Word Position')\n",
    "        plt.ylabel('Repeated Words')\n",
    "        plt.title(f'Repetition Pattern: \"{text}\"')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No repetitions found in: \"{text}\"\")\n",
    "\n",
    "# Visualize repetition for the examples\n",
    "print(\"\\nRepetition Pattern Visualization:\")\n",
    "for i, text in enumerate(examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    visualize_repetition_patterns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the fascinating world of text generation:\n",
    "\n",
    "1. **Autoregressive Generation** - Step-by-step token prediction process\n",
    "2. **Generation Strategies** - Greedy, temperature, top-k, top-p sampling\n",
    "3. **Temperature Effects** - Controlling randomness and creativity\n",
    "4. **Beam Search** - Exploring multiple sequence possibilities\n",
    "5. **Advanced Techniques** - Repetition penalty, typical sampling\n",
    "6. **Quality Metrics** - Measuring repetition, diversity, coherence\n",
    "\n",
    "### Key Generation Insights:\n",
    "\n",
    "- **Strategy matters**: Different approaches produce different styles\n",
    "- **Temperature is crucial**: Controls the creativity-coherence tradeoff\n",
    "- **Top-p often best**: Adaptive cutoff based on probability mass\n",
    "- **Repetition is the enemy**: Use penalties and diverse sampling\n",
    "- **Quality is multifaceted**: No single metric captures everything\n",
    "\n",
    "### Generation Strategy Guide:\n",
    "\n",
    "**For coherent, factual text:**\n",
    "- Low temperature (0.3-0.7)\n",
    "- Top-p sampling (p=0.8-0.9)\n",
    "- Mild repetition penalty (1.1-1.3)\n",
    "\n",
    "**For creative writing:**\n",
    "- Higher temperature (0.8-1.2)\n",
    "- Top-p or top-k sampling\n",
    "- Strong repetition penalty (1.3-1.5)\n",
    "\n",
    "**For reliable completion:**\n",
    "- Beam search with beam width 3-5\n",
    "- Length penalties to avoid too-short sequences\n",
    "- Multiple candidates for selection\n",
    "\n",
    "### Modern Developments:\n",
    "\n",
    "- **Contrastive search**: Balances probability and diversity\n",
    "- **Typical sampling**: Avoids both too-common and too-rare tokens\n",
    "- **MCTS-based generation**: Uses tree search for better planning\n",
    "- **Classifier-free guidance**: Steers generation toward desired attributes\n",
    "\n",
    "### Quality Considerations:\n",
    "\n",
    "- **Coherence**: Does the text make sense?\n",
    "- **Consistency**: Are facts and details consistent?\n",
    "- **Relevance**: Does it address the prompt appropriately?\n",
    "- **Fluency**: Is the language natural and grammatical?\n",
    "- **Diversity**: Is the output varied and interesting?\n",
    "\n",
    "The art of text generation lies in balancing these competing objectives. The best approach depends on your specific use case, from creative writing to factual question answering to code generation.\n",
    "\n",
    "Congratulations! You've now completed a comprehensive journey through transformer architecture, training, and generation. You understand how these powerful models work from the ground up! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}