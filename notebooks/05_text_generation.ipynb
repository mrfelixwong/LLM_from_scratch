{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Text Generation: From Probabilities to Words\n\nOnce trained, how do transformers generate text? This notebook explores generation strategies from simple greedy decoding to advanced sampling techniques.\n\n## Learning Objectives\n\n1. **Autoregressive Generation**: How transformers generate text step-by-step\n2. **Generation Strategies**: Greedy, temperature, top-k, top-p sampling  \n3. **Temperature Effects**: Controlling creativity vs coherence\n4. **Beam Search**: Exploring multiple possibilities simultaneously\n5. **Quality Metrics**: Measuring generation quality and diversity\n\nLet's unlock the creative potential of your trained transformer! ðŸŽ¨"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import math\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Optional\nimport math\nfrom collections import Counter\nimport random\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Autoregressive Generation\n\n**Autoregressive = One token at a time, using previous tokens as context**\n\n**Process**:\n1. Start with prompt: \"The cat\"\n2. Model predicts probabilities for next word\n3. Select next word using some strategy  \n4. Add to sequence: \"The cat sat\"\n5. Repeat until complete\n\nThis is how all language models generate text!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"dog\", \"ran\"]\nword_to_id = {word: i for i, word in enumerate(vocab)}\n\ndef simulate_generation_step(context_words):\n    torch.manual_seed(42 + len(context_words))\n    logits = torch.randn(len(vocab))\n    \n    # Add realistic biases\n    if \"The\" in context_words:\n        logits[word_to_id[\"cat\"]] += 2.0\n    if \"cat\" in context_words:\n        logits[word_to_id[\"sat\"]] += 2.5\n    if \"sat\" in context_words:\n        logits[word_to_id[\"on\"]] += 2.0\n        \n    return logits\n\n# Demonstrate step-by-step generation\ncontext = [\"The\"]\nmax_steps = 4\n\nprint(\"ðŸ”„ Autoregressive Generation Demo\")\nprint(\"=\" * 35)\n\nfor step in range(max_steps):\n    logits = simulate_generation_step(context)\n    probs = F.softmax(logits, dim=0)\n    \n    print(f\"\\nStep {step + 1}:\")\n    print(f\"Context: {' '.join(context)}\")\n    print(\"Top 3 predictions:\")\n    \n    top_probs, top_indices = torch.topk(probs, 3)\n    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n        word = vocab[idx.item()]\n        print(f\"  {i+1}. {word}: {prob.item():.3f}\")\n    \n    # Select most likely (greedy)\n    next_word = vocab[top_indices[0].item()]\n    context.append(next_word)\n    print(f\"Selected: {next_word}\")\n    \n    if next_word == \".\":\n        break\n\nprint(f\"\\nFinal text: '{' '.join(context)}'\")\nprint(\"âœ… That's autoregressive generation!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Generation Strategies\n\n**How do we choose the next token?** Different strategies produce different text styles:\n\n1. **Greedy**: Always pick most likely token (deterministic, boring)\n2. **Temperature**: Scale probabilities to control randomness  \n3. **Top-k**: Sample from k most likely tokens\n4. **Top-p**: Sample from tokens with cumulative probability p\n\nEach has trade-offs between coherence and creativity!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vocab = [\"cat\", \"dog\", \"bird\", \"fish\", \"mouse\", \"horse\", \"cow\", \"sheep\"]\nprobs = torch.tensor([0.5, 0.3, 0.1, 0.05, 0.03, 0.01, 0.005, 0.005])\nlogits = torch.log(probs)\n\ndef greedy_selection(probs):\n    return torch.argmax(probs)\n\ndef temperature_sampling(logits, temperature=1.0):\n    if temperature == 0:\n        return torch.argmax(logits)\n    scaled_logits = logits / temperature\n    probs = F.softmax(scaled_logits, dim=0)\n    return torch.multinomial(probs, 1).item()\n\ndef top_k_sampling(logits, k=3):\n    top_k_logits, top_k_indices = torch.topk(logits, k)\n    probs = F.softmax(top_k_logits, dim=0)\n    selected_idx = torch.multinomial(probs, 1).item()\n    return top_k_indices[selected_idx].item()\n\ndef top_p_sampling(logits, p=0.9):\n    probs = F.softmax(logits, dim=0)\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n    \n    sorted_indices_to_remove = cumulative_probs > p\n    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n    sorted_indices_to_remove[0] = 0\n    \n    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n    probs[indices_to_remove] = 0\n    probs = probs / probs.sum()\n    \n    return torch.multinomial(probs, 1).item()\n\nprint(\"Generation Strategy Comparison:\")\nprint(f\"Vocabulary: {vocab}\")\n\nprint(f\"\\nGreedy: {vocab[greedy_selection(probs)]} (always same)\")\n\nprint(\"\\nTemperature sampling (5 samples each):\")\nfor temp in [0.1, 0.5, 1.0, 2.0]:\n    samples = [vocab[temperature_sampling(logits, temp)] for _ in range(5)]\n    print(f\"  T={temp}: {samples}\")\n\nprint(f\"\\nTop-k (k=3): {[vocab[top_k_sampling(logits, k=3)] for _ in range(5)]}\")\nprint(f\"Top-p (p=0.9): {[vocab[top_p_sampling(logits, p=0.9)] for _ in range(5)]}\")\n\nprint(\"\\nðŸ”‘ Key insight: Strategy dramatically affects output diversity!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Temperature Effects\n\nTemperature controls the \"sharpness\" of probability distributions, directly affecting creativity vs coherence."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vocab = [\"the\", \"cat\", \"dog\", \"sat\", \"ran\", \"jumped\", \"quickly\", \"slowly\"]\nlogits = torch.tensor([3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5])\n\ntemperatures = [0.1, 0.5, 1.0, 2.0]\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, temp in enumerate(temperatures):\n    if temp == 0:\n        probs = torch.zeros_like(logits)\n        probs[torch.argmax(logits)] = 1.0\n    else:\n        scaled_logits = logits / temp\n        probs = F.softmax(scaled_logits, dim=0)\n    \n    bars = axes[i].bar(range(len(vocab)), probs, alpha=0.7)\n    axes[i].set_xlabel('Tokens')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Temperature = {temp}')\n    axes[i].set_xticks(range(len(vocab)))\n    axes[i].set_xticklabels(vocab, rotation=45)\n    axes[i].grid(True, alpha=0.3)\n    \n    # Highlight max probability\n    max_idx = torch.argmax(probs)\n    bars[max_idx].set_color('red')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Temperature Effects:\")\nprint(\"â€¢ T < 1.0: More focused, deterministic\")\nprint(\"â€¢ T = 1.0: Original distribution\")  \nprint(\"â€¢ T > 1.0: More random, creative\")\nprint(\"â€¢ T â†’ 0:   Greedy decoding\")\nprint(\"â€¢ T â†’ âˆž:   Uniform random\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Generation Quality Metrics\n\nHow do we measure the quality of generated text? Key metrics include diversity, coherence, and repetition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_text_quality(text):\n    words = text.split()\n    unique_words = set(words)\n    \n    # Basic metrics\n    print(f\"Text: '{text}'\")\n    print(f\"Length: {len(words)} words\")\n    print(f\"Unique words: {len(unique_words)}\")\n    print(f\"Repetition ratio: {1 - len(unique_words)/len(words):.3f}\")\n    \n    # Repetition analysis\n    word_counts = Counter(words)\n    repeated_words = {word: count for word, count in word_counts.items() if count > 1}\n    if repeated_words:\n        print(f\"Repeated words: {repeated_words}\")\n    \n    # Diversity score (estimated)\n    diversity = len(unique_words) / len(words) if len(words) > 0 else 0\n    print(f\"Diversity score: {diversity:.3f}\")\n    print()\n\n# Example texts with different quality patterns\nexamples = [\n    \"The cat sat on the mat and looked around\",  # Good\n    \"The the the cat cat sat sat on on\",         # Repetitive  \n    \"Quantum flux temporal paradox synthesis\",    # Too random\n]\n\nprint(\"ðŸ“Š Text Quality Analysis:\")\nprint(\"=\" * 30)\n\nfor i, text in enumerate(examples, 1):\n    print(f\"Example {i}:\")\n    analyze_text_quality(text)\n\nprint(\"Quality Guidelines:\")\nprint(\"â€¢ Lower repetition ratio = better\")\nprint(\"â€¢ Higher diversity = more interesting\") \nprint(\"â€¢ Must balance diversity with coherence\")\nprint(\"â€¢ Human evaluation often most reliable\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Text Generation Mastery\n\nYou've learned how to generate text with transformers!\n\n### Core Concepts\n- **Autoregressive**: Generate one token at a time using previous context\n- **Strategies**: Greedy, temperature, top-k, top-p each have different trade-offs\n- **Temperature**: Controls creativity (low = focused, high = random)\n- **Quality**: Balance diversity, coherence, and repetition avoidance\n\n### Strategy Guide\n\n**For factual, coherent text**:\n- Low temperature (0.3-0.7)\n- Top-p sampling (p=0.8-0.9)\n- Avoid high randomness\n\n**For creative writing**:\n- Higher temperature (0.8-1.2) \n- Top-k or top-p sampling\n- Allow more exploration\n\n**For reliable completion**:\n- Beam search for multiple candidates\n- Lower temperature for consistency\n\n### Next Steps\nNow you understand the complete transformer pipeline: tokenization â†’ attention â†’ blocks â†’ training â†’ generation!\n\nYou're ready to build and deploy your own language models! ðŸš€"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Techniques\n",
    "\n",
    "Let's explore some advanced techniques for improving generation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation Quality Metrics\n",
    "\n",
    "How do we measure the quality of generated text? Let's explore some metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the fascinating world of text generation:\n",
    "\n",
    "1. **Autoregressive Generation** - Step-by-step token prediction process\n",
    "2. **Generation Strategies** - Greedy, temperature, top-k, top-p sampling\n",
    "3. **Temperature Effects** - Controlling randomness and creativity\n",
    "4. **Beam Search** - Exploring multiple sequence possibilities\n",
    "5. **Advanced Techniques** - Repetition penalty, typical sampling\n",
    "6. **Quality Metrics** - Measuring repetition, diversity, coherence\n",
    "\n",
    "### Key Generation Insights:\n",
    "\n",
    "- **Strategy matters**: Different approaches produce different styles\n",
    "- **Temperature is crucial**: Controls the creativity-coherence tradeoff\n",
    "- **Top-p often best**: Adaptive cutoff based on probability mass\n",
    "- **Repetition is the enemy**: Use penalties and diverse sampling\n",
    "- **Quality is multifaceted**: No single metric captures everything\n",
    "\n",
    "### Generation Strategy Guide:\n",
    "\n",
    "**For coherent, factual text:**\n",
    "- Low temperature (0.3-0.7)\n",
    "- Top-p sampling (p=0.8-0.9)\n",
    "- Mild repetition penalty (1.1-1.3)\n",
    "\n",
    "**For creative writing:**\n",
    "- Higher temperature (0.8-1.2)\n",
    "- Top-p or top-k sampling\n",
    "- Strong repetition penalty (1.3-1.5)\n",
    "\n",
    "**For reliable completion:**\n",
    "- Beam search with beam width 3-5\n",
    "- Length penalties to avoid too-short sequences\n",
    "- Multiple candidates for selection\n",
    "\n",
    "### Modern Developments:\n",
    "\n",
    "- **Contrastive search**: Balances probability and diversity\n",
    "- **Typical sampling**: Avoids both too-common and too-rare tokens\n",
    "- **MCTS-based generation**: Uses tree search for better planning\n",
    "- **Classifier-free guidance**: Steers generation toward desired attributes\n",
    "\n",
    "### Quality Considerations:\n",
    "\n",
    "- **Coherence**: Does the text make sense?\n",
    "- **Consistency**: Are facts and details consistent?\n",
    "- **Relevance**: Does it address the prompt appropriately?\n",
    "- **Fluency**: Is the language natural and grammatical?\n",
    "- **Diversity**: Is the output varied and interesting?\n",
    "\n",
    "The art of text generation lies in balancing these competing objectives. The best approach depends on your specific use case, from creative writing to factual question answering to code generation.\n",
    "\n",
    "Congratulations! You've now completed a comprehensive journey through transformer architecture, training, and generation. You understand how these powerful models work from the ground up! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}