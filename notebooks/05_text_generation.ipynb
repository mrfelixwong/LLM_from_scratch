{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Text Generation: From Probabilities to Creative Text\n\nOnce trained, how do transformers generate text? They produce probability distributions over vocabulary, then use sampling strategies to choose the next word.\n\n## The Generation Process\n1. **Start with prompt**: \"The cat\"\n2. **Get probabilities**: Model outputs distribution over all possible next words\n3. **Sample next word**: Use strategy (greedy, temperature, top-k, etc.)\n4. **Add to sequence**: \"The cat sat\"\n5. **Repeat**: Until complete sentence or max length\n\n## What You'll Learn\n- **Autoregressive generation** - Step-by-step text creation\n- **Sampling strategies** - Greedy, temperature, top-k, top-p\n- **Temperature effects** - Controlling creativity vs coherence  \n- **Quality metrics** - Measuring generation quality"
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Optional\nimport math\nfrom collections import Counter\nimport random\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Autoregressive Generation Demo\n\nDemonstrate step-by-step text generation process with realistic probability distributions."
  },
  {
   "cell_type": "markdown",
   "source": "## Sampling Strategies Comparison\n\nCompare different strategies for selecting the next token from probability distributions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Simulate realistic step-by-step generation\nvocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"dog\", \"ran\"]\nword_to_id = {word: i for i, word in enumerate(vocab)}\n\ndef simulate_generation_step(context_words):\n    \"\"\"Simulate model probabilities based on context\"\"\"\n    torch.manual_seed(42 + len(context_words))\n    logits = torch.randn(len(vocab))\n    \n    # Add realistic biases based on context\n    if \"The\" in context_words:\n        logits[word_to_id[\"cat\"]] += 2.0\n    if \"cat\" in context_words:\n        logits[word_to_id[\"sat\"]] += 2.5\n    if \"sat\" in context_words:\n        logits[word_to_id[\"on\"]] += 2.0\n        \n    return logits\n\n# Demonstrate autoregressive generation step by step\ncontext = [\"The\"]\nprint(\"üîÑ Autoregressive Generation Process\")\nprint(\"=\" * 40)\n\nfor step in range(4):\n    logits = simulate_generation_step(context)\n    probs = F.softmax(logits, dim=0)\n    \n    print(f\"\\nStep {step + 1}:\")\n    print(f\"Context: '{' '.join(context)}'\")\n    print(\"Top 3 next word predictions:\")\n    \n    top_probs, top_indices = torch.topk(probs, 3)\n    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n        word = vocab[idx.item()]\n        print(f\"  {i+1}. '{word}': {prob.item():.3f}\")\n    \n    # Select most likely (greedy decoding)\n    next_word = vocab[top_indices[0].item()]\n    context.append(next_word)\n    print(f\"‚úì Selected: '{next_word}'\")\n    \n    if next_word == \".\":\n        break\n\nprint(f\"\\nüéØ Final generated text: '{' '.join(context)}'\")\nprint(\"‚úÖ That's autoregressive generation!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Sampling Strategy Implementations\n\nImplement and compare different token selection strategies."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Implement different sampling strategies\nvocab = [\"cat\", \"dog\", \"bird\", \"fish\", \"mouse\", \"horse\", \"cow\", \"sheep\"]\nprobs = torch.tensor([0.5, 0.3, 0.1, 0.05, 0.03, 0.01, 0.005, 0.005])\nlogits = torch.log(probs)\n\ndef greedy_selection(probs):\n    \"\"\"Always select the most likely token\"\"\"\n    return torch.argmax(probs)\n\ndef temperature_sampling(logits, temperature=1.0):\n    \"\"\"Scale logits by temperature before sampling\"\"\"\n    if temperature == 0:\n        return torch.argmax(logits)\n    scaled_logits = logits / temperature\n    probs = F.softmax(scaled_logits, dim=0)\n    return torch.multinomial(probs, 1).item()\n\ndef top_k_sampling(logits, k=3):\n    \"\"\"Sample from top-k most likely tokens\"\"\"\n    top_k_logits, top_k_indices = torch.topk(logits, k)\n    probs = F.softmax(top_k_logits, dim=0)\n    selected_idx = torch.multinomial(probs, 1).item()\n    return top_k_indices[selected_idx].item()\n\ndef top_p_sampling(logits, p=0.9):\n    \"\"\"Sample from smallest set with cumulative probability >= p\"\"\"\n    probs = F.softmax(logits, dim=0)\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n    \n    # Remove tokens with cumulative probability above p\n    sorted_indices_to_remove = cumulative_probs > p\n    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n    sorted_indices_to_remove[0] = 0\n    \n    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n    probs[indices_to_remove] = 0\n    probs = probs / probs.sum()\n    \n    return torch.multinomial(probs, 1).item()\n\nprint(\"üé≤ Generation Strategy Comparison\")\nprint(f\"Vocabulary: {vocab}\")\nprint(f\"True probabilities: {probs.tolist()}\")\n\nprint(f\"\\nGreedy: {vocab[greedy_selection(probs)]} (always deterministic)\")\n\nprint(\"\\nTemperature sampling (5 samples each):\")\nfor temp in [0.1, 0.5, 1.0, 2.0]:\n    samples = [vocab[temperature_sampling(logits, temp)] for _ in range(5)]\n    print(f\"  T={temp}: {samples}\")\n\nprint(f\"\\nTop-k (k=3): {[vocab[top_k_sampling(logits, k=3)] for _ in range(5)]}\")\nprint(f\"Top-p (p=0.9): {[vocab[top_p_sampling(logits, p=0.9)] for _ in range(5)]}\")\n\nprint(\"\\nüîë Key insight: Strategy dramatically affects diversity!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Temperature Effects Visualization\n\nVisualize how temperature affects probability distributions and creativity."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize temperature effects on probability distributions\nvocab = [\"the\", \"cat\", \"dog\", \"sat\", \"ran\", \"jumped\", \"quickly\", \"slowly\"]\nlogits = torch.tensor([3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5])\n\ntemperatures = [0.1, 0.5, 1.0, 2.0]\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, temp in enumerate(temperatures):\n    if temp == 0:\n        probs = torch.zeros_like(logits)\n        probs[torch.argmax(logits)] = 1.0\n    else:\n        scaled_logits = logits / temp\n        probs = F.softmax(scaled_logits, dim=0)\n    \n    bars = axes[i].bar(range(len(vocab)), probs, alpha=0.7, color='skyblue')\n    axes[i].set_xlabel('Tokens')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Temperature = {temp}')\n    axes[i].set_xticks(range(len(vocab)))\n    axes[i].set_xticklabels(vocab, rotation=45)\n    axes[i].grid(True, alpha=0.3)\n    \n    # Highlight most likely token\n    max_idx = torch.argmax(probs)\n    bars[max_idx].set_color('red')\n    \n    # Add entropy (measure of randomness)\n    entropy = -torch.sum(probs * torch.log(probs + 1e-10))\n    axes[i].text(0.7, 0.9, f'Entropy: {entropy:.2f}', transform=axes[i].transAxes, \n                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üå°Ô∏è Temperature Effects:\")\nprint(\"‚Ä¢ T < 1.0: More focused, deterministic (low entropy)\")\nprint(\"‚Ä¢ T = 1.0: Original distribution\")  \nprint(\"‚Ä¢ T > 1.0: More random, creative (high entropy)\")\nprint(\"‚Ä¢ T ‚Üí 0:   Greedy decoding (entropy = 0)\")\nprint(\"‚Ä¢ T ‚Üí ‚àû:   Uniform random (maximum entropy)\")\nprint(\"\\nüéØ Use case guide:\")\nprint(\"‚Ä¢ Factual tasks: T = 0.3-0.7\")\nprint(\"‚Ä¢ Creative writing: T = 0.8-1.2\")\nprint(\"‚Ä¢ Brainstorming: T = 1.0-2.0\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Text Quality Analysis\n\nAnalyze generated text quality using diversity and repetition metrics."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def analyze_text_quality(text):\n    \"\"\"Analyze text quality using various metrics\"\"\"\n    words = text.split()\n    unique_words = set(words)\n    \n    print(f\"Text: '{text}'\")\n    print(f\"Length: {len(words)} words\")\n    print(f\"Unique words: {len(unique_words)}\")\n    \n    # Repetition analysis\n    if len(words) > 0:\n        repetition_ratio = 1 - len(unique_words)/len(words)\n        print(f\"Repetition ratio: {repetition_ratio:.3f}\")\n        \n        # Diversity score  \n        diversity = len(unique_words) / len(words)\n        print(f\"Diversity score: {diversity:.3f}\")\n        \n        # Find repeated words\n        word_counts = Counter(words)\n        repeated_words = {word: count for word, count in word_counts.items() if count > 1}\n        if repeated_words:\n            print(f\"Repeated words: {repeated_words}\")\n    \n    print()\n\n# Analyze different quality examples\nexamples = [\n    \"The cat sat on the mat and looked around carefully\",  # Good quality\n    \"The the the cat cat sat sat on on the the\",          # High repetition\n    \"Quantum flux temporal paradox synthesis nebula\",      # Too random\n    \"Cat cat cat cat cat cat cat cat cat\",                 # Extreme repetition\n    \"The weather is nice today and tomorrow looks good\"    # Balanced\n]\n\nprint(\"üìä Text Quality Analysis Examples:\")\nprint(\"=\" * 45)\n\nfor i, text in enumerate(examples, 1):\n    print(f\"Example {i}:\")\n    analyze_text_quality(text)\n\nprint(\"Quality Guidelines:\")\nprint(\"‚úÖ Lower repetition ratio = better (< 0.3 good)\")\nprint(\"‚úÖ Higher diversity = more interesting (> 0.7 good)\") \nprint(\"‚úÖ Balance diversity with coherence\")\nprint(\"‚úÖ Avoid excessive repetition\")\nprint(\"‚ö†Ô∏è Human evaluation often most reliable metric\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nYou've mastered text generation with transformers!\n\n**Core Concepts**:\n- **Autoregressive**: Generate one token at a time using previous context\n- **Strategies**: Greedy, temperature, top-k, top-p each balance coherence vs creativity\n- **Temperature**: Controls randomness (low = focused, high = creative)\n- **Quality**: Balance diversity, coherence, and repetition avoidance\n\n**Strategy Selection Guide**:\n\n**For factual, coherent text**:\n- Low temperature (0.3-0.7)\n- Top-p sampling (p=0.8-0.9)\n- Minimize randomness\n\n**For creative writing**:\n- Higher temperature (0.8-1.2) \n- Top-k or top-p sampling\n- Allow exploration\n\n**For reliable completion**:\n- Greedy or low-temperature sampling\n- Focus on consistency\n\n**Next Steps**: You now understand the complete transformer pipeline from tokenization to generation! This knowledge applies to all modern language models.\n\nReady to build your own language model! üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Techniques\n",
    "\n",
    "Let's explore some advanced techniques for improving generation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the fascinating world of text generation:\n",
    "\n",
    "1. **Autoregressive Generation** - Step-by-step token prediction process\n",
    "2. **Generation Strategies** - Greedy, temperature, top-k, top-p sampling\n",
    "3. **Temperature Effects** - Controlling randomness and creativity\n",
    "4. **Beam Search** - Exploring multiple sequence possibilities\n",
    "5. **Advanced Techniques** - Repetition penalty, typical sampling\n",
    "6. **Quality Metrics** - Measuring repetition, diversity, coherence\n",
    "\n",
    "### Key Generation Insights:\n",
    "\n",
    "- **Strategy matters**: Different approaches produce different styles\n",
    "- **Temperature is crucial**: Controls the creativity-coherence tradeoff\n",
    "- **Top-p often best**: Adaptive cutoff based on probability mass\n",
    "- **Repetition is the enemy**: Use penalties and diverse sampling\n",
    "- **Quality is multifaceted**: No single metric captures everything\n",
    "\n",
    "### Generation Strategy Guide:\n",
    "\n",
    "**For coherent, factual text:**\n",
    "- Low temperature (0.3-0.7)\n",
    "- Top-p sampling (p=0.8-0.9)\n",
    "- Mild repetition penalty (1.1-1.3)\n",
    "\n",
    "**For creative writing:**\n",
    "- Higher temperature (0.8-1.2)\n",
    "- Top-p or top-k sampling\n",
    "- Strong repetition penalty (1.3-1.5)\n",
    "\n",
    "**For reliable completion:**\n",
    "- Beam search with beam width 3-5\n",
    "- Length penalties to avoid too-short sequences\n",
    "- Multiple candidates for selection\n",
    "\n",
    "### Modern Developments:\n",
    "\n",
    "- **Contrastive search**: Balances probability and diversity\n",
    "- **Typical sampling**: Avoids both too-common and too-rare tokens\n",
    "- **MCTS-based generation**: Uses tree search for better planning\n",
    "- **Classifier-free guidance**: Steers generation toward desired attributes\n",
    "\n",
    "### Quality Considerations:\n",
    "\n",
    "- **Coherence**: Does the text make sense?\n",
    "- **Consistency**: Are facts and details consistent?\n",
    "- **Relevance**: Does it address the prompt appropriately?\n",
    "- **Fluency**: Is the language natural and grammatical?\n",
    "- **Diversity**: Is the output varied and interesting?\n",
    "\n",
    "The art of text generation lies in balancing these competing objectives. The best approach depends on your specific use case, from creative writing to factual question answering to code generation.\n",
    "\n",
    "Congratulations! You've now completed a comprehensive journey through transformer architecture, training, and generation. You understand how these powerful models work from the ground up! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}