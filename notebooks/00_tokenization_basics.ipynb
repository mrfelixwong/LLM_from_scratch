{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Understanding Tokenization: From Text to Numbers\n",
    "\n",
    "Welcome to the foundation of language models! Before transformers can process text, they need to convert it into numbers. This process is called **tokenization**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What is Tokenization?** - Converting text to numerical representations\n",
    "2. **Character-Level Tokenization** - Simplest approach for learning\n",
    "3. **Subword Tokenization** - How modern LLMs handle text (BPE, GPT-2 style)\n",
    "4. **Building Vocabularies** - Creating token mappings from text\n",
    "5. **Special Tokens** - Handling padding, unknown words, and boundaries\n",
    "6. **Real-World Impact** - How tokenization affects model performance\n",
    "\n",
    "Let's start with the basics and work our way up to modern approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Try to import advanced tokenizers\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "    print(\"‚úÖ tiktoken available - we can use GPT-2 style tokenization\")\n",
    "except ImportError:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è tiktoken not available - we'll focus on character-level tokenization\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down text into smaller units called **tokens**, then converting these tokens into numerical IDs that neural networks can process.\n",
    "\n",
    "```\n",
    "Text: \"Hello world!\"\n",
    "  ‚Üì Tokenization\n",
    "Tokens: [\"Hello\", \" world\", \"!\"]\n",
    "  ‚Üì Convert to IDs\n",
    "Token IDs: [15496, 995, 0]\n",
    "```\n",
    "\n",
    "Let's see this in action with different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_tokenization_concept():\n",
    "    \"\"\"Show the basic concept of tokenization with different granularities.\"\"\"\n",
    "    \n",
    "    text = \"Hello world! üåç\"\n",
    "    print(f\"Original text: '{text}'\")\n",
    "    print()\n",
    "    \n",
    "    # Different tokenization approaches\n",
    "    approaches = [\n",
    "        (\"Character-level\", list(text)),\n",
    "        (\"Word-level\", text.split()),\n",
    "        (\"Subword (manual)\", [\"Hello\", \" world\", \"!\", \" üåç\"])\n",
    "    ]\n",
    "    \n",
    "    for name, tokens in approaches:\n",
    "        print(f\"{name:15}: {len(tokens):2d} tokens ‚Üí {tokens}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üîç Key Observations:\")\n",
    "    print(\"‚Ä¢ Character-level: Many tokens, handles any text, but very long sequences\")\n",
    "    print(\"‚Ä¢ Word-level: Fewer tokens, but struggles with new/rare words\")\n",
    "    print(\"‚Ä¢ Subword: Balanced approach - handles new words while keeping sequences manageable\")\n",
    "    \n",
    "    return approaches\n",
    "\n",
    "# Demonstrate the concept\n",
    "tokenization_examples = demonstrate_tokenization_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Character-Level Tokenization\n",
    "\n",
    "Let's start with the simplest approach: treating each character as a token. This is great for learning because:\n",
    "- ‚úÖ Simple to understand and implement\n",
    "- ‚úÖ No out-of-vocabulary (OOV) problems\n",
    "- ‚úÖ Works with any language\n",
    "- ‚ùå Creates very long sequences\n",
    "- ‚ùå Hard to learn word-level patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_corpus: str = None):\n",
    "        # Special tokens\n",
    "        self.PAD = '<PAD>'  # For padding sequences to same length\n",
    "        self.UNK = '<UNK>'  # For unknown characters\n",
    "        self.BOS = '<BOS>'  # Beginning of sequence\n",
    "        self.EOS = '<EOS>'  # End of sequence\n",
    "        \n",
    "        # Build vocabulary\n",
    "        if text_corpus:\n",
    "            self.build_vocab(text_corpus)\n",
    "        else:\n",
    "            # Default vocabulary with common characters\n",
    "            self.vocab = self._create_default_vocab()\n",
    "            self._create_mappings()\n",
    "    \n",
    "    def _create_default_vocab(self):\n",
    "        \"\"\"Create a default vocabulary with common characters.\"\"\"\n",
    "        # Letters, digits, punctuation, and special tokens\n",
    "        chars = []\n",
    "        chars.extend([chr(i) for i in range(32, 127)])  # Printable ASCII\n",
    "        chars.extend([self.PAD, self.UNK, self.BOS, self.EOS])\n",
    "        return chars\n",
    "    \n",
    "    def build_vocab(self, text: str):\n",
    "        \"\"\"Build vocabulary from text corpus.\"\"\"\n",
    "        # Get unique characters from text\n",
    "        unique_chars = sorted(set(text))\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.vocab = [self.PAD, self.UNK, self.BOS, self.EOS] + unique_chars\n",
    "        self._create_mappings()\n",
    "        \n",
    "        print(f\"Built vocabulary from {len(text):,} characters\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} unique characters\")\n",
    "    \n",
    "    def _create_mappings(self):\n",
    "        \"\"\"Create character ‚Üî ID mappings.\"\"\"\n",
    "        self.char_to_id = {char: i for i, char in enumerate(self.vocab)}\n",
    "        self.id_to_char = {i: char for i, char in enumerate(self.vocab)}\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.char_to_id[self.BOS])\n",
    "        \n",
    "        for char in text:\n",
    "            token_id = self.char_to_id.get(char, self.char_to_id[self.UNK])\n",
    "            tokens.append(token_id)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.char_to_id[self.EOS])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        special_ids = {\n",
    "            self.char_to_id[self.PAD],\n",
    "            self.char_to_id[self.BOS],\n",
    "            self.char_to_id[self.EOS]\n",
    "        }\n",
    "        \n",
    "        chars = []\n",
    "        for token_id in token_ids:\n",
    "            if skip_special_tokens and token_id in special_ids:\n",
    "                continue\n",
    "            chars.append(self.id_to_char.get(token_id, self.UNK))\n",
    "        \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Return list of character tokens.\"\"\"\n",
    "        return [self.BOS] + list(text) + [self.EOS]\n",
    "\n",
    "# Create and test character tokenizer\n",
    "char_tokenizer = SimpleCharacterTokenizer()\n",
    "\n",
    "# Test with example text\n",
    "test_text = \"Hello, world! üöÄ\"\n",
    "print(f\"Original text: '{test_text}'\")\n",
    "print(f\"Vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "print()\n",
    "\n",
    "# Tokenization process\n",
    "tokens = char_tokenizer.tokenize(test_text)\n",
    "token_ids = char_tokenizer.encode(test_text)\n",
    "decoded_text = char_tokenizer.decode(token_ids)\n",
    "\n",
    "print(\"Tokenization process:\")\n",
    "print(f\"1. Tokens:     {tokens}\")\n",
    "print(f\"2. Token IDs:  {token_ids}\")\n",
    "print(f\"3. Decoded:    '{decoded_text}'\")\n",
    "print()\n",
    "print(f\"‚úÖ Round-trip successful: {test_text == decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Visualizing Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_character_tokenization():\n",
    "    \"\"\"Visualize how character tokenization works.\"\"\"\n",
    "    \n",
    "    text = \"AI is amazing!\"\n",
    "    tokens = char_tokenizer.tokenize(text)\n",
    "    token_ids = char_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print()\n",
    "    \n",
    "    # Show character-by-character mapping\n",
    "    print(\"Character ‚Üí Token ID mapping:\")\n",
    "    print(\"‚îÄ\" * 30)\n",
    "    \n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        if token in ['<BOS>', '<EOS>']:\n",
    "            print(f\"{i:2d}: {token:>6} ‚Üí {token_id:3d}  (special)\")\n",
    "        else:\n",
    "            print(f\"{i:2d}: {repr(token):>6} ‚Üí {token_id:3d}\")\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    print(f\"\\nüìä Vocabulary Analysis:\")\n",
    "    print(f\"Total vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "    print(f\"Sequence length: {len(token_ids)} tokens\")\n",
    "    print(f\"Original text length: {len(text)} characters\")\n",
    "    \n",
    "    # Show some vocabulary examples\n",
    "    print(\"\\nüî§ Sample vocabulary (first 20 tokens):\")\n",
    "    for i in range(min(20, len(char_tokenizer.vocab))):\n",
    "        char = char_tokenizer.vocab[i]\n",
    "        if char in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']:\n",
    "            print(f\"{i:3d}: {char}\")\n",
    "        else:\n",
    "            print(f\"{i:3d}: {repr(char)}\")\n",
    "\n",
    "visualize_character_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Building Vocabulary from Real Text\n",
    "\n",
    "Let's see how to build a vocabulary from a real text corpus and analyze character frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_corpus():\n",
    "    \"\"\"Analyze character frequencies in a text corpus.\"\"\"\n",
    "    \n",
    "    # Sample text corpus (you could load from a file)\n",
    "    corpus = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet!\n",
    "    Machine learning is transforming how we process natural language. \n",
    "    Transformers use attention mechanisms to understand context and relationships between words.\n",
    "    Deep learning models can generate human-like text with remarkable accuracy.\n",
    "    The future of AI looks very promising! ü§ñ‚ú®\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    print(f\"Corpus length: {len(corpus):,} characters\")\n",
    "    print(f\"Sample text: {repr(corpus[:100])}...\")\n",
    "    print()\n",
    "    \n",
    "    # Build tokenizer from this corpus\n",
    "    corpus_tokenizer = SimpleCharacterTokenizer(corpus)\n",
    "    \n",
    "    # Analyze character frequencies\n",
    "    char_counts = Counter(corpus)\n",
    "    total_chars = len(corpus)\n",
    "    \n",
    "    print(\"üìä Character Frequency Analysis:\")\n",
    "    print(\"‚îÄ\" * 40)\n",
    "    \n",
    "    # Show top 15 most common characters\n",
    "    for char, count in char_counts.most_common(15):\n",
    "        percentage = (count / total_chars) * 100\n",
    "        char_repr = repr(char) if char.isprintable() else f\"'\\\\x{ord(char):02x}'\"\n",
    "        print(f\"{char_repr:>8}: {count:3d} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Visualize character frequencies\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Top characters\n",
    "    plt.subplot(2, 1, 1)\n",
    "    top_chars = char_counts.most_common(20)\n",
    "    chars, counts = zip(*top_chars)\n",
    "    char_labels = [repr(c) if c.isprintable() else f\"\\\\x{ord(c):02x}\" for c in chars]\n",
    "    \n",
    "    plt.bar(range(len(chars)), counts, color='skyblue')\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 20 Most Frequent Characters')\n",
    "    plt.xticks(range(len(chars)), char_labels, rotation=45)\n",
    "    \n",
    "    # Character distribution\n",
    "    plt.subplot(2, 1, 2)\n",
    "    frequencies = list(char_counts.values())\n",
    "    plt.hist(frequencies, bins=20, color='lightgreen', alpha=0.7)\n",
    "    plt.xlabel('Character Frequency')\n",
    "    plt.ylabel('Number of Characters')\n",
    "    plt.title('Distribution of Character Frequencies')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test encoding/decoding\n",
    "    test_sentence = \"Hello AI! ü§ñ\"\n",
    "    encoded = corpus_tokenizer.encode(test_sentence)\n",
    "    decoded = corpus_tokenizer.decode(encoded)\n",
    "    \n",
    "    print(f\"\\nüß™ Encoding Test:\")\n",
    "    print(f\"Original: '{test_sentence}'\")\n",
    "    print(f\"Encoded:  {encoded}\")\n",
    "    print(f\"Decoded:  '{decoded}'\")\n",
    "    print(f\"Success:  {test_sentence == decoded}\")\n",
    "    \n",
    "    return corpus_tokenizer, char_counts\n",
    "\n",
    "# Analyze the corpus\n",
    "corpus_tokenizer, char_frequencies = analyze_text_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Introduction to Subword Tokenization\n",
    "\n",
    "While character-level tokenization is simple, modern language models use **subword tokenization**. This approach:\n",
    "- ‚úÖ Handles new words by breaking them into known subparts\n",
    "- ‚úÖ Creates shorter sequences than character-level\n",
    "- ‚úÖ Captures morphological patterns (prefixes, suffixes)\n",
    "- ‚úÖ Works well across different languages\n",
    "\n",
    "Let's implement a simple version of **Byte Pair Encoding (BPE)**, which is used by GPT-2 and many other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    \"\"\"Simple Byte Pair Encoding tokenizer for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.word_freqs = {}\n",
    "    \n",
    "    def train(self, texts: List[str], vocab_size: int = 1000):\n",
    "        \"\"\"Train BPE on a corpus of texts.\"\"\"\n",
    "        print(f\"Training BPE tokenizer on {len(texts)} texts...\")\n",
    "        \n",
    "        # Step 1: Count word frequencies and initialize with characters\n",
    "        self.word_freqs = self._count_word_frequencies(texts)\n",
    "        \n",
    "        # Step 2: Initialize vocabulary with characters\n",
    "        self.vocab = self._get_character_vocab()\n",
    "        \n",
    "        # Step 3: Iteratively merge most frequent pairs\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            # Find most frequent pair\n",
    "            pair_counts = self._count_pairs()\n",
    "            if not pair_counts:\n",
    "                break\n",
    "                \n",
    "            most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
    "            \n",
    "            # Merge the pair\n",
    "            self._merge_pair(most_frequent_pair)\n",
    "            \n",
    "            # Add to vocabulary and merges\n",
    "            new_token = ''.join(most_frequent_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.merges.append(most_frequent_pair)\n",
    "        \n",
    "        print(f\"Training complete! Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Number of merges: {len(self.merges)}\")\n",
    "    \n",
    "    def _count_word_frequencies(self, texts: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Count word frequencies in the corpus.\"\"\"\n",
    "        word_freqs = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            # Simple word splitting (could be more sophisticated)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "            word_freqs.update(words)\n",
    "        \n",
    "        # Convert to character-separated format for BPE\n",
    "        char_word_freqs = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            # Add space to mark word boundary\n",
    "            char_word = ' '.join(word) + ' </w>'\n",
    "            char_word_freqs[char_word] = freq\n",
    "        \n",
    "        return char_word_freqs\n",
    "    \n",
    "    def _get_character_vocab(self) -> Dict[str, int]:\n",
    "        \"\"\"Initialize vocabulary with all characters.\"\"\"\n",
    "        chars = set()\n",
    "        for word in self.word_freqs:\n",
    "            chars.update(word.split())\n",
    "        \n",
    "        # Add special tokens\n",
    "        chars.update(['<pad>', '<unk>', '<bos>', '<eos>'])\n",
    "        \n",
    "        return {char: i for i, char in enumerate(sorted(chars))}\n",
    "    \n",
    "    def _count_pairs(self) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Count frequency of adjacent pairs in the vocabulary.\"\"\"\n",
    "        pair_counts = Counter()\n",
    "        \n",
    "        for word, freq in self.word_freqs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pair_counts[pair] += freq\n",
    "        \n",
    "        return pair_counts\n",
    "    \n",
    "    def _merge_pair(self, pair: Tuple[str, str]):\n",
    "        \"\"\"Merge a pair in all words.\"\"\"\n",
    "        new_word_freqs = {}\n",
    "        \n",
    "        for word, freq in self.word_freqs.items():\n",
    "            new_word = self._merge_word(word, pair)\n",
    "            new_word_freqs[new_word] = freq\n",
    "        \n",
    "        self.word_freqs = new_word_freqs\n",
    "    \n",
    "    def _merge_word(self, word: str, pair: Tuple[str, str]) -> str:\n",
    "        \"\"\"Merge a specific pair in a word.\"\"\"\n",
    "        symbols = word.split()\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == pair:\n",
    "                # Merge the pair\n",
    "                new_symbols.append(symbols[i] + symbols[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        \n",
    "        return ' '.join(new_symbols)\n",
    "    \n",
    "    def encode(self, text: str) -> List[str]:\n",
    "        \"\"\"Encode text using learned BPE merges.\"\"\"\n",
    "        # Simple word splitting\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "        \n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            # Start with character-separated word\n",
    "            word_tokens = ' '.join(word) + ' </w>'\n",
    "            \n",
    "            # Apply merges in order\n",
    "            for pair in self.merges:\n",
    "                word_tokens = self._merge_word(word_tokens, pair)\n",
    "            \n",
    "            tokens.extend(word_tokens.split())\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def show_merges(self, n: int = 10):\n",
    "        \"\"\"Show the first n merges learned.\"\"\"\n",
    "        print(f\"First {n} BPE merges:\")\n",
    "        print(\"‚îÄ\" * 30)\n",
    "        for i, (left, right) in enumerate(self.merges[:n]):\n",
    "            merged = left + right\n",
    "            print(f\"{i+1:2d}: '{left}' + '{right}' ‚Üí '{merged}'\")\n",
    "\n",
    "# Train a simple BPE tokenizer\n",
    "training_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning is amazing and transformative\",\n",
    "    \"Natural language processing with transformers\",\n",
    "    \"Deep learning models learn patterns from data\",\n",
    "    \"Artificial intelligence is changing the world\",\n",
    "    \"Tokenization is the first step in text processing\"\n",
    "]\n",
    "\n",
    "bpe_tokenizer = SimpleBPETokenizer()\n",
    "bpe_tokenizer.train(training_texts, vocab_size=100)\n",
    "\n",
    "# Show some learned merges\n",
    "bpe_tokenizer.show_merges(10)\n",
    "\n",
    "# Test encoding\n",
    "test_text = \"learning transformers\"\n",
    "bpe_tokens = bpe_tokenizer.encode(test_text)\n",
    "char_tokens = char_tokenizer.tokenize(test_text)\n",
    "\n",
    "print(f\"\\nüß™ Tokenization Comparison:\")\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"BPE tokens:       {bpe_tokens} ({len(bpe_tokens)} tokens)\")\n",
    "print(f\"Character tokens: {char_tokens} ({len(char_tokens)} tokens)\")\n",
    "print(f\"\\nüìä BPE creates {len(char_tokens)/len(bpe_tokens):.1f}x shorter sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Real-World Tokenization: GPT-2 Style\n",
    "\n",
    "Let's see how modern language models like GPT-2 actually tokenize text using the `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gpt2_tokenization():\n",
    "    \"\"\"Show how GPT-2 tokenizes text using tiktoken.\"\"\"\n",
    "    \n",
    "    if not TIKTOKEN_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è tiktoken not available. Install with: pip install tiktoken\")\n",
    "        return\n",
    "    \n",
    "    # Load GPT-2 tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    print(f\"GPT-2 Tokenizer:\")\n",
    "    print(f\"Vocabulary size: {enc.n_vocab:,} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Test different types of text\n",
    "    test_cases = [\n",
    "        \"Hello world!\",\n",
    "        \"Tokenization is fascinating\",\n",
    "        \"The transformer architecture revolutionized NLP\",\n",
    "        \"ü§ñ AI + ML = üöÄ\",\n",
    "        \"antidisestablishmentarianism\",  # Long word\n",
    "        \"New_words_with_underscores\",\n",
    "        \"CamelCaseWords\",\n",
    "        \"123-456-7890\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üîç GPT-2 Tokenization Examples:\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    \n",
    "    for text in test_cases:\n",
    "        # Encode to token IDs\n",
    "        token_ids = enc.encode(text)\n",
    "        \n",
    "        # Decode back to check\n",
    "        decoded = enc.decode(token_ids)\n",
    "        \n",
    "        # Get token strings\n",
    "        tokens = [enc.decode([token_id]) for token_id in token_ids]\n",
    "        \n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Length: {len(token_ids)} tokens (vs {len(text)} chars)\")\n",
    "        print(f\"Round-trip: {text == decoded}\")\n",
    "        print()\n",
    "    \n",
    "    # Compare tokenization approaches\n",
    "    comparison_text = \"The transformer model uses self-attention mechanisms\"\n",
    "    \n",
    "    gpt2_tokens = enc.encode(comparison_text)\n",
    "    char_tokens = char_tokenizer.encode(comparison_text, add_special_tokens=False)\n",
    "    \n",
    "    print(\"üìä Tokenization Comparison:\")\n",
    "    print(f\"Text: '{comparison_text}'\")\n",
    "    print(f\"Characters: {len(comparison_text)} chars\")\n",
    "    print(f\"GPT-2 tokens: {len(gpt2_tokens)} tokens\")\n",
    "    print(f\"Character tokens: {len(char_tokens)} tokens\")\n",
    "    print(f\"GPT-2 efficiency: {len(char_tokens)/len(gpt2_tokens):.1f}x compression\")\n",
    "    \n",
    "    # Show actual GPT-2 tokens\n",
    "    gpt2_token_strings = [enc.decode([tid]) for tid in gpt2_tokens]\n",
    "    print(f\"\\nGPT-2 tokens: {gpt2_token_strings}\")\n",
    "    \n",
    "    return enc\n",
    "\n",
    "# Demonstrate GPT-2 tokenization\n",
    "gpt2_encoder = demonstrate_gpt2_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Special Tokens and Their Importance\n",
    "\n",
    "Special tokens serve crucial roles in language models. Let's understand what they do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_special_tokens():\n",
    "    \"\"\"Show the role of special tokens in language models.\"\"\"\n",
    "    \n",
    "    print(\"üéØ Special Tokens and Their Roles:\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    \n",
    "    special_tokens = {\n",
    "        '<PAD>': 'Padding - Make all sequences the same length',\n",
    "        '<UNK>': 'Unknown - Handle out-of-vocabulary words',\n",
    "        '<BOS>': 'Beginning of Sequence - Mark sequence start',\n",
    "        '<EOS>': 'End of Sequence - Mark sequence end',\n",
    "        '<MASK>': 'Mask - For masked language modeling (BERT)',\n",
    "        '<CLS>': 'Classification - For sequence classification',\n",
    "        '<SEP>': 'Separator - Separate different parts of input'\n",
    "    }\n",
    "    \n",
    "    for token, description in special_tokens.items():\n",
    "        print(f\"{token:8} : {description}\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "    \n",
    "    # Demonstrate padding\n",
    "    sentences = [\n",
    "        \"AI is cool\",\n",
    "        \"Machine learning\",\n",
    "        \"Natural language processing with transformers\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüì¶ Padding Example (making sequences same length):\")\n",
    "    \n",
    "    # Tokenize all sentences\n",
    "    tokenized = [char_tokenizer.encode(s, add_special_tokens=False) for s in sentences]\n",
    "    \n",
    "    # Find max length\n",
    "    max_length = max(len(tokens) for tokens in tokenized)\n",
    "    pad_id = char_tokenizer.char_to_id['<PAD>']\n",
    "    \n",
    "    print(f\"Max sequence length: {max_length}\")\n",
    "    print(f\"PAD token ID: {pad_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Pad sequences\n",
    "    for i, (sentence, tokens) in enumerate(zip(sentences, tokenized)):\n",
    "        # Pad to max length\n",
    "        padded_tokens = tokens + [pad_id] * (max_length - len(tokens))\n",
    "        \n",
    "        print(f\"Sentence {i+1}: '{sentence}'\")\n",
    "        print(f\"Original:  {tokens} (len: {len(tokens)})\")\n",
    "        print(f\"Padded:    {padded_tokens} (len: {len(padded_tokens)})\")\n",
    "        print()\n",
    "    \n",
    "    # Demonstrate unknown tokens\n",
    "    print(\"‚ùì Unknown Token Example:\")\n",
    "    \n",
    "    # Text with emoji that might not be in vocabulary\n",
    "    text_with_unknown = \"Hello ü¶Ñ world\"\n",
    "    \n",
    "    # Create a limited tokenizer\n",
    "    limited_vocab = list(\"abcdefghijklmnopqrstuvwxyz HELLO\")\n",
    "    limited_tokenizer = SimpleCharacterTokenizer()\n",
    "    limited_tokenizer.vocab = ['<PAD>', '<UNK>', '<BOS>', '<EOS>'] + limited_vocab\n",
    "    limited_tokenizer._create_mappings()\n",
    "    \n",
    "    tokens = limited_tokenizer.encode(text_with_unknown)\n",
    "    decoded = limited_tokenizer.decode(tokens)\n",
    "    \n",
    "    print(f\"Original: '{text_with_unknown}'\")\n",
    "    print(f\"Tokens:   {tokens}\")\n",
    "    print(f\"Decoded:  '{decoded}'\")\n",
    "    print(f\"Note: Emoji became {limited_tokenizer.UNK} (unknown token)\")\n",
    "\n",
    "demonstrate_special_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Impact on Model Architecture\n",
    "\n",
    "Tokenization decisions directly affect model architecture and performance. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization_impact():\n",
    "    \"\"\"Analyze how tokenization affects model architecture and performance.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è TOKENIZATION IMPACT ON MODEL ARCHITECTURE\")\n",
    "    print(\"‚ïê\" * 60)\n",
    "    \n",
    "    # Different tokenization scenarios\n",
    "    scenarios = [\n",
    "        (\"Character-level\", 128, \"Small vocab, long sequences\"),\n",
    "        (\"Word-level\", 50000, \"Large vocab, medium sequences\"),\n",
    "        (\"Subword (GPT-2)\", 50257, \"Balanced vocab and sequences\"),\n",
    "        (\"Subword (Large)\", 100000, \"Very large vocab, short sequences\")\n",
    "    ]\n",
    "    \n",
    "    d_model = 512  # Model dimension\n",
    "    sample_text = \"The transformer architecture revolutionized natural language processing\"\n",
    "    \n",
    "    print(f\"Sample text: '{sample_text}'\")\n",
    "    print(f\"Text length: {len(sample_text)} characters\")\n",
    "    print()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, vocab_size, description in scenarios:\n",
    "        # Estimate sequence length based on tokenization type\n",
    "        if \"Character\" in name:\n",
    "            seq_len = len(sample_text) + 2  # +2 for BOS/EOS\n",
    "        elif \"Word\" in name:\n",
    "            seq_len = len(sample_text.split()) + 2\n",
    "        else:  # Subword\n",
    "            # Estimate based on compression ratio\n",
    "            if vocab_size <= 50257:\n",
    "                seq_len = int(len(sample_text) / 4) + 2  # ~4 chars per token\n",
    "            else:\n",
    "                seq_len = int(len(sample_text) / 6) + 2  # ~6 chars per token\n",
    "        \n",
    "        # Calculate memory requirements\n",
    "        embedding_params = vocab_size * d_model\n",
    "        attention_memory = seq_len ** 2  # Attention matrix\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'vocab_size': vocab_size,\n",
    "            'seq_len': seq_len,\n",
    "            'embedding_params': embedding_params,\n",
    "            'attention_memory': attention_memory,\n",
    "            'description': description\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"üìä Architecture Impact Analysis:\")\n",
    "    print(\"‚îÄ\" * 80)\n",
    "    print(f\"{'Tokenization':<15} {'Vocab':<8} {'Seq Len':<8} {'Embed Params':<12} {'Attn Memory':<12} {'Description':<25}\")\n",
    "    print(\"‚îÄ\" * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<15} {r['vocab_size']:<8,} {r['seq_len']:<8} {r['embedding_params']:<12,} {r['attention_memory']:<12,} {r['description']:<25}\")\n",
    "    \n",
    "    # Visualize trade-offs\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    names = [r['name'] for r in results]\n",
    "    \n",
    "    # Vocabulary size\n",
    "    vocab_sizes = [r['vocab_size'] for r in results]\n",
    "    ax1.bar(names, vocab_sizes, color='skyblue')\n",
    "    ax1.set_title('Vocabulary Size')\n",
    "    ax1.set_ylabel('Number of Tokens')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Sequence length\n",
    "    seq_lens = [r['seq_len'] for r in results]\n",
    "    ax2.bar(names, seq_lens, color='lightgreen')\n",
    "    ax2.set_title('Sequence Length')\n",
    "    ax2.set_ylabel('Number of Tokens')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embed_params = [r['embedding_params'] for r in results]\n",
    "    ax3.bar(names, embed_params, color='lightcoral')\n",
    "    ax3.set_title('Embedding Parameters')\n",
    "    ax3.set_ylabel('Number of Parameters')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Attention memory\n",
    "    attn_memory = [r['attention_memory'] for r in results]\n",
    "    ax4.bar(names, attn_memory, color='gold')\n",
    "    ax4.set_title('Attention Memory (O(n¬≤))')\n",
    "    ax4.set_ylabel('Memory Units')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key Trade-offs:\")\n",
    "    print(\"‚Ä¢ Large vocabulary ‚Üí More embedding parameters\")\n",
    "    print(\"‚Ä¢ Long sequences ‚Üí Quadratic attention memory growth\")\n",
    "    print(\"‚Ä¢ Character-level ‚Üí Simple but inefficient\")\n",
    "    print(\"‚Ä¢ Subword ‚Üí Best balance for most applications\")\n",
    "    print(\"‚Ä¢ Optimal choice depends on task, compute, and language\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze tokenization impact\n",
    "impact_analysis = analyze_tokenization_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Practical Considerations\n",
    "\n",
    "Let's wrap up with real-world considerations when choosing tokenization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def practical_tokenization_guide():\n",
    "    \"\"\"Provide practical guidance for choosing tokenization strategies.\"\"\"\n",
    "    \n",
    "    print(\"üéØ PRACTICAL TOKENIZATION GUIDE\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    \n",
    "    guidelines = {\n",
    "        \"üî§ Character-Level\": {\n",
    "            \"Best for\": [\n",
    "                \"Learning and experimentation\",\n",
    "                \"Languages without clear word boundaries\",\n",
    "                \"Small datasets\",\n",
    "                \"Character-level tasks (spelling correction)\"\n",
    "            ],\n",
    "            \"Pros\": [\"No OOV words\", \"Simple implementation\", \"Language agnostic\"],\n",
    "            \"Cons\": [\"Long sequences\", \"Hard to learn word-level patterns\", \"Computationally expensive\"]\n",
    "        },\n",
    "        \n",
    "        \"üìù Word-Level\": {\n",
    "            \"Best for\": [\n",
    "                \"Well-defined vocabularies\",\n",
    "                \"Domain-specific applications\",\n",
    "                \"When semantic meaning is crucial\"\n",
    "            ],\n",
    "            \"Pros\": [\"Preserves word meaning\", \"Shorter sequences\", \"Interpretable\"],\n",
    "            \"Cons\": [\"Large vocabularies\", \"OOV problems\", \"Language-specific\", \"Poor morphology handling\"]\n",
    "        },\n",
    "        \n",
    "        \"üß© Subword (BPE/WordPiece)\": {\n",
    "            \"Best for\": [\n",
    "                \"General-purpose language models\",\n",
    "                \"Multilingual applications\",\n",
    "                \"Production systems\",\n",
    "                \"Most modern NLP tasks\"\n",
    "            ],\n",
    "            \"Pros\": [\"Handles OOV words\", \"Balanced vocab/sequence length\", \"Good morphology\", \"Cross-lingual\"],\n",
    "            \"Cons\": [\"More complex\", \"Requires training\", \"Less interpretable\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, info in guidelines.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"‚îÄ\" * 30)\n",
    "        \n",
    "        print(\"‚úÖ Best for:\")\n",
    "        for item in info[\"Best for\"]:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Pros:\")\n",
    "        for pro in info[\"Pros\"]:\n",
    "            print(f\"   ‚Ä¢ {pro}\")\n",
    "        \n",
    "        print(\"\\n‚ùå Cons:\")\n",
    "        for con in info[\"Cons\"]:\n",
    "            print(f\"   ‚Ä¢ {con}\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚ïê\" * 50)\n",
    "    print(\"üé¨ TOKENIZATION IN TRANSFORMER PIPELINE\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    \n",
    "    pipeline_steps = [\n",
    "        (\"1. Raw Text\", \"'The transformer is amazing!'\"),\n",
    "        (\"2. Tokenization\", \"['The', ' transform', 'er', ' is', ' amazing', '!']\"),\n",
    "        (\"3. Token IDs\", \"[464, 5516, 263, 318, 4998, 0]\"),\n",
    "        (\"4. Embeddings\", \"[[0.1, -0.3, 0.8, ...], [0.2, 0.1, -0.4, ...], ...]\"),\n",
    "        (\"5. Transformer\", \"Attention ‚Üí Feed-Forward ‚Üí Layer Norm ‚Üí ...\"),\n",
    "        (\"6. Output Logits\", \"Probability distribution over vocabulary\"),\n",
    "        (\"7. Decoding\", \"Convert back to text\"),\n",
    "        (\"8. Final Text\", \"'Transformers revolutionized AI!'\") \n",
    "    ]\n",
    "    \n",
    "    for step, description in pipeline_steps:\n",
    "        print(f\"{step:<15} ‚Üí {description}\")\n",
    "    \n",
    "    print(\"\\nüîë KEY TAKEAWAYS:\")\n",
    "    print(\"‚îÄ\" * 20)\n",
    "    print(\"1. Tokenization is the bridge between human language and AI\")\n",
    "    print(\"2. Choice affects model size, speed, and performance\")\n",
    "    print(\"3. Subword tokenization is the current best practice\")\n",
    "    print(\"4. Consider your specific use case and constraints\")\n",
    "    print(\"5. Vocabulary size directly impacts embedding layer size\")\n",
    "    print(\"6. Sequence length affects attention computation (O(n¬≤))\")\n",
    "\n",
    "practical_tokenization_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of tokenization - the essential first step in all language models.\n",
    "\n",
    "### What We Covered:\n",
    "\n",
    "1. **Tokenization Basics** - Converting text to numbers for neural networks\n",
    "2. **Character-Level** - Simple approach treating each character as a token\n",
    "3. **Subword Tokenization** - Modern approach using BPE and similar algorithms\n",
    "4. **Vocabulary Building** - Creating token mappings from text corpora\n",
    "5. **Special Tokens** - Handling padding, unknowns, and sequence boundaries\n",
    "6. **Architecture Impact** - How tokenization affects model size and performance\n",
    "7. **Practical Guidelines** - Choosing the right approach for your needs\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Tokenization is crucial** - It's the foundation that enables all text processing\n",
    "- **Trade-offs matter** - Vocabulary size vs sequence length vs interpretability\n",
    "- **Subword wins** - Best balance for most modern applications\n",
    "- **Context matters** - Choose based on your specific task and constraints\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand how text becomes numbers, you're ready to explore:\n",
    "- **01_attention_mechanism** - How transformers process token sequences\n",
    "- **02_transformer_blocks** - Building complete processing units\n",
    "- **03_positional_encoding** - Adding position information to tokens\n",
    "\n",
    "The journey from text to intelligent AI responses starts with tokenization - and you've just mastered it! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}