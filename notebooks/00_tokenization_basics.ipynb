{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Understanding Tokenization: From Text to Numbers\n\nBefore transformers can process text, they need to convert it into numbers. This process is called **tokenization** - the essential bridge between human language and AI.\n\n## What You'll Learn\n\n1. **Tokenization Fundamentals** - Converting text to numerical representations\n2. **Character-Level Approach** - Simplest method for learning\n3. **Subword Tokenization** - Modern approach used by GPT and BERT\n4. **Special Tokens** - Handling boundaries and unknown content\n\nLet's master the foundation that makes all language models possible!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Try to import advanced tokenizers\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "    print(\"\u2705 tiktoken available - we can use GPT-2 style tokenization\")\n",
    "except ImportError:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    print(\"\u26a0\ufe0f tiktoken not available - we'll focus on character-level tokenization\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Character-Level Tokenization\n\n**Tokenization** converts text into numerical tokens that neural networks can process: `Text \u2192 Tokens \u2192 IDs \u2192 Embeddings`\n\nLet's implement the simplest approach - character-level tokenization:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "def demonstrate_tokenization_concept():\n    \"\"\"Show the basic concept with different approaches.\"\"\"\n    \n    text = \"Hello world! \ud83c\udf0d\"\n    \n    approaches = [\n        (\"Character-level\", list(text), \"Simple, handles any text, long sequences\"),\n        (\"Subword (GPT-style)\", [\"Hello\", \" world\", \"!\", \" \ud83c\udf0d\"], \"Balanced approach, modern standard\")\n    ]\n    \n    print(f\"Text: '{text}' ({len(text)} chars)\")\n    print(\"\\nTokenization Approaches:\")\n    for name, tokens, description in approaches:\n        print(f\"{name:15}: {len(tokens):2d} tokens \u2192 {description}\")\n    \n    return approaches[0][1]  # Return character tokens for next cell\n\nchar_tokens = demonstrate_tokenization_concept()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Implementing Character Tokenization\n\nCharacter-level is perfect for learning because it's simple and has no out-of-vocabulary issues:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_corpus: str = None):\n",
    "        # Special tokens\n",
    "        self.PAD = '<PAD>'  # For padding sequences to same length\n",
    "        self.UNK = '<UNK>'  # For unknown characters\n",
    "        self.BOS = '<BOS>'  # Beginning of sequence\n",
    "        self.EOS = '<EOS>'  # End of sequence\n",
    "        \n",
    "        # Build vocabulary\n",
    "        if text_corpus:\n",
    "            self.build_vocab(text_corpus)\n",
    "        else:\n",
    "            # Default vocabulary with common characters\n",
    "            self.vocab = self._create_default_vocab()\n",
    "            self._create_mappings()\n",
    "    \n",
    "    def _create_default_vocab(self):\n",
    "        \"\"\"Create a default vocabulary with common characters.\"\"\"\n",
    "        # Letters, digits, punctuation, and special tokens\n",
    "        chars = []\n",
    "        chars.extend([chr(i) for i in range(32, 127)])  # Printable ASCII\n",
    "        chars.extend([self.PAD, self.UNK, self.BOS, self.EOS])\n",
    "        return chars\n",
    "    \n",
    "    def build_vocab(self, text: str):\n",
    "        \"\"\"Build vocabulary from text corpus.\"\"\"\n",
    "        # Get unique characters from text\n",
    "        unique_chars = sorted(set(text))\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.vocab = [self.PAD, self.UNK, self.BOS, self.EOS] + unique_chars\n",
    "        self._create_mappings()\n",
    "        \n",
    "        print(f\"Built vocabulary from {len(text):,} characters\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} unique characters\")\n",
    "    \n",
    "    def _create_mappings(self):\n",
    "        \"\"\"Create character \u2194 ID mappings.\"\"\"\n",
    "        self.char_to_id = {char: i for i, char in enumerate(self.vocab)}\n",
    "        self.id_to_char = {i: char for i, char in enumerate(self.vocab)}\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.char_to_id[self.BOS])\n",
    "        \n",
    "        for char in text:\n",
    "            token_id = self.char_to_id.get(char, self.char_to_id[self.UNK])\n",
    "            tokens.append(token_id)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.char_to_id[self.EOS])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        special_ids = {\n",
    "            self.char_to_id[self.PAD],\n",
    "            self.char_to_id[self.BOS],\n",
    "            self.char_to_id[self.EOS]\n",
    "        }\n",
    "        \n",
    "        chars = []\n",
    "        for token_id in token_ids:\n",
    "            if skip_special_tokens and token_id in special_ids:\n",
    "                continue\n",
    "            chars.append(self.id_to_char.get(token_id, self.UNK))\n",
    "        \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Return list of character tokens.\"\"\"\n",
    "        return [self.BOS] + list(text) + [self.EOS]\n",
    "\n",
    "# Create and test character tokenizer\n",
    "char_tokenizer = SimpleCharacterTokenizer()\n",
    "\n",
    "# Test with example text\n",
    "test_text = \"Hello, world! \ud83d\ude80\"\n",
    "print(f\"Original text: '{test_text}'\")\n",
    "print(f\"Vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "print()\n",
    "\n",
    "# Tokenization process\n",
    "tokens = char_tokenizer.tokenize(test_text)\n",
    "token_ids = char_tokenizer.encode(test_text)\n",
    "decoded_text = char_tokenizer.decode(token_ids)\n",
    "\n",
    "print(\"Tokenization process:\")\n",
    "print(f\"1. Tokens:     {tokens}\")\n",
    "print(f\"2. Token IDs:  {token_ids}\")\n",
    "print(f\"3. Decoded:    '{decoded_text}'\")\n",
    "print()\n",
    "print(f\"\u2705 Round-trip successful: {test_text == decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_character_tokenization():\n",
    "    \"\"\"Visualize how character tokenization works.\"\"\"\n",
    "    \n",
    "    text = \"AI is amazing!\"\n",
    "    tokens = char_tokenizer.tokenize(text)\n",
    "    token_ids = char_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print()\n",
    "    \n",
    "    # Show character-by-character mapping\n",
    "    print(\"Character \u2192 Token ID mapping:\")\n",
    "    print(\"\u2500\" * 30)\n",
    "    \n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        if token in ['<BOS>', '<EOS>']:\n",
    "            print(f\"{i:2d}: {token:>6} \u2192 {token_id:3d}  (special)\")\n",
    "        else:\n",
    "            print(f\"{i:2d}: {repr(token):>6} \u2192 {token_id:3d}\")\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    print(f\"\\n\ud83d\udcca Vocabulary Analysis:\")\n",
    "    print(f\"Total vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "    print(f\"Sequence length: {len(token_ids)} tokens\")\n",
    "    print(f\"Original text length: {len(text)} characters\")\n",
    "    \n",
    "    # Show some vocabulary examples\n",
    "    print(\"\\n\ud83d\udd24 Sample vocabulary (first 20 tokens):\")\n",
    "    for i in range(min(20, len(char_tokenizer.vocab))):\n",
    "        char = char_tokenizer.vocab[i]\n",
    "        if char in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']:\n",
    "            print(f\"{i:3d}: {char}\")\n",
    "        else:\n",
    "            print(f\"{i:3d}: {repr(char)}\")\n",
    "\n",
    "visualize_character_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "## GPT-2 Comparison & Summary\n\nLet's compare our character approach with GPT-2's subword tokenization:"
  },
  {
   "cell_type": "code",
   "id": "yttfl54g6n",
   "source": "def compare_tokenization_approaches():\n    \"\"\"Compare character vs GPT-2 tokenization.\"\"\"\n    \n    if not TIKTOKEN_AVAILABLE:\n        print(\"\u26a0\ufe0f tiktoken not available - showing character-level only\")\n        return\n    \n    enc = tiktoken.get_encoding(\"gpt2\")\n    test_cases = [\n        \"Hello world!\",\n        \"Tokenization is fascinating\",\n        \"antidisestablishmentarianism\"  # Long word\n    ]\n    \n    print(\"\ud83d\udcca Tokenization Comparison:\")\n    print(\"\u2500\" * 50)\n    \n    for text in test_cases:\n        gpt2_tokens = enc.encode(text)\n        char_tokens = char_tokenizer.encode(text, add_special_tokens=False)\n        \n        # Show token breakdown\n        gpt2_token_strings = [enc.decode([tid]) for tid in gpt2_tokens]\n        \n        print(f\"Text: '{text}'\")\n        print(f\"GPT-2 ({len(gpt2_tokens):2d}): {gpt2_token_strings}\")\n        print(f\"Chars ({len(char_tokens):2d}): compression = {len(char_tokens)/len(gpt2_tokens):.1f}x\")\n        print()\n    \n    print(\"\ud83c\udfaf Key Takeaways:\")\n    print(\"\u2022 Character-level: Simple but creates long sequences\")\n    print(\"\u2022 Subword (GPT-2): Balanced vocabulary size and sequence length\")\n    print(\"\u2022 Special tokens: Essential for sequence boundaries and padding\")\n    print(f\"\u2022 GPT-2 vocabulary: {enc.n_vocab:,} tokens vs {len(char_tokenizer.vocab)} characters\")\n\ncompare_tokenization_approaches()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r88yiw69al",
   "source": "## What's Next?\n\nYou've mastered tokenization - the foundation that enables all language models! \n\n### The Learning Path:\n1. **01_attention_mechanism** - How transformers process token sequences\n2. **02_transformer_blocks** - Complete processing units  \n3. **03_positional_encoding** - Adding position information\n\n### Remember:\n- **Tokenization** converts `Text \u2192 Numbers` for AI processing\n- **Character-level** is simple but creates long sequences\n- **Subword** (like GPT-2) balances efficiency and coverage\n- **Special tokens** handle boundaries and padding\n\nReady to dive deeper into transformer architecture! \ud83d\ude80",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}