{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Understanding Tokenization: From Text to Numbers\n\nBefore transformers can process text, they need to convert it into numbers. This process is called **tokenization** - the essential bridge between human language and AI.\n\n## What You'll Learn\n\n1. **Tokenization Fundamentals** - Converting text to numerical representations\n2. **Character-Level Approach** - Simplest method for learning\n3. **Subword Tokenization** - Modern approach used by GPT and BERT\n4. **Special Tokens** - Handling boundaries and unknown content\n\nLet's master the foundation that makes all language models possible!"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "## Environment Setup\n\nImport required libraries for tokenization implementation and comparison."
  },
  {
   "cell_type": "code",
   "id": "qcxxhmmr5q",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport re\nimport json\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple\n\ntry:\n    import tiktoken\n    TIKTOKEN_AVAILABLE = True\n    print(\"‚úÖ tiktoken available\")\nexcept ImportError:\n    TIKTOKEN_AVAILABLE = False\n    print(\"‚ö†Ô∏è tiktoken not available\")\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\nprint(\"Environment setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jqzdfur5r",
   "source": "## Tokenization Concept Demo\n\nDemonstrate the fundamental concept of tokenization by comparing different approaches to breaking down text.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "sypsuaw9ofi",
   "source": "## Character Tokenizer Implementation\n\nBuild a complete character-level tokenizer with vocabulary management and special tokens.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "**Core Concepts**:\n- **Tokenization**: Converts text into numerical tokens that neural networks can process\n- **Character-level**: Simple approach, handles any text but creates long sequences  \n- **Subword**: Modern approach balancing vocabulary size and sequence length\n- **Special tokens**: Handle boundaries (`<BOS>`, `<EOS>`) and padding (`<PAD>`)\n\n**Trade-offs**:\n- Character-level: Simple, no unknown words, but inefficient for long texts\n- Subword (GPT-2): Efficient representation, smaller vocabularies, industry standard\n\n**Next**: Now that text becomes numbers, transformers can process these token sequences using attention mechanisms!"
  },
  {
   "cell_type": "code",
   "id": "7t77bzsuqgp",
   "source": "class SimpleCharacterTokenizer:\n    def __init__(self, text_corpus: str = None):\n        self.PAD = '<PAD>'\n        self.UNK = '<UNK>'\n        self.BOS = '<BOS>'\n        self.EOS = '<EOS>'\n        \n        if text_corpus:\n            self.build_vocab(text_corpus)\n        else:\n            self.vocab = self._create_default_vocab()\n            self._create_mappings()\n    \n    def _create_default_vocab(self):\n        chars = []\n        chars.extend([chr(i) for i in range(32, 127)])\n        chars.extend([self.PAD, self.UNK, self.BOS, self.EOS])\n        return chars\n    \n    def build_vocab(self, text: str):\n        unique_chars = sorted(set(text))\n        self.vocab = [self.PAD, self.UNK, self.BOS, self.EOS] + unique_chars\n        self._create_mappings()\n        print(f\"Built vocabulary from {len(text):,} characters\")\n        print(f\"Vocabulary size: {len(self.vocab)} unique characters\")\n    \n    def _create_mappings(self):\n        self.char_to_id = {char: i for i, char in enumerate(self.vocab)}\n        self.id_to_char = {i: char for i, char in enumerate(self.vocab)}\n    \n    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n        tokens = []\n        if add_special_tokens:\n            tokens.append(self.char_to_id[self.BOS])\n        \n        for char in text:\n            token_id = self.char_to_id.get(char, self.char_to_id[self.UNK])\n            tokens.append(token_id)\n        \n        if add_special_tokens:\n            tokens.append(self.char_to_id[self.EOS])\n        return tokens\n    \n    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n        special_ids = {\n            self.char_to_id[self.PAD],\n            self.char_to_id[self.BOS],\n            self.char_to_id[self.EOS]\n        }\n        \n        chars = []\n        for token_id in token_ids:\n            if skip_special_tokens and token_id in special_ids:\n                continue\n            chars.append(self.id_to_char.get(token_id, self.UNK))\n        return ''.join(chars)\n    \n    def tokenize(self, text: str) -> List[str]:\n        return [self.BOS] + list(text) + [self.EOS]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nuvbqi8p5yo",
   "source": "char_tokenizer = SimpleCharacterTokenizer()\n\ntest_text = \"Hello, world! üöÄ\"\nprint(f\"Original text: '{test_text}'\")\nprint(f\"Vocabulary size: {len(char_tokenizer.vocab)}\")\n\ntokens = char_tokenizer.tokenize(test_text)\ntoken_ids = char_tokenizer.encode(test_text)\ndecoded_text = char_tokenizer.decode(token_ids)\n\nprint(\"\\nTokenization process:\")\nprint(f\"1. Tokens:     {tokens}\")\nprint(f\"2. Token IDs:  {token_ids}\")\nprint(f\"3. Decoded:    '{decoded_text}'\")\nprint(f\"‚úÖ Round-trip successful: {test_text == decoded_text}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tams4wmn0i",
   "source": "## Tokenization Visualization\n\nBreak down the tokenization process step-by-step to see how text becomes numbers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rw019amx5fl",
   "source": "def visualize_character_tokenization():\n    text = \"AI is amazing!\"\n    tokens = char_tokenizer.tokenize(text)\n    token_ids = char_tokenizer.encode(text)\n    \n    print(f\"Text: '{text}'\")\n    print(\"\\nCharacter ‚Üí Token ID mapping:\")\n    print(\"‚îÄ\" * 30)\n    \n    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n        if token in ['<BOS>', '<EOS>']:\n            print(f\"{i:2d}: {token:>6} ‚Üí {token_id:3d}  (special)\")\n        else:\n            print(f\"{i:2d}: {repr(token):>6} ‚Üí {token_id:3d}\")\n    \n    print(f\"\\nVocabulary size: {len(char_tokenizer.vocab)}\")\n    print(f\"Sequence length: {len(token_ids)} tokens\")\n\nvisualize_character_tokenization()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2mpyyvna85t",
   "source": "## GPT-2 Comparison\n\nCompare character-level tokenization with modern subword tokenization to understand the trade-offs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "esbbwyczjkc",
   "source": "def compare_tokenization_approaches():\n    if not TIKTOKEN_AVAILABLE:\n        print(\"‚ö†Ô∏è tiktoken not available - showing character-level only\")\n        return\n    \n    enc = tiktoken.get_encoding(\"gpt2\")\n    test_cases = [\n        \"Hello world!\",\n        \"Tokenization is fascinating\",\n        \"antidisestablishmentarianism\"\n    ]\n    \n    print(\"üìä Tokenization Comparison:\")\n    print(\"‚îÄ\" * 50)\n    \n    for text in test_cases:\n        gpt2_tokens = enc.encode(text)\n        char_tokens = char_tokenizer.encode(text, add_special_tokens=False)\n        \n        gpt2_token_strings = [enc.decode([tid]) for tid in gpt2_tokens]\n        \n        print(f\"Text: '{text}'\")\n        print(f\"GPT-2 ({len(gpt2_tokens):2d}): {gpt2_token_strings}\")\n        print(f\"Chars ({len(char_tokens):2d}): compression = {len(char_tokens)/len(gpt2_tokens):.1f}x\")\n        print()\n    \n    print(\"üéØ Key Takeaways:\")\n    print(\"‚Ä¢ Character-level: Simple but creates long sequences\")\n    print(\"‚Ä¢ Subword (GPT-2): Balanced vocabulary size and sequence length\")\n    print(\"‚Ä¢ Special tokens: Essential for sequence boundaries and padding\")\n    print(f\"‚Ä¢ GPT-2 vocabulary: {enc.n_vocab:,} tokens vs {len(char_tokenizer.vocab)} characters\")\n\ncompare_tokenization_approaches()",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "wmw02dt02xp",
   "source": "## Summary\n\n**Core Concepts**:\n- **Tokenization**: Converts text into numerical tokens that neural networks can process\n- **Character-level**: Simple approach, handles any text but creates long sequences  \n- **Subword**: Modern approach balancing vocabulary size and sequence length\n- **Special tokens**: Handle boundaries (`<BOS>`, `<EOS>`) and padding (`<PAD>`)\n\n**Trade-offs**:\n- Character-level: Simple, no unknown words, but inefficient for long texts\n- Subword (GPT-2): Efficient representation, smaller vocabularies, industry standard\n\n**Next**: Now that text becomes numbers, transformers can process these token sequences using attention mechanisms!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_character_tokenization():\n",
    "    \"\"\"Visualize how character tokenization works.\"\"\"\n",
    "    \n",
    "    text = \"AI is amazing!\"\n",
    "    tokens = char_tokenizer.tokenize(text)\n",
    "    token_ids = char_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print()\n",
    "    \n",
    "    # Show character-by-character mapping\n",
    "    print(\"Character ‚Üí Token ID mapping:\")\n",
    "    print(\"‚îÄ\" * 30)\n",
    "    \n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        if token in ['<BOS>', '<EOS>']:\n",
    "            print(f\"{i:2d}: {token:>6} ‚Üí {token_id:3d}  (special)\")\n",
    "        else:\n",
    "            print(f\"{i:2d}: {repr(token):>6} ‚Üí {token_id:3d}\")\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    print(f\"\\nüìä Vocabulary Analysis:\")\n",
    "    print(f\"Total vocabulary size: {len(char_tokenizer.vocab)}\")\n",
    "    print(f\"Sequence length: {len(token_ids)} tokens\")\n",
    "    print(f\"Original text length: {len(text)} characters\")\n",
    "    \n",
    "    # Show some vocabulary examples\n",
    "    print(\"\\nüî§ Sample vocabulary (first 20 tokens):\")\n",
    "    for i in range(min(20, len(char_tokenizer.vocab))):\n",
    "        char = char_tokenizer.vocab[i]\n",
    "        if char in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']:\n",
    "            print(f\"{i:3d}: {char}\")\n",
    "        else:\n",
    "            print(f\"{i:3d}: {repr(char)}\")\n",
    "\n",
    "visualize_character_tokenization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}