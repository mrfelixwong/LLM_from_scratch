{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Positional Encoding: Teaching Transformers About Position\n\nTransformers process all positions in parallel, which creates a problem: **how does the model know about word order?** \n\nWithout positional information, \"cat sat on mat\" and \"mat on sat cat\" would look identical!\n\n## What You'll Learn\n\n1. **The Position Problem** - Why transformers need positional information\n2. **Sinusoidal Solution** - The elegant mathematical approach\n3. **Addition vs Concatenation** - Why we add instead of concatenate\n4. **Implementation** - Building positional encoding from scratch\n\nLet's solve the position puzzle!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom typing import Tuple, Optional\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(\"Environment setup complete!\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. The Position Problem\n\nLet's see why transformers need positional encoding:"
  },
  {
   "cell_type": "markdown",
   "source": "sentence1 = [\"cat\", \"sat\", \"on\", \"mat\"]\nsentence2 = [\"mat\", \"on\", \"sat\", \"cat\"]\n\nword_embeddings = {\n    \"cat\": [1, 0, 0], \"sat\": [0, 1, 0], \n    \"on\": [0, 0, 1], \"mat\": [1, 1, 0]\n}\n\nwords1 = [word_embeddings[word] for word in sentence1]\nwords2 = [word_embeddings[word] for word in sentence2]\n\nsum1 = [sum(x) for x in zip(*words1)]\nsum2 = [sum(x) for x in zip(*words2)]\n\nprint(f\"Sentence 1: {' '.join(sentence1)}\")\nprint(f\"Sentence 2: {' '.join(sentence2)}\")\nprint(f\"Without position encoding:\")\nprint(f\"Representation 1: {sum1}\")\nprint(f\"Representation 2: {sum2}\")\nprint(f\"Identical? {sum1 == sum2}\")\nprint(\"âŒ Problem: Can't distinguish word order!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Sinusoidal Positional Encoding\n\n**The Solution**: Use sine and cosine functions to create unique position signatures.\n\n**Requirements**:\n- Unique pattern for each position\n- Bounded values (don't explode)  \n- Smooth transitions between nearby positions\n- Works for any sequence length\n\n**Formula**: For position `pos` and dimension `i`:\n- Even dims: `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))`\n- Odd dims: `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def create_sinusoidal_encoding(max_len: int, d_model: int) -> torch.Tensor:\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    \n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                        (-math.log(10000.0) / d_model))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\nmax_len, d_model = 10, 8\npos_encoding = create_sinusoidal_encoding(max_len, d_model)\n\nprint(f\"Positional encoding shape: {pos_encoding.shape}\")\nprint(f\"Value range: [{pos_encoding.min():.3f}, {pos_encoding.max():.3f}]\")\n\nfor i in range(3):\n    print(f\"Position {i}: {[round(x, 3) for x in pos_encoding[i].tolist()]}\")\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(pos_encoding.T, cmap='RdBu_r', center=0)\nplt.title('Positional Encoding Pattern')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\n\nplt.subplot(1, 2, 2)\nfor dim in [0, 1, 6, 7]:\n    plt.plot(pos_encoding[:, dim], label=f'Dim {dim}')\nplt.title('Values by Position')\nplt.xlabel('Position')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ… Each position gets a unique bounded pattern!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Why Addition Instead of Concatenation?\n\n**Critical design choice**: We ADD position encoding to word embeddings rather than concatenating them.\n\n**Addition benefits**:\n- Same dimensionality (efficient)\n- Creates blended word-position representations\n- Attention sees unified \"word-at-position\" features\n\n**Concatenation problems**:\n- Doubles dimensions (expensive)\n- Separates word and position information\n- Requires learning to combine them"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "word_emb = torch.tensor([1.0, 0.5, -0.3, 0.8])\npos_emb = pos_encoding[1][:4]\n\nprint(f\"Word embedding:     {word_emb.tolist()}\")\nprint(f\"Position embedding: {[round(x, 3) for x in pos_emb.tolist()]}\")\n\nadded = word_emb + pos_emb\nconcatenated = torch.cat([word_emb, pos_emb])\n\nprint(f\"\\nADDITION (what transformers use):\")\nprint(f\"Result: {[round(x, 3) for x in added.tolist()]} (shape: {added.shape})\")\nprint(\"âœ… Same dimensionality, blended representation\")\n\nprint(f\"\\nCONCATENATION (alternative):\")\nprint(f\"Result: {[round(x, 3) for x in concatenated.tolist()]} (shape: {concatenated.shape})\")\nprint(\"âŒ Double dimensions, separated information\")\n\n# Solve original problem with positional encoding\nprint(f\"\\nðŸŽ‰ SOLVING THE POSITION PROBLEM:\")\nemb1 = torch.tensor([[1,0,0,0], [0,1,0,0], [0,0,1,0], [1,1,0,0]]).float()  # cat sat on mat\nemb2 = torch.tensor([[1,1,0,0], [0,0,1,0], [0,1,0,0], [1,0,0,0]]).float()  # mat on sat cat\n\npos_enc_4 = create_sinusoidal_encoding(4, 4)\ncombined1 = emb1 + pos_enc_4\ncombined2 = emb2 + pos_enc_4\n\nsum1, sum2 = combined1.sum(dim=0), combined2.sum(dim=0)\nare_different = not torch.allclose(sum1, sum2, atol=1e-6)\n\nprint(f\"After adding positional encoding:\")\nprint(f\"Different representations? {are_different}\")\nprint(\"âœ… Position encoding solved the word order problem!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Complete Positional Embedding Layer\n\nCombine word embeddings with positional encoding in a complete neural network layer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "class PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, max_len: int, d_model: int):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        \n        pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n        self.register_buffer('pos_encoding', pos_encoding)\n    \n    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n        batch_size, seq_len = token_ids.shape\n        \n        word_emb = self.token_embedding(token_ids)\n        pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)\n        pos_emb = pos_emb.expand(batch_size, -1, -1)\n        \n        return word_emb + pos_emb\n\nvocab_size, max_len, d_model = 100, 20, 8\npos_emb_layer = PositionalEmbedding(vocab_size, max_len, d_model)\n\nbatch_size, seq_len = 2, 5\ntoken_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\nembeddings = pos_emb_layer(token_ids)\n\nprint(f\"Input shape: {token_ids.shape}\")\nprint(f\"Output shape: {embeddings.shape}\")\n\n# Show same token at different positions gets different embeddings\ntoken_id = 42\nprint(f\"\\nSame token (ID={token_id}) at different positions:\")\nfor pos in range(4):\n    test_input = torch.full((1, pos+1), token_id)\n    test_output = pos_emb_layer(test_input)\n    final_embedding = test_output[0, pos]\n    print(f\"Position {pos}: {[round(x, 3) for x in final_embedding[:3].tolist()]}...\")\n\nprint(\"\\nâœ… Same token gets different embeddings at different positions!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nYou've mastered positional encoding - the key to teaching transformers about word order!\n\n### Key Concepts:\n1. **The Problem**: Transformers process all positions in parallel and need explicit position information\n2. **Sinusoidal Solution**: Sine and cosine functions create unique, bounded position signatures  \n3. **Addition > Concatenation**: Adding creates richer word-position interactions efficiently\n4. **Implementation**: Simple but powerful - transforms how models understand sequences\n\n### What's Next?\nNow you understand all the core transformer components:\n- **Tokenization** (notebook 0) - Text â†’ numbers\n- **Attention** (notebook 1) - How to focus on relevant information\n- **Transformer blocks** (notebook 2) - Complete processing units\n- **Position encoding** (notebook 3) - Understanding word order\n\nReady to see it all working together in a complete transformer! ðŸš€",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.embeddings import GPTEmbedding\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Complete positional embedding layer with multiple options.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, max_len: int, d_model: int, \n",
    "                 pos_type: str = \"learned\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_type = pos_type\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if pos_type == \"learned\":\n",
    "            self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        elif pos_type == \"sinusoidal\":\n",
    "            # Register as buffer (not a parameter)\n",
    "            pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n",
    "            self.register_buffer('pos_encoding', pos_encoding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pos_type: {pos_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize token embeddings\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        if pos_type == \"learned\":\n",
    "            nn.init.normal_(self.pos_embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: combine token and positional embeddings.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Token IDs [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Combined embeddings [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(token_ids)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if self.pos_type == \"learned\":\n",
    "            positions = torch.arange(seq_len, device=token_ids.device)\n",
    "            pos_emb = self.pos_embedding(positions)  # [seq_len, d_model]\n",
    "            pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        else:  # sinusoidal\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)  # [1, seq_len, d_model]\n",
    "            pos_emb = pos_emb.expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_emb + pos_emb\n",
    "        \n",
    "        # Apply dropout\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test both types of positional embedding\n",
    "vocab_size, max_len, d_model = 100, 20, 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "# Create test input\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Test learned embeddings\n",
    "learned_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"learned\")\n",
    "output_learned = learned_emb(token_ids)\n",
    "\n",
    "# Test sinusoidal embeddings\n",
    "sinusoidal_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"sinusoidal\")\n",
    "output_sinusoidal = sinusoidal_emb(token_ids)\n",
    "\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Output embeddings shape: {output_learned.shape}\")\n",
    "print()\n",
    "\n",
    "# Compare parameter counts\n",
    "learned_params = sum(p.numel() for p in learned_emb.parameters())\n",
    "sinusoidal_params = sum(p.numel() for p in sinusoidal_emb.parameters())\n",
    "\n",
    "print(f\"Parameter comparison:\")\n",
    "print(f\"Learned embeddings:    {learned_params:,} parameters\")\n",
    "print(f\"Sinusoidal embeddings: {sinusoidal_params:,} parameters\")\n",
    "print(f\"Difference: {learned_params - sinusoidal_params:,} (positional embedding table)\")\n",
    "\n",
    "# Visualize the embeddings for first batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learned embeddings\n",
    "sns.heatmap(output_learned[0].detach().T, cmap='viridis', ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Learned Positional Embeddings')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Sinusoidal embeddings\n",
    "sns.heatmap(output_sinusoidal[0].detach().T, cmap='viridis', ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Sinusoidal Positional Embeddings')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Both embedding types work and produce the same output shape!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}