{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Positional Encoding: Teaching Transformers About Position\n\nTransformers process all positions in parallel, which creates a problem: **how does the model know about word order?** \n\nWithout positional information, \"cat sat on mat\" and \"mat on sat cat\" would look identical!\n\n## What You'll Learn\n\n1. **The Position Problem** - Why transformers need positional information\n2. **Sinusoidal Solution** - The elegant mathematical approach\n3. **Addition vs Concatenation** - Why we add instead of concatenate\n4. **Implementation** - Building positional encoding from scratch\n\nLet's solve the position puzzle!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. The Position Problem\n\nLet's see why transformers need positional encoding:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demonstrate_position_problem():\n    \"\"\"Show how transformers without position encoding can't distinguish word order.\"\"\"\n    \n    print(\"ðŸš¨ THE POSITION PROBLEM\")\n    \n    # Two sentences with different word order\n    sentence1 = [\"cat\", \"sat\", \"on\", \"mat\"]\n    sentence2 = [\"mat\", \"on\", \"sat\", \"cat\"]  # Scrambled!\n    \n    print(f\"Sentence 1: {' '.join(sentence1)}\")\n    print(f\"Sentence 2: {' '.join(sentence2)}\")\n    print()\n    \n    # Simple word embeddings (just for demonstration)\n    word_embeddings = {\n        \"cat\": [1, 0, 0],\n        \"sat\": [0, 1, 0], \n        \"on\": [0, 0, 1],\n        \"mat\": [1, 1, 0]\n    }\n    \n    # Without position info, both sentences have same words\n    words1 = [word_embeddings[word] for word in sentence1]\n    words2 = [word_embeddings[word] for word in sentence2]\n    \n    # Sum of embeddings (bag-of-words)\n    sum1 = [sum(x) for x in zip(*words1)]\n    sum2 = [sum(x) for x in zip(*words2)]\n    \n    print(\"Without position encoding:\")\n    print(f\"Sentence 1 representation: {sum1}\")\n    print(f\"Sentence 2 representation: {sum2}\")\n    print(f\"Are they identical? {sum1 == sum2}\")\n    print()\n    print(\"âŒ Problem: Can't tell word order apart!\")\n    print(\"âœ… Solution: Add position information to each word\")\n\ndemonstrate_position_problem()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Sinusoidal Positional Encoding: The Solution\n\nThe original transformer paper used **sinusoidal positional encoding**. Here's why it's brilliant:\n\n### Why Sine and Cosine? ðŸŒŠ\n\n**Requirements for position encoding:**\n- âœ… **Unique**: Each position needs a different \"signature\"\n- âœ… **Bounded**: Values shouldn't explode (position 1000 â‰  1000!)\n- âœ… **Smooth**: Nearby positions should be similar\n- âœ… **Consistent**: Work for any sequence length\n\n**Sine and cosine meet all requirements:**\n- Always between -1 and 1 (bounded)\n- Smooth, continuous functions\n- Create unique patterns when combined at different frequencies\n\n### The Formula\n\nFor position `pos` and dimension `i`:\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n**Translation:**\n- Even dimensions (0, 2, 4...) get sine waves\n- Odd dimensions (1, 3, 5...) get cosine waves  \n- Different frequencies for different dimensions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import math\n\ndef create_sinusoidal_encoding(max_len: int, d_model: int) -> torch.Tensor:\n    \"\"\"Create sinusoidal positional encoding.\"\"\"\n    \n    print(f\"Creating positional encoding for max_len={max_len}, d_model={d_model}\")\n    \n    # Initialize encoding matrix\n    pe = torch.zeros(max_len, d_model)\n    \n    # Create position indices [0, 1, 2, ..., max_len-1]\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    \n    # Create frequency terms for different dimensions\n    # Each dimension pair gets a different frequency\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                        (-math.log(10000.0) / d_model))\n    \n    # Apply sine to even dimensions (0, 2, 4, ...)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    \n    # Apply cosine to odd dimensions (1, 3, 5, ...)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    print(f\"âœ… Created encoding with shape: {pe.shape}\")\n    print(f\"Value range: [{pe.min():.3f}, {pe.max():.3f}]\")\n    \n    return pe\n\n# Create positional encoding\nmax_len, d_model = 10, 8\npos_encoding = create_sinusoidal_encoding(max_len, d_model)\n\n# Show the first few positions\nprint(f\"\\nFirst 5 position encodings:\")\nfor i in range(5):\n    print(f\"Position {i}: {pos_encoding[i].tolist()!r}\")\n\n# Visualize the encoding pattern\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\n# Heatmap showing the pattern\nsns.heatmap(pos_encoding.T, cmap='RdBu_r', center=0, \n            xticklabels=range(max_len), yticklabels=range(d_model))\nplt.title('Positional Encoding Heatmap')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\n\nplt.subplot(1, 2, 2) \n# Show how different dimensions change over position\nfor dim in [0, 1, 6, 7]:  # Show a few dimensions\n    plt.plot(pos_encoding[:, dim], label=f'Dim {dim}', linewidth=2)\nplt.title('Encoding Values by Position')\nplt.xlabel('Position')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ”‘ Key observations:\")\nprint(\"â€¢ Each position gets a unique pattern\")\nprint(\"â€¢ Values stay bounded between -1 and 1\") \nprint(\"â€¢ Different dimensions oscillate at different speeds\")\nprint(\"â€¢ Pattern creates unique 'fingerprint' for each position\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Why ADD Instead of Concatenate?\n\nThis is crucial! Why do we **add** positional encodings to word embeddings instead of **concatenating** them?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def explain_addition_vs_concatenation():\n    \"\"\"Show why we add instead of concatenate.\"\"\"\n    \n    print(\"ðŸ¤” ADDITION VS CONCATENATION\")\n    print(\"=\" * 40)\n    \n    # Example embeddings\n    d_model = 4\n    word_emb = torch.tensor([1.0, 0.5, -0.3, 0.8])  # Word: \"cat\"\n    pos_emb = pos_encoding[1]  # Position 1\n    \n    print(f\"Word embedding:     {word_emb.tolist()}\")\n    print(f\"Position embedding: {pos_emb.tolist()!r}\")\n    print()\n    \n    # Addition (what transformers do)\n    added = word_emb + pos_emb\n    print(\"ADDITION APPROACH:\")\n    print(f\"Combined: {added.tolist()!r} (shape: {added.shape})\")\n    print(\"âœ… Same dimensionality as original\")\n    print(\"âœ… Word and position info blended together\")\n    print(\"âœ… Attention sees unified 'word-at-position' representation\")\n    print()\n    \n    # Concatenation (alternative approach)  \n    concatenated = torch.cat([word_emb, pos_emb])\n    print(\"CONCATENATION APPROACH:\")\n    print(f\"Combined: {concatenated.tolist()!r} (shape: {concatenated.shape})\")\n    print(\"âŒ Doubles the dimensionality (expensive!)\")\n    print(\"âŒ Word and position info separated\")\n    print(\"âŒ Attention must learn to combine them\")\n    print()\n    \n    print(\"ðŸŽ¯ WHY ADDITION WINS:\")\n    print(\"â€¢ More efficient (same dimensions)\")\n    print(\"â€¢ Creates richer word-position interactions\")  \n    print(\"â€¢ Attention naturally considers both word + position\")\n    print(\"â€¢ Model learns to balance word meaning vs position\")\n\nexplain_addition_vs_concatenation()\n\n# Show the solution in action\ndef solve_position_problem():\n    \"\"\"Show how position encoding solves the original problem.\"\"\"\n    \n    print(\"\\nðŸŽ‰ SOLVING THE ORIGINAL PROBLEM\")\n    print(\"=\" * 35)\n    \n    # Our problem sentences\n    sentence1 = [\"cat\", \"sat\", \"on\", \"mat\"] \n    sentence2 = [\"mat\", \"on\", \"sat\", \"cat\"]\n    \n    # Simple word embeddings\n    word_embeddings = torch.tensor([\n        [1, 0, 0, 0],  # cat\n        [0, 1, 0, 0],  # sat  \n        [0, 0, 1, 0],  # on\n        [1, 1, 0, 0],  # mat\n    ])\n    \n    # Map words to embeddings\n    word_to_idx = {\"cat\": 0, \"sat\": 1, \"on\": 2, \"mat\": 3}\n    \n    # Get word embeddings for both sentences\n    emb1 = torch.stack([word_embeddings[word_to_idx[word]] for word in sentence1])\n    emb2 = torch.stack([word_embeddings[word_to_idx[word]] for word in sentence2])\n    \n    # Add positional encoding\n    pos_enc_small = create_sinusoidal_encoding(4, 4)\n    combined1 = emb1.float() + pos_enc_small\n    combined2 = emb2.float() + pos_enc_small\n    \n    print(f\"Sentence 1: {' '.join(sentence1)}\")\n    print(f\"Sentence 2: {' '.join(sentence2)}\")\n    print()\n    \n    # Check if they're different now\n    sum1 = combined1.sum(dim=0)\n    sum2 = combined2.sum(dim=0) \n    are_different = not torch.allclose(sum1, sum2, atol=1e-6)\n    \n    print(\"After adding positional encoding:\")\n    print(f\"Sentence 1 representation: {sum1.tolist()!r}\")\n    print(f\"Sentence 2 representation: {sum2.tolist()!r}\")\n    print(f\"Are they different? {are_different}\")\n    \n    if are_different:\n        print(\"\\nâœ… SUCCESS! Position encoding solved the problem!\")\n        print(\"âœ… Different word orders â†’ Different representations\")\n        print(\"âœ… Model can now understand word order\")\n    else:\n        print(\"âŒ Something went wrong...\")\n\nsolve_position_problem()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Complete Implementation\n\nLet's create a simple positional embedding layer:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PositionalEmbedding(nn.Module):\n    \"\"\"Combines word embeddings with sinusoidal positional encoding.\"\"\"\n    \n    def __init__(self, vocab_size: int, max_len: int, d_model: int):\n        super().__init__()\n        \n        # Word embeddings\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Sinusoidal positional encoding (not learned)\n        pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n        self.register_buffer('pos_encoding', pos_encoding)\n    \n    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            token_ids: [batch_size, seq_len]\n        Returns:\n            embeddings: [batch_size, seq_len, d_model]\n        \"\"\"\n        batch_size, seq_len = token_ids.shape\n        \n        # Get word embeddings\n        word_emb = self.token_embedding(token_ids)  # [batch_size, seq_len, d_model]\n        \n        # Get positional encoding for this sequence length\n        pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)  # [1, seq_len, d_model]\n        pos_emb = pos_emb.expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n        \n        # Add them together (the key insight!)\n        embeddings = word_emb + pos_emb\n        \n        return embeddings\n\n# Test the complete implementation\nvocab_size, max_len, d_model = 100, 20, 8\npos_emb_layer = PositionalEmbedding(vocab_size, max_len, d_model)\n\n# Create test input\nbatch_size, seq_len = 2, 5\ntoken_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n\n# Forward pass\nembeddings = pos_emb_layer(token_ids)\n\nprint(f\"Input shape: {token_ids.shape}\")\nprint(f\"Output shape: {embeddings.shape}\")\nprint(\"âœ… Complete positional embedding layer working!\")\n\n# Show what happens to the same token at different positions\ntoken_id = 42  # Arbitrary token\npositions = [0, 1, 2, 3]\n\nprint(f\"\\nSame token (ID={token_id}) at different positions:\")\nfor pos in positions:\n    # Create input with our token at this position\n    test_input = torch.full((1, pos+1), token_id)\n    test_output = pos_emb_layer(test_input)\n    final_embedding = test_output[0, pos]  # Get embedding at the position\n    \n    print(f\"Position {pos}: {final_embedding[:3].tolist()!r}...\") # Show first 3 dims\n\nprint(\"\\nðŸ”‘ Notice: Same token gets different embeddings at different positions!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nYou've mastered positional encoding - the key to teaching transformers about word order!\n\n### Key Concepts:\n1. **The Problem**: Transformers process all positions in parallel and need explicit position information\n2. **Sinusoidal Solution**: Sine and cosine functions create unique, bounded position signatures  \n3. **Addition > Concatenation**: Adding creates richer word-position interactions efficiently\n4. **Implementation**: Simple but powerful - transforms how models understand sequences\n\n### What's Next?\nNow you understand all the core transformer components:\n- **Tokenization** (notebook 0) - Text â†’ numbers\n- **Attention** (notebook 1) - How to focus on relevant information\n- **Transformer blocks** (notebook 2) - Complete processing units\n- **Position encoding** (notebook 3) - Understanding word order\n\nReady to see it all working together in a complete transformer! ðŸš€",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.embeddings import GPTEmbedding\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Complete positional embedding layer with multiple options.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, max_len: int, d_model: int, \n",
    "                 pos_type: str = \"learned\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_type = pos_type\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if pos_type == \"learned\":\n",
    "            self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        elif pos_type == \"sinusoidal\":\n",
    "            # Register as buffer (not a parameter)\n",
    "            pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n",
    "            self.register_buffer('pos_encoding', pos_encoding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pos_type: {pos_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize token embeddings\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        if pos_type == \"learned\":\n",
    "            nn.init.normal_(self.pos_embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: combine token and positional embeddings.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Token IDs [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Combined embeddings [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(token_ids)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if self.pos_type == \"learned\":\n",
    "            positions = torch.arange(seq_len, device=token_ids.device)\n",
    "            pos_emb = self.pos_embedding(positions)  # [seq_len, d_model]\n",
    "            pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        else:  # sinusoidal\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)  # [1, seq_len, d_model]\n",
    "            pos_emb = pos_emb.expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_emb + pos_emb\n",
    "        \n",
    "        # Apply dropout\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test both types of positional embedding\n",
    "vocab_size, max_len, d_model = 100, 20, 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "# Create test input\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Test learned embeddings\n",
    "learned_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"learned\")\n",
    "output_learned = learned_emb(token_ids)\n",
    "\n",
    "# Test sinusoidal embeddings\n",
    "sinusoidal_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"sinusoidal\")\n",
    "output_sinusoidal = sinusoidal_emb(token_ids)\n",
    "\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Output embeddings shape: {output_learned.shape}\")\n",
    "print()\n",
    "\n",
    "# Compare parameter counts\n",
    "learned_params = sum(p.numel() for p in learned_emb.parameters())\n",
    "sinusoidal_params = sum(p.numel() for p in sinusoidal_emb.parameters())\n",
    "\n",
    "print(f\"Parameter comparison:\")\n",
    "print(f\"Learned embeddings:    {learned_params:,} parameters\")\n",
    "print(f\"Sinusoidal embeddings: {sinusoidal_params:,} parameters\")\n",
    "print(f\"Difference: {learned_params - sinusoidal_params:,} (positional embedding table)\")\n",
    "\n",
    "# Visualize the embeddings for first batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learned embeddings\n",
    "sns.heatmap(output_learned[0].detach().T, cmap='viridis', ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Learned Positional Embeddings')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Sinusoidal embeddings\n",
    "sns.heatmap(output_sinusoidal[0].detach().T, cmap='viridis', ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Sinusoidal Positional Embeddings')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Both embedding types work and produce the same output shape!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}