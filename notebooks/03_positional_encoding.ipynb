{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Positional Encoding: Teaching Transformers About Position\n\nTransformers process all positions in parallel, creating a problem: **how does the model know about word order?**\n\nWithout positional information, \"cat sat on mat\" and \"mat on sat cat\" would look identical!\n\n## The Solution\n**Positional encoding** adds unique position signatures to word embeddings, giving transformers spatial awareness.\n\n## What You'll Learn\n1. **The Position Problem** - Why order matters\n2. **Sinusoidal Encoding** - Mathematical solution using sine/cosine\n3. **Addition vs Concatenation** - Why we add position info\n4. **Complete Implementation** - Building the full embedding layer"
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom typing import Tuple, Optional\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(\"Environment setup complete!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate the position problem\nsentence1 = [\"cat\", \"sat\", \"on\", \"mat\"]\nsentence2 = [\"mat\", \"on\", \"sat\", \"cat\"]\n\nword_embeddings = {\n    \"cat\": [1, 0, 0], \"sat\": [0, 1, 0], \n    \"on\": [0, 0, 1], \"mat\": [1, 1, 0]\n}\n\nemb1 = [word_embeddings[word] for word in sentence1]\nemb2 = [word_embeddings[word] for word in sentence2]\n\n# Sum embeddings (what attention might see)\nsum1 = [sum(x) for x in zip(*emb1)]\nsum2 = [sum(x) for x in zip(*emb2)]\n\nprint(f\"Sentence 1: {' '.join(sentence1)}\")\nprint(f\"Sentence 2: {' '.join(sentence2)}\")\nprint(f\"Aggregated representation 1: {sum1}\")\nprint(f\"Aggregated representation 2: {sum2}\")\nprint(f\"Identical? {sum1 == sum2}\")\nprint(\"âŒ Problem: Can't distinguish word order!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Sinusoidal Positional Encoding\n\nUse sine and cosine functions to create unique, bounded position signatures."
  },
  {
   "cell_type": "code",
   "source": "def create_sinusoidal_encoding(max_len, d_model):\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    \n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                        (-math.log(10000.0) / d_model))\n    \n    pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n    pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n    \n    return pe\n\n# Create and visualize positional encoding\nmax_len, d_model = 10, 8\npos_encoding = create_sinusoidal_encoding(max_len, d_model)\n\nprint(f\"Positional encoding shape: {pos_encoding.shape}\")\nprint(f\"Value range: [{pos_encoding.min():.3f}, {pos_encoding.max():.3f}]\")\n\n# Show first few positions\nfor i in range(3):\n    print(f\"Position {i}: {[round(x, 3) for x in pos_encoding[i].tolist()]}\")\n\n# Visualize the encoding pattern\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(pos_encoding.T, cmap='RdBu_r', center=0)\nplt.title('Positional Encoding Pattern')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\n\nplt.subplot(1, 2, 2)\nfor dim in [0, 1, 6, 7]:\n    plt.plot(pos_encoding[:, dim], label=f'Dim {dim}')\nplt.title('Values by Position')\nplt.xlabel('Position')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ… Each position gets a unique, bounded pattern!\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Addition vs Concatenation\n\nCompare adding positional encoding to word embeddings versus concatenating them."
  },
  {
   "cell_type": "code",
   "source": "# Compare addition vs concatenation\nword_emb = torch.tensor([1.0, 0.5, -0.3, 0.8])\npos_emb = pos_encoding[1][:4]  # Position 1, first 4 dimensions\n\nprint(f\"Word embedding:     {word_emb.tolist()}\")\nprint(f\"Position embedding: {[round(x, 3) for x in pos_emb.tolist()]}\")\n\n# Addition (what transformers use)\nadded = word_emb + pos_emb\nprint(f\"\\nADDITION:\")\nprint(f\"Result: {[round(x, 3) for x in added.tolist()]} (shape: {added.shape})\")\nprint(\"âœ… Same dimensionality, blended word-position representation\")\n\n# Concatenation (alternative)\nconcatenated = torch.cat([word_emb, pos_emb])\nprint(f\"\\nCONCATENATION:\")\nprint(f\"Result: {[round(x, 3) for x in concatenated.tolist()]} (shape: {concatenated.shape})\")\nprint(\"âŒ Double dimensions, separated information\")\n\n# Show how addition solves the original problem\nprint(f\"\\nðŸŽ‰ SOLVING THE POSITION PROBLEM:\")\nemb1 = torch.tensor([[1,0,0,0], [0,1,0,0], [0,0,1,0], [1,1,0,0]]).float()  # cat sat on mat\nemb2 = torch.tensor([[1,1,0,0], [0,0,1,0], [0,1,0,0], [1,0,0,0]]).float()  # mat on sat cat\n\npos_enc_4 = create_sinusoidal_encoding(4, 4)\ncombined1 = emb1 + pos_enc_4\ncombined2 = emb2 + pos_enc_4\n\nsum1, sum2 = combined1.sum(dim=0), combined2.sum(dim=0)\nare_different = not torch.allclose(sum1, sum2, atol=1e-6)\n\nprint(f\"Different representations after adding position? {are_different}\")\nprint(\"âœ… Position encoding solved the word order problem!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Complete Positional Embedding Layer\n\nBuild a complete neural network layer that combines token embeddings with positional encoding."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size, max_len, d_model):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Create and register positional encoding as buffer (not trainable)\n        pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n        self.register_buffer('pos_encoding', pos_encoding)\n    \n    def forward(self, token_ids):\n        batch_size, seq_len = token_ids.shape\n        \n        # Get token embeddings\n        word_emb = self.token_embedding(token_ids)\n        \n        # Get positional encoding for this sequence length\n        pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)\n        pos_emb = pos_emb.expand(batch_size, -1, -1)\n        \n        # Add them together\n        return word_emb + pos_emb\n\n# Test the complete embedding layer\nvocab_size, max_len, d_model = 100, 20, 8\npos_emb_layer = PositionalEmbedding(vocab_size, max_len, d_model)\n\nbatch_size, seq_len = 2, 5\ntoken_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\nembeddings = pos_emb_layer(token_ids)\n\nprint(f\"Input token IDs shape: {token_ids.shape}\")\nprint(f\"Output embeddings shape: {embeddings.shape}\")\n\n# Show same token at different positions gets different embeddings\ntoken_id = 42\nprint(f\"\\nSame token (ID={token_id}) at different positions:\")\nfor pos in range(4):\n    test_input = torch.full((1, pos+1), token_id)\n    test_output = pos_emb_layer(test_input)\n    final_embedding = test_output[0, pos]\n    print(f\"Position {pos}: {[round(x, 3) for x in final_embedding[:3].tolist()]}...\")\n\nprint(\"\\nâœ… Same token gets different embeddings at different positions!\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nYou've mastered positional encoding - the key to teaching transformers about word order!\n\n**Key Concepts**:\n- **The Problem**: Transformers process positions in parallel and need explicit position information\n- **Sinusoidal Solution**: Sine/cosine functions create unique, bounded position signatures  \n- **Addition**: Adding position to word embeddings creates efficient blended representations\n- **Implementation**: Simple but powerful transformation enabling spatial understanding\n\n**What's Next**: Now you understand all core transformer components:\n- Tokenization (notebook 0) - Text â†’ numbers\n- Attention (notebook 1) - Information routing  \n- Transformer blocks (notebook 2) - Complete processing units\n- Position encoding (notebook 3) - Spatial awareness\n\nReady to see it all working together in training! ðŸš€"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Complete Positional Embedding Layer\n\nCombine word embeddings with positional encoding in a complete neural network layer."
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nYou've mastered positional encoding - the key to teaching transformers about word order!\n\n### Key Concepts:\n1. **The Problem**: Transformers process all positions in parallel and need explicit position information\n2. **Sinusoidal Solution**: Sine and cosine functions create unique, bounded position signatures  \n3. **Addition > Concatenation**: Adding creates richer word-position interactions efficiently\n4. **Implementation**: Simple but powerful - transforms how models understand sequences\n\n### What's Next?\nNow you understand all the core transformer components:\n- **Tokenization** (notebook 0) - Text â†’ numbers\n- **Attention** (notebook 1) - How to focus on relevant information\n- **Transformer blocks** (notebook 2) - Complete processing units\n- **Position encoding** (notebook 3) - Understanding word order\n\nReady to see it all working together in a complete transformer! ðŸš€",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}