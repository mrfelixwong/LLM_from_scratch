{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding: Teaching Transformers About Position\n",
    "\n",
    "Transformers process all positions in parallel, which is great for speed but creates a problem: **how does the model know about word order?** Without positional information, \"cat sat on mat\" and \"mat on sat cat\" would look identical!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Position Problem** - Why transformers need positional information\n",
    "2. **Sinusoidal Encoding** - The original transformer's elegant solution\n",
    "3. **Learned Embeddings** - A simple alternative approach\n",
    "4. **Relative Positions** - Modern improvements\n",
    "5. **Visualizing Patterns** - Understanding how position encoding works\n",
    "\n",
    "Let's solve the position puzzle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Position Problem\n",
    "\n",
    "Let's demonstrate why transformers need positional encoding by showing how attention works without position information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_position_problem():\n",
    "    \"\"\"Show how transformers without position encoding can't distinguish word order.\"\"\"\n",
    "    \n",
    "    # Create simple word embeddings\n",
    "    vocab = {\"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4}\n",
    "    d_model = 4\n",
    "    \n",
    "    # Simple embeddings (just for demonstration)\n",
    "    embeddings = torch.tensor([\n",
    "        [1.0, 0.0, 0.0, 0.0],  # \"the\"\n",
    "        [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
    "        [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n",
    "        [0.0, 0.0, 0.0, 1.0],  # \"on\"\n",
    "        [1.0, 1.0, 0.0, 0.0],  # \"mat\"\n",
    "    ])\n",
    "    \n",
    "    # Two sentences with different word order\n",
    "    sentence1 = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]  # Normal order\n",
    "    sentence2 = [\"cat\", \"the\", \"mat\", \"sat\", \"on\"]  # Scrambled order\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    emb1 = torch.stack([embeddings[vocab[word]] for word in sentence1])\n",
    "    emb2 = torch.stack([embeddings[vocab[word]] for word in sentence2])\n",
    "    \n",
    "    print(\"Sentence 1:\", \" \".join(sentence1))\n",
    "    print(\"Sentence 2:\", \" \".join(sentence2))\n",
    "    print()\n",
    "    \n",
    "    # Show that without position, both sentences have same information\n",
    "    print(\"Word embeddings (same for both sentences):\")\n",
    "    unique_words = sorted(set(sentence1))\n",
    "    for word in unique_words:\n",
    "        print(f\"{word}: {embeddings[vocab[word]].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare sentence representations (sum of embeddings)\n",
    "    sum1 = emb1.sum(dim=0)\n",
    "    sum2 = emb2.sum(dim=0)\n",
    "    \n",
    "    print(\"Sum of embeddings (bag-of-words):\")\n",
    "    print(f\"Sentence 1: {sum1.tolist()}\")\n",
    "    print(f\"Sentence 2: {sum2.tolist()}\")\n",
    "    print(f\"Are they equal? {torch.allclose(sum1, sum2)}\")\n",
    "    print()\n",
    "    print(\"Problem: Without position info, transformers can't distinguish word order!\")\n",
    "    \n",
    "    return emb1, emb2, vocab, embeddings\n",
    "\n",
    "emb1, emb2, vocab, word_embeddings = demonstrate_position_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sinusoidal Positional Encoding\n",
    "\n",
    "The original transformer paper introduced sinusoidal positional encoding. The idea is elegant: use sine and cosine functions of different frequencies to create unique position signatures.\n",
    "\n",
    "For position $pos$ and dimension $i$:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encoding.\n",
    "    \n",
    "    Args:\n",
    "        max_len: Maximum sequence length\n",
    "        d_model: Model dimension\n",
    "    \n",
    "    Returns:\n",
    "        Positional encoding tensor [max_len, d_model]\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Create the div_term for the sinusoidal pattern\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                        (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    # Apply sine to even dimensions\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    \n",
    "    # Apply cosine to odd dimensions\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Create positional encoding\n",
    "max_len, d_model = 50, 8\n",
    "pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n",
    "\n",
    "print(f\"Positional encoding shape: {pos_encoding.shape}\")\n",
    "print(f\"First few positions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Position {i}: {pos_encoding[i][:4].tolist()!r}...\")  # Show first 4 dimensions\n",
    "\n",
    "# Visualize the positional encoding\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the full encoding as a heatmap\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(pos_encoding[:20].T, cmap='RdBu_r', center=0, \n",
    "            xticklabels=range(20), yticklabels=range(d_model))\n",
    "plt.title('Positional Encoding Heatmap\\n(First 20 positions)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "\n",
    "# Plot individual dimensions\n",
    "plt.subplot(2, 2, 2)\n",
    "for dim in [0, 1, 2, 3]:\n",
    "    plt.plot(pos_encoding[:30, dim], label=f'Dim {dim}')\n",
    "plt.title('Individual Dimensions Over Position')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show the frequency pattern\n",
    "plt.subplot(2, 2, 3)\n",
    "frequencies = []\n",
    "for i in range(0, d_model, 2):\n",
    "    freq = 1 / (10000 ** (i / d_model))\n",
    "    frequencies.append(freq)\n",
    "\n",
    "plt.bar(range(len(frequencies)), frequencies)\n",
    "plt.title('Frequencies by Dimension Pair')\n",
    "plt.xlabel('Dimension Pair (i//2)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Show similarity between positions\n",
    "plt.subplot(2, 2, 4)\n",
    "# Compute cosine similarity between positions\n",
    "similarities = []\n",
    "pos_0 = pos_encoding[0]\n",
    "for i in range(20):\n",
    "    pos_i = pos_encoding[i]\n",
    "    similarity = F.cosine_similarity(pos_0.unsqueeze(0), pos_i.unsqueeze(0))\n",
    "    similarities.append(similarity.item())\n",
    "\n",
    "plt.plot(similarities, 'o-')\n",
    "plt.title('Similarity to Position 0')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Properties of Sinusoidal Encoding:\")\n",
    "print(\"• Each position has a unique signature\")\n",
    "print(\"• Different frequencies capture different granularities\")\n",
    "print(\"• Low frequencies change slowly (long-range patterns)\")\n",
    "print(\"• High frequencies change quickly (fine-grained positions)\")\n",
    "print(\"• The model can learn to attend to relative positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learned Positional Embeddings\n",
    "\n",
    "An alternative approach is to learn positional embeddings just like word embeddings. This is simpler but requires a fixed maximum sequence length during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"Get positional embeddings for sequence length.\"\"\"\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        \n",
    "        positions = torch.arange(seq_len)\n",
    "        return self.embedding(positions)\n",
    "\n",
    "# Create learned positional embeddings\n",
    "learned_pos = LearnedPositionalEmbedding(max_len=50, d_model=8)\n",
    "\n",
    "# Get embeddings for a sequence\n",
    "seq_len = 10\n",
    "learned_encoding = learned_pos(seq_len)\n",
    "\n",
    "print(f\"Learned positional encoding shape: {learned_encoding.shape}\")\n",
    "print(f\"First few positions (before training):\")\n",
    "for i in range(5):\n",
    "    print(f\"Position {i}: {learned_encoding[i][:4].detach().tolist()!r}...\")  # Show first 4 dimensions\n",
    "\n",
    "# Compare sinusoidal vs learned (before training)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Sinusoidal encoding\n",
    "sns.heatmap(pos_encoding[:seq_len].T, cmap='RdBu_r', center=0, ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Sinusoidal Positional Encoding')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Learned encoding (random initialization)\n",
    "sns.heatmap(learned_encoding.detach().T, cmap='RdBu_r', center=0, ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Learned Positional Encoding\\n(Random Initialization)')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Sinusoidal Encoding:\")\n",
    "print(\"  ✓ Works for any sequence length\")\n",
    "print(\"  ✓ Has mathematical structure (relative positions)\")\n",
    "print(\"  ✓ No additional parameters\")\n",
    "print(\"\\nLearned Encoding:\")\n",
    "print(\"  ✓ Can adapt to specific tasks\")\n",
    "print(\"  ✓ Often works better in practice\")\n",
    "print(\"  ✗ Limited to training sequence length\")\n",
    "print(\"  ✗ Requires additional parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Word and Position Embeddings\n",
    "\n",
    "Now let's see how positional encoding solves the word order problem by adding it to word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_position_problem_with_encoding():\n",
    "    \"\"\"Show how positional encoding solves the word order problem.\"\"\"\n",
    "    \n",
    "    # Use our previous example\n",
    "    sentence1 = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "    sentence2 = [\"cat\", \"the\", \"mat\", \"sat\", \"on\"]\n",
    "    \n",
    "    # Get word embeddings\n",
    "    vocab = {\"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4}\n",
    "    d_model = 4\n",
    "    \n",
    "    word_embeddings = torch.tensor([\n",
    "        [1.0, 0.0, 0.0, 0.0],  # \"the\"\n",
    "        [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
    "        [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n",
    "        [0.0, 0.0, 0.0, 1.0],  # \"on\"\n",
    "        [1.0, 1.0, 0.0, 0.0],  # \"mat\"\n",
    "    ])\n",
    "    \n",
    "    # Create positional encoding for sequence length\n",
    "    pos_enc = create_sinusoidal_encoding(5, d_model)\n",
    "    \n",
    "    # Convert sentences to embeddings\n",
    "    def sentence_to_embeddings(sentence):\n",
    "        word_embs = torch.stack([word_embeddings[vocab[word]] for word in sentence])\n",
    "        pos_embs = pos_enc[:len(sentence)]\n",
    "        combined = word_embs + pos_embs  # Add positional encoding\n",
    "        return word_embs, pos_embs, combined\n",
    "    \n",
    "    # Process both sentences\n",
    "    word_emb1, pos_emb1, combined1 = sentence_to_embeddings(sentence1)\n",
    "    word_emb2, pos_emb2, combined2 = sentence_to_embeddings(sentence2)\n",
    "    \n",
    "    print(\"Sentence 1:\", \" \".join(sentence1))\n",
    "    print(\"Sentence 2:\", \" \".join(sentence2))\n",
    "    print()\n",
    "    \n",
    "    # Show embeddings for first word in each position\n",
    "    print(\"Word embeddings + Positional encoding:\")\n",
    "    print(\"Position | Word1 | Word2 | Same?\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for i in range(5):\n",
    "        word1 = sentence1[i]\n",
    "        word2 = sentence2[i]\n",
    "        emb1 = combined1[i]\n",
    "        emb2 = combined2[i]\n",
    "        same = torch.allclose(emb1, emb2)\n",
    "        print(f\"{i:8} | {word1:5} | {word2:5} | {same}\")\n",
    "    \n",
    "    # Compare overall sentence representations\n",
    "    sum1 = combined1.sum(dim=0)\n",
    "    sum2 = combined2.sum(dim=0)\n",
    "    \n",
    "    print(f\"\\nSentence representations (sum):\")\n",
    "    print(f\"Sentence 1: {sum1.tolist()!r}\")\n",
    "    print(f\"Sentence 2: {sum2.tolist()!r}\")\n",
    "    print(f\"Are they equal? {torch.allclose(sum1, sum2)}\")\n",
    "    \n",
    "    print(\"\\n✅ Success! Positional encoding makes word order matter!\")\n",
    "    \n",
    "    return combined1, combined2, pos_emb1\n",
    "\n",
    "combined1, combined2, pos_embeddings = solve_position_problem_with_encoding()\n",
    "\n",
    "# Visualize the effect\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Word embeddings\n",
    "word_emb1 = combined1 - pos_embeddings  # Extract word embeddings\n",
    "sns.heatmap(word_emb1.T, annot=True, cmap='Blues', ax=axes[0, 0], cbar=False)\n",
    "axes[0, 0].set_title('Word Embeddings\\n(Sentence 1)')\n",
    "axes[0, 0].set_xlabel('Position')\n",
    "axes[0, 0].set_ylabel('Dimension')\n",
    "\n",
    "# Positional embeddings\n",
    "sns.heatmap(pos_embeddings.T, annot=True, cmap='Reds', ax=axes[0, 1], cbar=False)\n",
    "axes[0, 1].set_title('Positional Embeddings')\n",
    "axes[0, 1].set_xlabel('Position')\n",
    "axes[0, 1].set_ylabel('Dimension')\n",
    "\n",
    "# Combined embeddings\n",
    "sns.heatmap(combined1.T, annot=True, cmap='Greens', ax=axes[0, 2], cbar=False)\n",
    "axes[0, 2].set_title('Combined Embeddings\\n(Word + Position)')\n",
    "axes[0, 2].set_xlabel('Position')\n",
    "axes[0, 2].set_ylabel('Dimension')\n",
    "\n",
    "# Sentence 2\n",
    "word_emb2 = combined2 - pos_embeddings\n",
    "sns.heatmap(word_emb2.T, annot=True, cmap='Blues', ax=axes[1, 0], cbar=False)\n",
    "axes[1, 0].set_title('Word Embeddings\\n(Sentence 2 - Scrambled)')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_ylabel('Dimension')\n",
    "\n",
    "sns.heatmap(pos_embeddings.T, annot=True, cmap='Reds', ax=axes[1, 1], cbar=False)\n",
    "axes[1, 1].set_title('Positional Embeddings\\n(Same for both)')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].set_ylabel('Dimension')\n",
    "\n",
    "sns.heatmap(combined2.T, annot=True, cmap='Greens', ax=axes[1, 2], cbar=False)\n",
    "axes[1, 2].set_title('Combined Embeddings\\n(Different from Sentence 1!)')\n",
    "axes[1, 2].set_xlabel('Position')\n",
    "axes[1, 2].set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Relative Positional Encoding\n",
    "\n",
    "Modern transformers often use relative positional encoding, which focuses on the *distance* between positions rather than absolute positions. This can be more effective for tasks where relative relationships matter more than absolute positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_relative_positions():\n",
    "    \"\"\"Show the concept of relative positional encoding.\"\"\"\n",
    "    \n",
    "    seq_len = 6\n",
    "    \n",
    "    # Create a matrix of relative distances\n",
    "    relative_distances = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            relative_distances[i, j] = j - i  # Relative position of j from i\n",
    "    \n",
    "    print(\"Relative Position Matrix:\")\n",
    "    print(\"(Each cell shows position j relative to position i)\")\n",
    "    print(relative_distances.numpy().astype(int))\n",
    "    print()\n",
    "    \n",
    "    # Create simple relative positional encoding\n",
    "    # In practice, this would be more sophisticated\n",
    "    max_relative_distance = seq_len - 1\n",
    "    relative_embeddings = torch.randn(2 * max_relative_distance + 1, 4)  # d_model = 4\n",
    "    \n",
    "    print(\"Relative embeddings for different distances:\")\n",
    "    for i in range(-max_relative_distance, max_relative_distance + 1):\n",
    "        idx = i + max_relative_distance\n",
    "        print(f\"Distance {i:2}: {relative_embeddings[idx][:2].tolist()!r}...\")\n",
    "    \n",
    "    # Visualize relative distances\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(relative_distances, annot=True, cmap='RdBu_r', center=0, \n",
    "                fmt='d', cbar_kws={'label': 'Relative Distance'})\n",
    "    plt.title('Relative Position Matrix')\n",
    "    plt.xlabel('Position j')\n",
    "    plt.ylabel('Position i')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    distances = range(-max_relative_distance, max_relative_distance + 1)\n",
    "    # Plot first dimension of relative embeddings\n",
    "    plt.plot(distances, relative_embeddings[:, 0], 'o-', label='Dimension 0')\n",
    "    plt.plot(distances, relative_embeddings[:, 1], 's-', label='Dimension 1')\n",
    "    plt.title('Relative Embeddings by Distance')\n",
    "    plt.xlabel('Relative Distance')\n",
    "    plt.ylabel('Embedding Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAdvantages of Relative Positioning:\")\n",
    "    print(\"• Focuses on relationships between positions\")\n",
    "    print(\"• Can generalize to longer sequences\")\n",
    "    print(\"• More natural for many language tasks\")\n",
    "    print(\"• Used in modern models like Transformer-XL, T5\")\n",
    "\n",
    "demonstrate_relative_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Complete Positional Embedding Layer\n",
    "\n",
    "Let's implement a complete positional embedding layer that can use either sinusoidal or learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.embeddings import GPTEmbedding\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Complete positional embedding layer with multiple options.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, max_len: int, d_model: int, \n",
    "                 pos_type: str = \"learned\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_type = pos_type\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if pos_type == \"learned\":\n",
    "            self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        elif pos_type == \"sinusoidal\":\n",
    "            # Register as buffer (not a parameter)\n",
    "            pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n",
    "            self.register_buffer('pos_encoding', pos_encoding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pos_type: {pos_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize token embeddings\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        if pos_type == \"learned\":\n",
    "            nn.init.normal_(self.pos_embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: combine token and positional embeddings.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Token IDs [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Combined embeddings [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(token_ids)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if self.pos_type == \"learned\":\n",
    "            positions = torch.arange(seq_len, device=token_ids.device)\n",
    "            pos_emb = self.pos_embedding(positions)  # [seq_len, d_model]\n",
    "            pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        else:  # sinusoidal\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)  # [1, seq_len, d_model]\n",
    "            pos_emb = pos_emb.expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_emb + pos_emb\n",
    "        \n",
    "        # Apply dropout\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test both types of positional embedding\n",
    "vocab_size, max_len, d_model = 100, 20, 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "# Create test input\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Test learned embeddings\n",
    "learned_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"learned\")\n",
    "output_learned = learned_emb(token_ids)\n",
    "\n",
    "# Test sinusoidal embeddings\n",
    "sinusoidal_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"sinusoidal\")\n",
    "output_sinusoidal = sinusoidal_emb(token_ids)\n",
    "\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Output embeddings shape: {output_learned.shape}\")\n",
    "print()\n",
    "\n",
    "# Compare parameter counts\n",
    "learned_params = sum(p.numel() for p in learned_emb.parameters())\n",
    "sinusoidal_params = sum(p.numel() for p in sinusoidal_emb.parameters())\n",
    "\n",
    "print(f\"Parameter comparison:\")\n",
    "print(f\"Learned embeddings:    {learned_params:,} parameters\")\n",
    "print(f\"Sinusoidal embeddings: {sinusoidal_params:,} parameters\")\n",
    "print(f\"Difference: {learned_params - sinusoidal_params:,} (positional embedding table)\")\n",
    "\n",
    "# Visualize the embeddings for first batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learned embeddings\n",
    "sns.heatmap(output_learned[0].detach().T, cmap='viridis', ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Learned Positional Embeddings')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Sinusoidal embeddings\n",
    "sns.heatmap(output_sinusoidal[0].detach().T, cmap='viridis', ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Sinusoidal Positional Embeddings')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Both embedding types work and produce the same output shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Position Encoding in Practice\n",
    "\n",
    "Let's see how our transformer implementation uses positional encoding and test it with a real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "def test_positional_encoding_in_transformer():\n",
    "    \"\"\"Test how positional encoding affects transformer behavior.\"\"\"\n",
    "    \n",
    "    # Create a small model\n",
    "    config = create_model_config(\"tiny\")\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Test sentences with different word orders\n",
    "    sentences = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The mat sat on the cat\",  # Different meaning!\n",
    "        \"Cat the sat mat on the\",  # Scrambled\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing positional encoding in transformer:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Tokenize\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            token_ids = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Get model output\n",
    "            logits, attention_weights = model(token_ids, return_attention=True)\n",
    "            \n",
    "            # Get the embedding layer output (after positional encoding)\n",
    "            embeddings = model.embedding(token_ids)\n",
    "            \n",
    "            print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "            print(f\"Token IDs: {tokens}\")\n",
    "            print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            \n",
    "            # Show how embeddings differ for same word in different positions\n",
    "            if i == 0:  # First sentence\n",
    "                first_embeddings = embeddings.clone()\n",
    "                print(f\"First word embedding: {embeddings[0, 0, :3].tolist()!r}...\")  # First 3 dims\n",
    "            else:\n",
    "                # Compare embeddings\n",
    "                embedding_diff = torch.norm(embeddings - first_embeddings).item()\n",
    "                print(f\"Difference from first sentence: {embedding_diff:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Key Observations:\")\n",
    "    print(\"• Same words in different positions have different embeddings\")\n",
    "    print(\"• Different word orders produce different representations\")\n",
    "    print(\"• The model can distinguish between sentences with same words\")\n",
    "    print(\"• Positional encoding enables understanding of word order!\")\n",
    "\n",
    "test_positional_encoding_in_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored how transformers understand position:\n",
    "\n",
    "1. **The Position Problem** - Transformers need explicit positional information\n",
    "2. **Sinusoidal Encoding** - Elegant mathematical solution using sine/cosine waves\n",
    "3. **Learned Embeddings** - Simple alternative that learns position representations\n",
    "4. **Relative Positions** - Modern approaches focusing on relationships\n",
    "5. **Complete Implementation** - Building a full positional embedding layer\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Position is crucial**: Without it, \"cat sat on mat\" = \"mat on sat cat\"\n",
    "- **Sinusoidal encoding**: Each position gets a unique frequency signature\n",
    "- **Learned embeddings**: Often work better but limited to training length\n",
    "- **Relative positions**: Focus on relationships rather than absolute positions\n",
    "- **Addition works**: Simply adding positional to word embeddings is effective\n",
    "\n",
    "### Design Choices:\n",
    "\n",
    "- **Sinusoidal**: Use for unlimited sequence length, mathematical elegance\n",
    "- **Learned**: Use for better task-specific performance, fixed length\n",
    "- **Relative**: Use for tasks where relationships matter more than absolute position\n",
    "\n",
    "### Modern Trends:\n",
    "\n",
    "- Most large language models use learned positional embeddings\n",
    "- Relative positioning is gaining popularity (T5, Transformer-XL)\n",
    "- Some models use rotary positional encoding (RoPE) for better extrapolation\n",
    "\n",
    "Next, we'll explore the training process and see how transformers learn from data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}