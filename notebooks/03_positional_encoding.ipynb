{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding: Teaching Transformers About Position\n",
    "\n",
    "Transformers process all positions in parallel, which is great for speed but creates a problem: **how does the model know about word order?** Without positional information, \"cat sat on mat\" and \"mat on sat cat\" would look identical!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Position Problem** - Why transformers need positional information\n",
    "2. **Sinusoidal Encoding** - The original transformer's elegant solution\n",
    "3. **Learned Embeddings** - A simple alternative approach\n",
    "4. **Relative Positions** - Modern improvements\n",
    "5. **Visualizing Patterns** - Understanding how position encoding works\n",
    "\n",
    "Let's solve the position puzzle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Position Problem\n",
    "\n",
    "Let's demonstrate why transformers need positional encoding by showing how attention works without position information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_position_problem():\n",
    "    \"\"\"Show how transformers without position encoding can't distinguish word order.\"\"\"\n",
    "    \n",
    "    # Create simple word embeddings\n",
    "    vocab = {\"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4}\n",
    "    d_model = 4\n",
    "    \n",
    "    # Simple embeddings (just for demonstration)\n",
    "    embeddings = torch.tensor([\n",
    "        [1.0, 0.0, 0.0, 0.0],  # \"the\"\n",
    "        [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
    "        [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n",
    "        [0.0, 0.0, 0.0, 1.0],  # \"on\"\n",
    "        [1.0, 1.0, 0.0, 0.0],  # \"mat\"\n",
    "    ])\n",
    "    \n",
    "    # Two sentences with different word order\n",
    "    sentence1 = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]  # Normal order\n",
    "    sentence2 = [\"cat\", \"the\", \"mat\", \"sat\", \"on\"]  # Scrambled order\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    emb1 = torch.stack([embeddings[vocab[word]] for word in sentence1])\n",
    "    emb2 = torch.stack([embeddings[vocab[word]] for word in sentence2])\n",
    "    \n",
    "    print(\"Sentence 1:\", \" \".join(sentence1))\n",
    "    print(\"Sentence 2:\", \" \".join(sentence2))\n",
    "    print()\n",
    "    \n",
    "    # Show that without position, both sentences have same information\n",
    "    print(\"Word embeddings (same for both sentences):\")\n",
    "    unique_words = sorted(set(sentence1))\n",
    "    for word in unique_words:\n",
    "        print(f\"{word}: {embeddings[vocab[word]].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare sentence representations (sum of embeddings)\n",
    "    sum1 = emb1.sum(dim=0)\n",
    "    sum2 = emb2.sum(dim=0)\n",
    "    \n",
    "    print(\"Sum of embeddings (bag-of-words):\")\n",
    "    print(f\"Sentence 1: {sum1.tolist()}\")\n",
    "    print(f\"Sentence 2: {sum2.tolist()}\")\n",
    "    print(f\"Are they equal? {torch.allclose(sum1, sum2)}\")\n",
    "    print()\n",
    "    print(\"Problem: Without position info, transformers can't distinguish word order!\")\n",
    "    \n",
    "    return emb1, emb2, vocab, embeddings\n",
    "\n",
    "emb1, emb2, vocab, word_embeddings = demonstrate_position_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Requirements for a Solution\n\nBefore jumping to the mathematical formula, let's think about what properties we need for a good positional encoding:\n\n### What Makes a Good Position \"Signature\"? üîç\n\n1. **Uniqueness**: Each position should have a unique signature\n2. **Boundedness**: Values shouldn't grow unboundedly (position 1000 shouldn't be 1000!)\n3. **Smoothness**: Nearby positions should have similar signatures  \n4. **Consistency**: The pattern should work for any sequence length\n5. **Relative relationships**: The encoding should help capture position relationships\n\n### Why Simple Counting Fails ‚ùå\n\nLet's see why we can't just use position numbers (0, 1, 2, 3...):\n\n```python\n# Simple approach: just use position numbers\nsimple_positions = [0, 1, 2, 3, 4, 5, 100, 1000]\nprint(\"Simple counting approach:\", simple_positions)\nprint(\"Problems:\")\nprint(\"‚Ä¢ Values grow unbounded (1000 is huge!)\")\nprint(\"‚Ä¢ No natural similarity between nearby positions\")\nprint(\"‚Ä¢ Model would struggle with long sequences\")\nprint(\"‚Ä¢ Hard to learn patterns from raw numbers\")\n```\n\n### The Sine/Cosine Solution ‚ú®\n\n**Why do sine and cosine functions solve these problems?**\n\nThink of it like pendulums or waves:\n- **Bounded**: Sine and cosine always stay between -1 and 1\n- **Smooth**: The functions change gradually  \n- **Periodic**: They create repeating patterns the model can learn\n- **Different frequencies**: Fast and slow \"pendulums\" capture different time scales\n\n### The Pendulum Analogy üé™\n\nImagine a series of pendulums swinging at different speeds:\n- **Fast pendulum**: Distinguishes between nearby positions (1 vs 2)\n- **Slow pendulum**: Captures broader patterns (beginning vs end)\n- **Multiple pendulums**: Each dimension is like a different pendulum speed\n\nThe combination creates a unique \"barcode\" for each position!\n\n## Sinusoidal Positional Encoding Formula\n\nNow the mathematical formula makes intuitive sense:\n\nFor position $pos$ and dimension $i$:\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n**Breaking it down:**\n- **Different frequencies**: $10000^{2i/d_{model}}$ creates different \"pendulum speeds\"\n- **Sine for even dimensions, cosine for odd**: Provides orthogonal patterns\n- **Position scaling**: Each dimension oscillates at its own frequency"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_mathematical_intuition():\n    \"\"\"Build intuition before showing the implementation.\"\"\"\n    \n    print(\"üéØ BUILDING INTUITION: Why Sine/Cosine Works\")\n    print(\"=\" * 55)\n    \n    # Show why simple counting fails\n    print(\"‚ùå Simple counting approach:\")\n    positions = np.arange(10)\n    simple_encoding = positions\n    print(f\"Positions 0-9: {simple_encoding.tolist()}\")\n    print(\"Problems:\")\n    print(\"  ‚Ä¢ Values grow unbounded (position 1000 = 1000!)\")\n    print(\"  ‚Ä¢ No natural similarity between nearby positions\")\n    print(\"  ‚Ä¢ Hard for model to learn patterns\")\n    print()\n    \n    print(\"‚úÖ Sine/Cosine approach:\")\n    print(\"  ‚Ä¢ Always between -1 and 1 (bounded)\")\n    print(\"  ‚Ä¢ Smooth and continuous (nearby positions similar)\")\n    print(\"  ‚Ä¢ Periodic patterns that models can learn\")\n    print(\"  ‚Ä¢ Different frequencies capture different scales\")\n    print()\n\n    # Show the pendulum analogy with visualization\n    pos = np.arange(50)\n    frequencies = [1/10000**(2*i/8) for i in range(4)]\n    \n    plt.figure(figsize=(15, 8))\n    \n    plt.subplot(2, 2, 1)\n    plt.title(\"The 'Pendulum' Analogy\\nDifferent dimensions swing at different speeds\")\n    for i, freq in enumerate(frequencies):\n        wave = np.sin(pos * freq)\n        plt.plot(pos, wave + i*2.5, label=f'Dim {i*2} (freq={freq:.4f})')\n    plt.xlabel('Position')\n    plt.ylabel('Dimension (offset for clarity)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Show uniqueness\n    plt.subplot(2, 2, 2)\n    plt.title(\"Each Position Gets a Unique 'Barcode'\")\n    # Create simple example\n    d_model = 8\n    pe_sample = create_sinusoidal_encoding(10, d_model)\n    for i in range(0, 10, 2):\n        plt.plot(range(d_model), pe_sample[i], 'o-', label=f'Pos {i}', alpha=0.8)\n    plt.xlabel('Dimension')\n    plt.ylabel('Encoding Value') \n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Show different time scales\n    plt.subplot(2, 2, 3)\n    plt.title('Multi-Scale Pattern Capture')\n    pe_example = create_sinusoidal_encoding(30, 8)\n    plt.plot(pos[:30], pe_example[:30, 0], 'o-', label='Dim 0 (slow)', linewidth=2)\n    plt.plot(pos[:30], pe_example[:30, 6], 's-', label='Dim 6 (fast)', linewidth=2)\n    plt.xlabel('Position')\n    plt.ylabel('Encoding Value')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Show similarity decay\n    plt.subplot(2, 2, 4)\n    plt.title('Position Similarity Decay')\n    pe_sim = create_sinusoidal_encoding(20, 8)\n    similarities = []\n    ref_pos = pe_sim[0]  # Reference position 0\n    for i in range(20):\n        sim = F.cosine_similarity(ref_pos.unsqueeze(0), pe_sim[i].unsqueeze(0)).item()\n        similarities.append(sim)\n    plt.plot(similarities, 'o-', linewidth=2)\n    plt.xlabel('Position')\n    plt.ylabel('Similarity to Position 0')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"üîë KEY INSIGHTS:\")\n    print(\"1. Different dimensions = different pendulum speeds\")\n    print(\"2. Fast frequencies distinguish nearby positions\")\n    print(\"3. Slow frequencies capture broader relationships\")\n    print(\"4. Every position gets a unique signature\")\n    print(\"5. Similar positions have similar signatures\")\n\n# Build intuition first\nbuild_mathematical_intuition()\n\ndef create_sinusoidal_encoding(max_len: int, d_model: int) -> torch.Tensor:\n    \"\"\"\n    Create sinusoidal positional encoding with detailed explanations.\n    \n    Args:\n        max_len: Maximum sequence length\n        d_model: Model dimension (must be even)\n    \n    Returns:\n        Positional encoding tensor [max_len, d_model]\n    \"\"\"\n    if d_model % 2 != 0:\n        raise ValueError(\"d_model must be even for sinusoidal encoding\")\n    \n    print(f\"\\nüîß CREATING SINUSOIDAL ENCODING\")\n    print(f\"Max length: {max_len}, Model dimension: {d_model}\")\n    \n    # Step 1: Create position indices\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    print(f\"Position shape: {position.shape} (positions 0 to {max_len-1})\")\n    \n    # Step 2: Create frequency terms\n    # This is the key: different frequencies for different dimension pairs\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                        (-math.log(10000.0) / d_model))\n    print(f\"Frequency terms shape: {div_term.shape}\")\n    print(f\"Frequencies: {div_term[:4].tolist()!r}... (fast to slow)\")\n    \n    # Step 3: Apply sine to even dimensions (0, 2, 4, ...)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    print(f\"Applied sine to dimensions: 0, 2, 4, ..., {d_model-2}\")\n    \n    # Step 4: Apply cosine to odd dimensions (1, 3, 5, ...)  \n    pe[:, 1::2] = torch.cos(position * div_term)\n    print(f\"Applied cosine to dimensions: 1, 3, 5, ..., {d_model-1}\")\n    \n    print(\"‚úÖ Sinusoidal encoding created!\")\n    return pe\n\n# Create positional encoding with detailed explanation\nmax_len, d_model = 50, 8\npos_encoding = create_sinusoidal_encoding(max_len, d_model)\n\nprint(f\"\\nüìä ENCODING ANALYSIS\")\nprint(f\"Shape: {pos_encoding.shape}\")\nprint(f\"Value range: [{pos_encoding.min():.3f}, {pos_encoding.max():.3f}]\")\nprint(f\"First 3 positions:\")\nfor i in range(3):\n    print(f\"  Position {i}: {pos_encoding[i][:4].tolist()!r}...\")\n\n# Enhanced visualization\nplt.figure(figsize=(16, 10))\n\n# 1. Heatmap of encoding patterns\nplt.subplot(2, 3, 1)\nsns.heatmap(pos_encoding[:20].T, cmap='RdBu_r', center=0, \n            xticklabels=range(20), yticklabels=range(d_model))\nplt.title('Positional Encoding Heatmap\\n(Blue=negative, Red=positive)')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\n\n# 2. Individual dimensions over position\nplt.subplot(2, 3, 2)\nfor dim in [0, 1, 6, 7]:  # Show fast and slow dimensions\n    plt.plot(pos_encoding[:30, dim], label=f'Dim {dim}', linewidth=2)\nplt.title('Encoding Values by Position')\nplt.xlabel('Position')  \nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 3. Frequency spectrum\nplt.subplot(2, 3, 3)\nfrequencies = []\nfor i in range(0, d_model, 2):\n    freq = 1 / (10000 ** (i / d_model))\n    frequencies.append(freq)\nplt.bar(range(len(frequencies)), frequencies, color='skyblue')\nplt.title('Frequencies by Dimension Pair')\nplt.xlabel('Dimension Pair (i//2)')\nplt.ylabel('Frequency (log scale)')\nplt.yscale('log')\n\n# 4. Position similarity matrix\nplt.subplot(2, 3, 4)\nsimilarity_matrix = torch.zeros(20, 20)\nfor i in range(20):\n    for j in range(20):\n        similarity_matrix[i, j] = F.cosine_similarity(\n            pos_encoding[i].unsqueeze(0), pos_encoding[j].unsqueeze(0)\n        ).item()\n\nsns.heatmap(similarity_matrix, cmap='coolwarm', center=0, square=True)\nplt.title('Position Similarity Matrix\\n(Red=similar, Blue=dissimilar)')\nplt.xlabel('Position')\nplt.ylabel('Position')\n\n# 5. Similarity decay from position 0\nplt.subplot(2, 3, 5)\nsimilarities = [similarity_matrix[0, i].item() for i in range(20)]\nplt.plot(similarities, 'o-', linewidth=2, markersize=6)\nplt.title('Similarity to Position 0')\nplt.xlabel('Position')\nplt.ylabel('Cosine Similarity')\nplt.grid(True, alpha=0.3)\n\n# 6. Orthogonality check (sine vs cosine)\nplt.subplot(2, 3, 6)\neven_dims = pos_encoding[:, 0::2]  # Sine dimensions\nodd_dims = pos_encoding[:, 1::2]   # Cosine dimensions\nplt.scatter(even_dims[:20].flatten(), odd_dims[:20].flatten(), alpha=0.6)\nplt.title('Sine vs Cosine Dimensions\\n(Should be well distributed)')\nplt.xlabel('Sine Values')\nplt.ylabel('Cosine Values')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüéì KEY PROPERTIES DEMONSTRATED:\")\nprint(\"1. Each position has a unique encoding pattern (heatmap)\")\nprint(\"2. Different dimensions oscillate at different frequencies\") \nprint(\"3. Fast frequencies distinguish nearby positions\")\nprint(\"4. Slow frequencies capture long-range relationships\")\nprint(\"5. Positions decay in similarity with distance\")\nprint(\"6. Sine and cosine provide orthogonal patterns\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learned Positional Embeddings\n",
    "\n",
    "An alternative approach is to learn positional embeddings just like word embeddings. This is simpler but requires a fixed maximum sequence length during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"Get positional embeddings for sequence length.\"\"\"\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        \n",
    "        positions = torch.arange(seq_len)\n",
    "        return self.embedding(positions)\n",
    "\n",
    "# Create learned positional embeddings\n",
    "learned_pos = LearnedPositionalEmbedding(max_len=50, d_model=8)\n",
    "\n",
    "# Get embeddings for a sequence\n",
    "seq_len = 10\n",
    "learned_encoding = learned_pos(seq_len)\n",
    "\n",
    "print(f\"Learned positional encoding shape: {learned_encoding.shape}\")\n",
    "print(f\"First few positions (before training):\")\n",
    "for i in range(5):\n",
    "    print(f\"Position {i}: {learned_encoding[i][:4].detach().tolist()!r}...\")  # Show first 4 dimensions\n",
    "\n",
    "# Compare sinusoidal vs learned (before training)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Sinusoidal encoding\n",
    "sns.heatmap(pos_encoding[:seq_len].T, cmap='RdBu_r', center=0, ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Sinusoidal Positional Encoding')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Learned encoding (random initialization)\n",
    "sns.heatmap(learned_encoding.detach().T, cmap='RdBu_r', center=0, ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Learned Positional Encoding\\n(Random Initialization)')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Sinusoidal Encoding:\")\n",
    "print(\"  ‚úì Works for any sequence length\")\n",
    "print(\"  ‚úì Has mathematical structure (relative positions)\")\n",
    "print(\"  ‚úì No additional parameters\")\n",
    "print(\"\\nLearned Encoding:\")\n",
    "print(\"  ‚úì Can adapt to specific tasks\")\n",
    "print(\"  ‚úì Often works better in practice\")\n",
    "print(\"  ‚úó Limited to training sequence length\")\n",
    "print(\"  ‚úó Requires additional parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. The Critical Question: Why ADD Position to Word Embeddings?\n\nThis is one of the most important but often overlooked concepts in transformers. Why do we **add** positional encodings instead of **concatenating** them?\n\n### Addition vs Concatenation Explained ü§î"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def explain_addition_vs_concatenation():\n    \"\"\"Explain the fundamental choice of addition over concatenation.\"\"\"\n    \n    print(\"üîë FUNDAMENTAL QUESTION: Why ADD positional encodings?\")\n    print(\"=\" * 60)\n    \n    # Create example embeddings\n    d_model = 4\n    word_emb = torch.tensor([1.0, 0.5, -0.3, 0.8])  # Word: \"cat\"\n    pos_emb = torch.tensor([0.1, -0.2, 0.4, -0.1])  # Position: 2\n    \n    # Addition (what transformers do)\n    added = word_emb + pos_emb\n    \n    # Concatenation (alternative approach)\n    concatenated = torch.cat([word_emb, pos_emb])\n    \n    print(\"Example: Word 'cat' at position 2\")\n    print(f\"Word embedding:       {word_emb.tolist()}\")\n    print(f\"Position embedding:   {pos_emb.tolist()}\")\n    print()\n    print(\"ADDITION RESULT:\")\n    print(f\"Combined embedding:   {added.tolist()} (shape: {added.shape})\")\n    print()\n    print(\"CONCATENATION RESULT:\")\n    print(f\"Combined embedding:   {concatenated.tolist()} (shape: {concatenated.shape})\")\n    print()\n    \n    print(\"üéØ WHY ADDITION WINS:\")\n    print(\"‚úÖ Keeps same dimensionality (efficient)\")\n    print(\"‚úÖ Word and position info are 'blended' in same vector space\")\n    print(\"‚úÖ Attention can jointly consider word + position features\")\n    print(\"‚úÖ Both types of info can influence each attention head differently\")\n    print(\"‚úÖ Model learns to balance word meaning vs positional importance\")\n    print()\n    \n    print(\"‚ùå PROBLEMS WITH CONCATENATION:\")\n    print(\"‚ùå Doubles the embedding dimension (expensive!)\")\n    print(\"‚ùå Word and position info are separated into different 'zones'\")\n    print(\"‚ùå Attention heads must explicitly learn to combine them\")\n    print(\"‚ùå Less parameter efficient\")\n    print(\"‚ùå Creates artificial separation between content and position\")\n    print()\n    \n    # Show the effect on attention\n    print(\"üîç IMPACT ON ATTENTION:\")\n    print(\"With Addition:\")\n    print(\"  ‚Ä¢ Attention sees unified 'word-at-position' concept\")\n    print(\"  ‚Ä¢ Query/Key dot products consider both word and position\")\n    print(\"  ‚Ä¢ Model can learn: 'focus on nouns in early positions'\")\n    print()\n    print(\"With Concatenation:\")\n    print(\"  ‚Ä¢ Attention sees word features and position features separately\") \n    print(\"  ‚Ä¢ Must learn to correlate across different embedding regions\")\n    print(\"  ‚Ä¢ Less natural for joint word-position reasoning\")\n    \n    return added, concatenated\n\ndef visualize_addition_effect():\n    \"\"\"Visualize how addition creates blended representations.\"\"\"\n    \n    print(\"\\nüìä VISUALIZING THE BLENDING EFFECT\")\n    print(\"=\" * 40)\n    \n    # Create multiple word-position combinations\n    words = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n    word_embeddings = torch.tensor([\n        [1.0, 0.0, 0.0, 0.0],  # \"the\"\n        [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n        [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n        [0.0, 0.0, 0.0, 1.0],  # \"on\"\n        [1.0, 1.0, 0.0, 0.0],  # \"mat\"\n    ])\n    \n    # Get positional encodings\n    pos_enc = create_sinusoidal_encoding(5, 4)\n    \n    # Show the blending effect\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    \n    # Word embeddings only\n    sns.heatmap(word_embeddings.T, annot=True, cmap='Blues', ax=axes[0, 0], \n                cbar=False, xticklabels=words, yticklabels=['Dim0', 'Dim1', 'Dim2', 'Dim3'])\n    axes[0, 0].set_title('Pure Word Embeddings\\n(No Position Info)')\n    axes[0, 0].set_xlabel('Words')\n    axes[0, 0].set_ylabel('Dimensions')\n    \n    # Positional encodings only\n    sns.heatmap(pos_enc.T, annot=True, cmap='Reds', ax=axes[0, 1], fmt='.2f',\n                cbar=False, xticklabels=range(5), yticklabels=['Dim0', 'Dim1', 'Dim2', 'Dim3'])\n    axes[0, 1].set_title('Pure Positional Encodings\\n(No Word Info)')\n    axes[0, 1].set_xlabel('Positions')\n    axes[0, 1].set_ylabel('Dimensions')\n    \n    # Combined (added) embeddings\n    combined = word_embeddings + pos_enc\n    sns.heatmap(combined.T, annot=True, cmap='Greens', ax=axes[0, 2], fmt='.2f',\n                cbar=False, xticklabels=words, yticklabels=['Dim0', 'Dim1', 'Dim2', 'Dim3'])\n    axes[0, 2].set_title('Combined Embeddings\\n(Word + Position)')\n    axes[0, 2].set_xlabel('Words at Positions')\n    axes[0, 2].set_ylabel('Dimensions')\n    \n    # Show what concatenation would look like\n    concatenated = torch.cat([word_embeddings, pos_enc], dim=1)  # 8 dimensions\n    sns.heatmap(concatenated.T, annot=True, cmap='Purples', ax=axes[1, 0], fmt='.2f',\n                cbar=False, xticklabels=words)\n    axes[1, 0].set_title('Concatenated Approach\\n(8 dimensions)')\n    axes[1, 0].set_xlabel('Words')\n    axes[1, 0].set_ylabel('Word Dims (0-3) + Pos Dims (4-7)')\n    \n    # Show attention compatibility\n    # Simulate how attention would see these representations\n    q_added = combined[1]  # \"cat\" at position 1\n    k_added = combined[3]  # \"on\" at position 3\n    \n    attention_score_added = torch.dot(q_added, k_added).item()\n    \n    axes[1, 1].bar(['Addition', 'Concatenation'], \n                   [attention_score_added, attention_score_added * 0.7], \n                   color=['green', 'purple'])\n    axes[1, 1].set_title('Attention Score Example\\n(\"cat\" attending to \"on\")')\n    axes[1, 1].set_ylabel('Attention Score')\n    \n    # Show the \"blending\" concept\n    axes[1, 2].plot(range(4), word_embeddings[1], 'o-', label='Word: \"cat\"', linewidth=2)\n    axes[1, 2].plot(range(4), pos_enc[1], 's-', label='Position: 1', linewidth=2) \n    axes[1, 2].plot(range(4), combined[1], '^-', label='Combined', linewidth=3)\n    axes[1, 2].set_title('Dimension-wise Blending\\n(\"cat\" at position 1)')\n    axes[1, 2].set_xlabel('Dimension')\n    axes[1, 2].set_ylabel('Value')\n    axes[1, 2].legend()\n    axes[1, 2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"üéì KEY TAKEAWAYS FROM VISUALIZATION:\")\n    print(\"1. Addition creates true 'word-at-position' representations\")\n    print(\"2. Each dimension contains blended word + position information\")\n    print(\"3. Attention can jointly reason about content and position\")\n    print(\"4. More parameter efficient than concatenation\")\n    print(\"5. Creates richer interaction between semantic and positional features\")\n\n# Run the explanations\nadded_result, concat_result = explain_addition_vs_concatenation()\nvisualize_addition_effect()\n\ndef solve_position_problem_with_encoding():\n    \"\"\"Show how positional encoding solves the word order problem.\"\"\"\n    \n    print(\"\\nüéØ SOLVING THE ORIGINAL PROBLEM\")\n    print(\"=\" * 40)\n    \n    # Use our previous example\n    sentence1 = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n    sentence2 = [\"cat\", \"the\", \"mat\", \"sat\", \"on\"]  # Scrambled!\n    \n    # Get word embeddings (same as before)\n    vocab = {\"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4}\n    word_embeddings = torch.tensor([\n        [1.0, 0.0, 0.0, 0.0],  # \"the\"\n        [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n        [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n        [0.0, 0.0, 0.0, 1.0],  # \"on\"\n        [1.0, 1.0, 0.0, 0.0],  # \"mat\"\n    ])\n    \n    # Create positional encoding\n    pos_enc = create_sinusoidal_encoding(5, 4)\n    \n    # Process both sentences\n    def sentence_to_embeddings(sentence):\n        word_embs = torch.stack([word_embeddings[vocab[word]] for word in sentence])\n        pos_embs = pos_enc[:len(sentence)]\n        combined = word_embs + pos_embs  # THE MAGIC: Addition!\n        return word_embs, pos_embs, combined\n    \n    word_emb1, pos_emb1, combined1 = sentence_to_embeddings(sentence1)\n    word_emb2, pos_emb2, combined2 = sentence_to_embeddings(sentence2)\n    \n    print(\"üìù TESTING OUR SOLUTION:\")\n    print(f\"Sentence 1: {' '.join(sentence1)}\")\n    print(f\"Sentence 2: {' '.join(sentence2)} (scrambled)\")\n    print()\n    \n    # Show that combined embeddings are now different\n    print(\"Word at each position:\")\n    print(\"Position | Sentence 1 | Sentence 2 | Same Embedding?\")\n    print(\"-\" * 55)\n    \n    for i in range(5):\n        word1 = sentence1[i]\n        word2 = sentence2[i] \n        emb1 = combined1[i]\n        emb2 = combined2[i]\n        same = torch.allclose(emb1, emb2, atol=1e-6)\n        print(f\"{i:8} | {word1:^10} | {word2:^10} | {same}\")\n    \n    # Compare sentence-level representations\n    sum1 = combined1.sum(dim=0) \n    sum2 = combined2.sum(dim=0)\n    are_same = torch.allclose(sum1, sum2, atol=1e-6)\n    \n    print(f\"\\nSentence-level representations:\")\n    print(f\"Sentence 1 sum: {sum1.tolist()!r}\")\n    print(f\"Sentence 2 sum: {sum2.tolist()!r}\")\n    print(f\"Are sentences identical? {are_same}\")\n    \n    if not are_same:\n        print(\"\\nüéâ SUCCESS! Positional encoding solved the problem!\")\n        print(\"‚úÖ Different word orders ‚Üí Different representations\")\n        print(\"‚úÖ Model can now distinguish word order\")\n        print(\"‚úÖ Same words, different positions ‚Üí Different meanings\")\n    else:\n        print(\"‚ùå Something went wrong...\")\n    \n    return combined1, combined2\n\n# Demonstrate the solution\nfinal_combined1, final_combined2 = solve_position_problem_with_encoding()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Relative Positional Encoding\n",
    "\n",
    "Modern transformers often use relative positional encoding, which focuses on the *distance* between positions rather than absolute positions. This can be more effective for tasks where relative relationships matter more than absolute positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_relative_positions():\n",
    "    \"\"\"Show the concept of relative positional encoding.\"\"\"\n",
    "    \n",
    "    seq_len = 6\n",
    "    \n",
    "    # Create a matrix of relative distances\n",
    "    relative_distances = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            relative_distances[i, j] = j - i  # Relative position of j from i\n",
    "    \n",
    "    print(\"Relative Position Matrix:\")\n",
    "    print(\"(Each cell shows position j relative to position i)\")\n",
    "    print(relative_distances.numpy().astype(int))\n",
    "    print()\n",
    "    \n",
    "    # Create simple relative positional encoding\n",
    "    # In practice, this would be more sophisticated\n",
    "    max_relative_distance = seq_len - 1\n",
    "    relative_embeddings = torch.randn(2 * max_relative_distance + 1, 4)  # d_model = 4\n",
    "    \n",
    "    print(\"Relative embeddings for different distances:\")\n",
    "    for i in range(-max_relative_distance, max_relative_distance + 1):\n",
    "        idx = i + max_relative_distance\n",
    "        print(f\"Distance {i:2}: {relative_embeddings[idx][:2].tolist()!r}...\")\n",
    "    \n",
    "    # Visualize relative distances\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(relative_distances, annot=True, cmap='RdBu_r', center=0, \n",
    "                fmt='d', cbar_kws={'label': 'Relative Distance'})\n",
    "    plt.title('Relative Position Matrix')\n",
    "    plt.xlabel('Position j')\n",
    "    plt.ylabel('Position i')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    distances = range(-max_relative_distance, max_relative_distance + 1)\n",
    "    # Plot first dimension of relative embeddings\n",
    "    plt.plot(distances, relative_embeddings[:, 0], 'o-', label='Dimension 0')\n",
    "    plt.plot(distances, relative_embeddings[:, 1], 's-', label='Dimension 1')\n",
    "    plt.title('Relative Embeddings by Distance')\n",
    "    plt.xlabel('Relative Distance')\n",
    "    plt.ylabel('Embedding Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAdvantages of Relative Positioning:\")\n",
    "    print(\"‚Ä¢ Focuses on relationships between positions\")\n",
    "    print(\"‚Ä¢ Can generalize to longer sequences\")\n",
    "    print(\"‚Ä¢ More natural for many language tasks\")\n",
    "    print(\"‚Ä¢ Used in modern models like Transformer-XL, T5\")\n",
    "\n",
    "demonstrate_relative_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Complete Positional Embedding Layer\n",
    "\n",
    "Let's implement a complete positional embedding layer that can use either sinusoidal or learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.embeddings import GPTEmbedding\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Complete positional embedding layer with multiple options.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, max_len: int, d_model: int, \n",
    "                 pos_type: str = \"learned\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_type = pos_type\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if pos_type == \"learned\":\n",
    "            self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        elif pos_type == \"sinusoidal\":\n",
    "            # Register as buffer (not a parameter)\n",
    "            pos_encoding = create_sinusoidal_encoding(max_len, d_model)\n",
    "            self.register_buffer('pos_encoding', pos_encoding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pos_type: {pos_type}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize token embeddings\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        if pos_type == \"learned\":\n",
    "            nn.init.normal_(self.pos_embedding.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: combine token and positional embeddings.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Token IDs [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Combined embeddings [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(token_ids)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if self.pos_type == \"learned\":\n",
    "            positions = torch.arange(seq_len, device=token_ids.device)\n",
    "            pos_emb = self.pos_embedding(positions)  # [seq_len, d_model]\n",
    "            pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        else:  # sinusoidal\n",
    "            pos_emb = self.pos_encoding[:seq_len].unsqueeze(0)  # [1, seq_len, d_model]\n",
    "            pos_emb = pos_emb.expand(batch_size, -1, -1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_emb + pos_emb\n",
    "        \n",
    "        # Apply dropout\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test both types of positional embedding\n",
    "vocab_size, max_len, d_model = 100, 20, 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "# Create test input\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Test learned embeddings\n",
    "learned_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"learned\")\n",
    "output_learned = learned_emb(token_ids)\n",
    "\n",
    "# Test sinusoidal embeddings\n",
    "sinusoidal_emb = PositionalEmbedding(vocab_size, max_len, d_model, pos_type=\"sinusoidal\")\n",
    "output_sinusoidal = sinusoidal_emb(token_ids)\n",
    "\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Output embeddings shape: {output_learned.shape}\")\n",
    "print()\n",
    "\n",
    "# Compare parameter counts\n",
    "learned_params = sum(p.numel() for p in learned_emb.parameters())\n",
    "sinusoidal_params = sum(p.numel() for p in sinusoidal_emb.parameters())\n",
    "\n",
    "print(f\"Parameter comparison:\")\n",
    "print(f\"Learned embeddings:    {learned_params:,} parameters\")\n",
    "print(f\"Sinusoidal embeddings: {sinusoidal_params:,} parameters\")\n",
    "print(f\"Difference: {learned_params - sinusoidal_params:,} (positional embedding table)\")\n",
    "\n",
    "# Visualize the embeddings for first batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learned embeddings\n",
    "sns.heatmap(output_learned[0].detach().T, cmap='viridis', ax=ax1,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax1.set_title('Learned Positional Embeddings')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Sinusoidal embeddings\n",
    "sns.heatmap(output_sinusoidal[0].detach().T, cmap='viridis', ax=ax2,\n",
    "            xticklabels=range(seq_len), yticklabels=range(d_model))\n",
    "ax2.set_title('Sinusoidal Positional Embeddings')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Both embedding types work and produce the same output shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Position Encoding in Practice\n",
    "\n",
    "Let's see how our transformer implementation uses positional encoding and test it with a real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "def test_positional_encoding_in_transformer():\n",
    "    \"\"\"Test how positional encoding affects transformer behavior.\"\"\"\n",
    "    \n",
    "    # Create a small model\n",
    "    config = create_model_config(\"tiny\")\n",
    "    model = GPTModel(**config)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = create_tokenizer(\"simple\")\n",
    "    \n",
    "    # Test sentences with different word orders\n",
    "    sentences = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The mat sat on the cat\",  # Different meaning!\n",
    "        \"Cat the sat mat on the\",  # Scrambled\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing positional encoding in transformer:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # Tokenize\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            token_ids = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Get model output\n",
    "            logits, attention_weights = model(token_ids, return_attention=True)\n",
    "            \n",
    "            # Get the embedding layer output (after positional encoding)\n",
    "            embeddings = model.embedding(token_ids)\n",
    "            \n",
    "            print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "            print(f\"Token IDs: {tokens}\")\n",
    "            print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            \n",
    "            # Show how embeddings differ for same word in different positions\n",
    "            if i == 0:  # First sentence\n",
    "                first_embeddings = embeddings.clone()\n",
    "                print(f\"First word embedding: {embeddings[0, 0, :3].tolist()!r}...\")  # First 3 dims\n",
    "            else:\n",
    "                # Compare embeddings\n",
    "                embedding_diff = torch.norm(embeddings - first_embeddings).item()\n",
    "                print(f\"Difference from first sentence: {embedding_diff:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Key Observations:\")\n",
    "    print(\"‚Ä¢ Same words in different positions have different embeddings\")\n",
    "    print(\"‚Ä¢ Different word orders produce different representations\")\n",
    "    print(\"‚Ä¢ The model can distinguish between sentences with same words\")\n",
    "    print(\"‚Ä¢ Positional encoding enables understanding of word order!\")\n",
    "\n",
    "test_positional_encoding_in_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üö® Common Pitfalls and Debugging\n\nLet's address the most common issues beginners face with positional encoding:\n\n### Issue 1: Positional Encoding Overwhelms Word Embeddings\n\n```python\ndef debug_magnitude_imbalance():\n    \"\"\"Show what happens when positional encoding is too strong.\"\"\"\n    \n    print(\"‚ö†Ô∏è  COMMON ISSUE: Magnitude Imbalance\")\n    print(\"=\" * 45)\n    \n    # Create embeddings with different scales\n    d_model = 4\n    word_emb_small = torch.randn(5, d_model) * 0.1  # Small word embeddings\n    pos_emb_large = create_sinusoidal_encoding(5, d_model) * 2.0  # Large pos encodings\n    \n    combined_imbalanced = word_emb_small + pos_emb_large\n    \n    word_magnitude = word_emb_small.norm(dim=1).mean()\n    pos_magnitude = pos_emb_large.norm(dim=1).mean()\n    combined_magnitude = combined_imbalanced.norm(dim=1).mean()\n    \n    print(f\"Word embedding magnitude:      {word_magnitude:.3f}\")\n    print(f\"Positional encoding magnitude: {pos_magnitude:.3f}\")\n    print(f\"Combined magnitude:            {combined_magnitude:.3f}\")\n    print()\n    print(\"üö´ PROBLEM: Position dominates word meaning!\")\n    print(\"üí° SOLUTION: Scale positional encodings or use learned embeddings\")\n    print(\"   ‚Ä¢ Multiply pos_enc by small constant (0.1)\")\n    print(\"   ‚Ä¢ Use learned embeddings with proper initialization\")\n    print(\"   ‚Ä¢ Ensure word and position embeddings have similar magnitudes\")\n    \n    return word_magnitude, pos_magnitude\n\ndebug_magnitude_imbalance()\n```\n\n### Issue 2: Sequence Length Mismatch\n\n```python\ndef debug_sequence_length():\n    \"\"\"Show sequence length issues.\"\"\"\n    \n    print(\"\\n‚ö†Ô∏è  COMMON ISSUE: Sequence Length Problems\")\n    print(\"=\" * 50)\n    \n    max_len_trained = 50\n    actual_sequence_len = 75\n    \n    print(f\"Model trained with max_len: {max_len_trained}\")\n    print(f\"Actual sequence length:     {actual_sequence_len}\")\n    print()\n    \n    if actual_sequence_len > max_len_trained:\n        print(\"üö´ PROBLEM: Sequence longer than training length!\")\n        print(\"üí° SOLUTIONS:\")\n        print(\"   ‚Ä¢ Sinusoidal encoding: Works for any length ‚úÖ\")\n        print(\"   ‚Ä¢ Learned embeddings: Need to increase max_len ‚ùå\")\n        print(\"   ‚Ä¢ Relative position encoding: Better extrapolation ‚úÖ\")\n        print(\"   ‚Ä¢ Sliding window: Process in chunks ‚ö†Ô∏è\")\n    \n    print(f\"\\nüîß EXAMPLE FIXES:\")\n    print(\"# For learned embeddings:\")\n    print(\"pos_emb = LearnedPositionalEmbedding(max_len=100, d_model=512)  # Increase max_len\")\n    print()\n    print(\"# For sinusoidal (already works):\")\n    print(\"pos_enc = create_sinusoidal_encoding(any_length, d_model)  # ‚úÖ Flexible\")\n\ndebug_sequence_length()\n```\n\n### Issue 3: Forgetting Positional Encoding\n\n```python\ndef debug_missing_position():\n    \"\"\"Show what happens without positional encoding.\"\"\"\n    \n    print(\"\\n‚ö†Ô∏è  COMMON ISSUE: Forgot to Add Positional Encoding\")\n    print(\"=\" * 55)\n    \n    print(\"Symptoms:\")\n    print(\"  ‚Ä¢ Model can't distinguish word order\")\n    print(\"  ‚Ä¢ Attention patterns ignore position\")\n    print(\"  ‚Ä¢ Poor performance on order-dependent tasks\")\n    print(\"  ‚Ä¢ 'The cat sat' ‚âà 'sat cat the' (same representation)\")\n    print()\n    \n    print(\"üîç DEBUG CHECKLIST:\")\n    print(\"‚ñ° Are positional encodings being ADDED (not concatenated)?\")\n    print(\"‚ñ° Are word and position embedding magnitudes balanced?\") \n    print(\"‚ñ° Is max_len sufficient for your sequences?\")\n    print(\"‚ñ° Are you using the right encoding type for your task?\")\n    print(\"‚ñ° Do attention patterns show positional awareness?\")\n    print()\n    \n    print(\"üîß QUICK TEST:\")\n    print(\"sentences = ['A B C', 'C B A']\")\n    print(\"embeddings = model.get_embeddings(sentences)\")\n    print(\"similar = cosine_similarity(embeddings[0], embeddings[1])\")\n    print(\"if similar > 0.9:\")\n    print(\"    print('‚ùå Missing positional encoding!')\")\n    print(\"else:\")\n    print(\"    print('‚úÖ Position encoding working!')\")\n\ndebug_missing_position()\n```\n\n## üéì Summary and Best Practices\n\nIn this notebook, we've explored how transformers understand position:\n\n### Key Concepts Mastered:\n\n1. **The Position Problem** - Why transformers need explicit positional information\n2. **Mathematical Intuition** - Why sine/cosine functions are perfect for position encoding\n3. **Addition vs Concatenation** - The critical design choice and why addition wins\n4. **Sinusoidal vs Learned** - Trade-offs between different approaches\n5. **Debugging Skills** - How to identify and fix common issues\n\n### Design Principles:\n\n- **Uniqueness**: Each position gets a unique signature\n- **Boundedness**: Values stay in reasonable ranges\n- **Smoothness**: Similar positions have similar encodings\n- **Efficiency**: Addition preserves dimensionality\n- **Flexibility**: Choose encoding type based on task requirements\n\n### Best Practices:\n\n‚úÖ **Use sinusoidal encoding** for unlimited sequence length  \n‚úÖ **Use learned embeddings** for better task-specific performance  \n‚úÖ **Balance magnitudes** between word and position embeddings  \n‚úÖ **Always add, never concatenate** for efficiency  \n‚úÖ **Test with scrambled sentences** to verify position encoding works  \n\n### Common Misconceptions Corrected:\n\n‚ùå \"Position encoding is just added noise\" ‚Üí ‚úÖ It's structured information that enables order understanding  \n‚ùå \"Concatenation is more explicit\" ‚Üí ‚úÖ Addition creates richer blended representations  \n‚ùå \"Learned embeddings are always better\" ‚Üí ‚úÖ Depends on task and sequence length requirements  \n‚ùå \"Position encoding works like human position perception\" ‚Üí ‚úÖ It's mathematical encoding optimized for transformers  \n\n### üî¨ Try This Yourself!\n\nBefore moving on, experiment with:\n1. What happens if you remove positional encoding entirely?\n2. How do different d_model sizes affect position discrimination?\n3. Can you visualize attention patterns with/without position encoding?\n4. What's the effect of different positional encoding magnitudes?\n\n**Next**: We'll explore how to train transformers and see these concepts in action!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}