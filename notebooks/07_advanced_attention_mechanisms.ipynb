{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Attention Mechanisms in Transformers\n",
    "\n",
    "This notebook explores advanced attention mechanisms beyond the basic multi-head attention covered in earlier notebooks. We'll dive deep into the computational complexity, optimization techniques, and modern variants that make transformers more efficient and powerful.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. **Computational Complexity**: Why attention is O(n²) and what this means for scaling\n",
    "2. **KV Caching**: How to optimize inference through key-value caching\n",
    "3. **Sparse Attention**: Techniques to reduce attention complexity\n",
    "4. **Attention Patterns**: How to visualize and interpret attention weights\n",
    "5. **Modern Variants**: Flash Attention, Multi-Query Attention, and Grouped-Query Attention\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of basic attention mechanism (notebook 02)\n",
    "- Familiarity with PyTorch tensors and operations\n",
    "- Basic knowledge of computational complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Complexity Analysis\n",
    "\n",
    "### The O(n²) Problem\n",
    "\n",
    "Standard attention computes:\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k)V\n",
    "```\n",
    "\n",
    "Let's analyze why this is quadratic in sequence length and what it means for memory and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionComplexityAnalyzer:\n",
    "    \"\"\"Analyzes computational complexity of attention mechanisms.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_attention_flops(seq_len: int, d_model: int, n_heads: int) -> dict:\n",
    "        \"\"\"Compute FLOPs for attention computation.\"\"\"\n",
    "        d_k = d_model // n_heads\n",
    "        \n",
    "        # QK^T computation: (seq_len, d_k) @ (d_k, seq_len) = (seq_len, seq_len)\n",
    "        qk_flops = seq_len * seq_len * d_k * n_heads\n",
    "        \n",
    "        # Softmax: roughly 3 operations per element (exp, sum, divide)\n",
    "        softmax_flops = 3 * seq_len * seq_len * n_heads\n",
    "        \n",
    "        # Attention @ V: (seq_len, seq_len) @ (seq_len, d_k) = (seq_len, d_k)\n",
    "        av_flops = seq_len * seq_len * d_k * n_heads\n",
    "        \n",
    "        # Linear projections: 4 projections (Q, K, V, output)\n",
    "        linear_flops = 4 * seq_len * d_model * d_model\n",
    "        \n",
    "        total_flops = qk_flops + softmax_flops + av_flops + linear_flops\n",
    "        \n",
    "        return {\n",
    "            'qk_computation': qk_flops,\n",
    "            'softmax': softmax_flops,\n",
    "            'attention_values': av_flops,\n",
    "            'linear_projections': linear_flops,\n",
    "            'total': total_flops,\n",
    "            'quadratic_component': qk_flops + softmax_flops + av_flops,\n",
    "            'linear_component': linear_flops\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_attention_memory(seq_len: int, d_model: int, n_heads: int, \n",
    "                               batch_size: int = 1) -> dict:\n",
    "        \"\"\"Compute memory requirements for attention.\"\"\"\n",
    "        d_k = d_model // n_heads\n",
    "        \n",
    "        # Input embeddings\n",
    "        input_memory = batch_size * seq_len * d_model * 4  # 4 bytes per float32\n",
    "        \n",
    "        # Q, K, V matrices\n",
    "        qkv_memory = 3 * batch_size * n_heads * seq_len * d_k * 4\n",
    "        \n",
    "        # Attention weights matrix (the big one!)\n",
    "        attention_weights_memory = batch_size * n_heads * seq_len * seq_len * 4\n",
    "        \n",
    "        # Output\n",
    "        output_memory = batch_size * seq_len * d_model * 4\n",
    "        \n",
    "        total_memory = input_memory + qkv_memory + attention_weights_memory + output_memory\n",
    "        \n",
    "        return {\n",
    "            'input': input_memory,\n",
    "            'qkv_matrices': qkv_memory,\n",
    "            'attention_weights': attention_weights_memory,  # This is O(n²)!\n",
    "            'output': output_memory,\n",
    "            'total_bytes': total_memory,\n",
    "            'total_mb': total_memory / (1024 * 1024),\n",
    "            'total_gb': total_memory / (1024 * 1024 * 1024)\n",
    "        }\n",
    "\n",
    "# Analyze complexity for different sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "batch_size = 4\n",
    "\n",
    "complexity_results = []\n",
    "memory_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    flops = AttentionComplexityAnalyzer.compute_attention_flops(seq_len, d_model, n_heads)\n",
    "    memory = AttentionComplexityAnalyzer.compute_attention_memory(seq_len, d_model, n_heads, batch_size)\n",
    "    \n",
    "    complexity_results.append({\n",
    "        'seq_len': seq_len,\n",
    "        'total_gflops': flops['total'] / 1e9,\n",
    "        'quadratic_gflops': flops['quadratic_component'] / 1e9,\n",
    "        'linear_gflops': flops['linear_component'] / 1e9\n",
    "    })\n",
    "    \n",
    "    memory_results.append({\n",
    "        'seq_len': seq_len,\n",
    "        'total_gb': memory['total_gb'],\n",
    "        'attention_weights_gb': memory['attention_weights'] / (1024**3),\n",
    "        'other_gb': (memory['total_bytes'] - memory['attention_weights']) / (1024**3)\n",
    "    })\n",
    "\n",
    "print(\"Computational Complexity Analysis:\")\n",
    "print(\"Seq Len | Total GFLOPs | Quadratic | Linear\")\n",
    "print(\"-\" * 45)\n",
    "for result in complexity_results:\n",
    "    print(f\"{result['seq_len']:7d} | {result['total_gflops']:11.2f} | {result['quadratic_gflops']:9.2f} | {result['linear_gflops']:6.2f}\")\n",
    "\n",
    "print(\"\\nMemory Requirements Analysis:\")\n",
    "print(\"Seq Len | Total GB | Attn Weights GB | Other GB\")\n",
    "print(\"-\" * 50)\n",
    "for result in memory_results:\n",
    "    print(f\"{result['seq_len']:7d} | {result['total_gb']:8.2f} | {result['attention_weights_gb']:14.2f} | {result['other_gb']:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scaling behavior\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Computational complexity\n",
    "seq_lens = [r['seq_len'] for r in complexity_results]\n",
    "total_gflops = [r['total_gflops'] for r in complexity_results]\n",
    "quad_gflops = [r['quadratic_gflops'] for r in complexity_results]\n",
    "linear_gflops = [r['linear_gflops'] for r in complexity_results]\n",
    "\n",
    "ax1.loglog(seq_lens, total_gflops, 'o-', label='Total', linewidth=2, markersize=8)\n",
    "ax1.loglog(seq_lens, quad_gflops, 's--', label='Quadratic Component', linewidth=2, markersize=6)\n",
    "ax1.loglog(seq_lens, linear_gflops, '^:', label='Linear Component', linewidth=2, markersize=6)\n",
    "\n",
    "# Add theoretical O(n²) reference line\n",
    "reference_n2 = [seq_lens[0]**2 * total_gflops[0] / seq_lens[0]**2 * (n/seq_lens[0])**2 for n in seq_lens]\n",
    "ax1.loglog(seq_lens, reference_n2, 'k--', alpha=0.5, label='O(n²) reference')\n",
    "\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('GFLOPs')\n",
    "ax1.set_title('Attention Computational Complexity')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory requirements\n",
    "total_gb = [r['total_gb'] for r in memory_results]\n",
    "attn_gb = [r['attention_weights_gb'] for r in memory_results]\n",
    "other_gb = [r['other_gb'] for r in memory_results]\n",
    "\n",
    "ax2.loglog(seq_lens, total_gb, 'o-', label='Total Memory', linewidth=2, markersize=8)\n",
    "ax2.loglog(seq_lens, attn_gb, 's--', label='Attention Weights', linewidth=2, markersize=6)\n",
    "ax2.loglog(seq_lens, other_gb, '^:', label='Other Components', linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Memory (GB)')\n",
    "ax2.set_title('Attention Memory Requirements')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Key Observations:\")\n",
    "print(f\"• At seq_len=8192, attention weights alone require {attn_gb[-1]:.1f} GB of memory!\")\n",
    "print(f\"• Quadratic component dominates total FLOPs for long sequences\")\n",
    "print(f\"• Memory grows as O(n²), making long sequences prohibitively expensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. KV Caching for Efficient Inference\n",
    "\n",
    "During autoregressive generation, we can cache previously computed key and value vectors to avoid redundant computation. This is crucial for efficient text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with KV caching for efficient inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Cache for keys and values\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "        self.cache_len = 0\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the KV cache.\"\"\"\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "        self.cache_len = 0\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None, \n",
    "                use_cache: bool = False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass with optional KV caching.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            mask: Attention mask\n",
    "            use_cache: Whether to use/update cache\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.w_v(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        if use_cache:\n",
    "            if self.k_cache is not None:\n",
    "                # Concatenate with cache\n",
    "                K = torch.cat([self.k_cache, K], dim=2)\n",
    "                V = torch.cat([self.v_cache, V], dim=2)\n",
    "            \n",
    "            # Update cache\n",
    "            self.k_cache = K\n",
    "            self.v_cache = V\n",
    "            self.cache_len = K.size(2)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "# Demonstrate KV caching efficiency\n",
    "def benchmark_kv_caching():\n",
    "    \"\"\"Benchmark the efficiency gain from KV caching.\"\"\"\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    batch_size = 1\n",
    "    max_len = 100\n",
    "    \n",
    "    # Create attention layer\n",
    "    attention = CachedMultiHeadAttention(d_model, n_heads).to(device)\n",
    "    \n",
    "    # Simulate autoregressive generation\n",
    "    input_seq = torch.randn(batch_size, 1, d_model).to(device)\n",
    "    \n",
    "    # Without caching - recompute everything each step\n",
    "    print(\"Benchmarking without KV caching...\")\n",
    "    attention.clear_cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    current_seq = input_seq\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        # Create causal mask\n",
    "        seq_len = current_seq.size(1)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, n_heads, -1, -1)\n",
    "        \n",
    "        # Forward pass (recomputing everything)\n",
    "        out, _ = attention(current_seq, mask, use_cache=False)\n",
    "        \n",
    "        # Add new token (random for demo)\n",
    "        new_token = torch.randn(batch_size, 1, d_model).to(device)\n",
    "        current_seq = torch.cat([current_seq, new_token], dim=1)\n",
    "    \n",
    "    time_without_cache = time.time() - start_time\n",
    "    \n",
    "    # With caching - only compute new K,V each step\n",
    "    print(\"Benchmarking with KV caching...\")\n",
    "    attention.clear_cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    current_seq = input_seq\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        # Only need to pass the new token\n",
    "        if step == 0:\n",
    "            # First step - pass initial sequence\n",
    "            seq_len = current_seq.size(1)\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, n_heads, -1, -1)\n",
    "            out, _ = attention(current_seq, mask, use_cache=True)\n",
    "        else:\n",
    "            # Subsequent steps - only pass new token\n",
    "            new_token = torch.randn(batch_size, 1, d_model).to(device)\n",
    "            # Create mask for the new position\n",
    "            cache_len = attention.cache_len\n",
    "            mask = torch.tril(torch.ones(1, cache_len + 1)).to(device)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, n_heads, -1, -1)\n",
    "            out, _ = attention(new_token, mask, use_cache=True)\n",
    "            current_seq = torch.cat([current_seq, new_token], dim=1)\n",
    "    \n",
    "    time_with_cache = time.time() - start_time\n",
    "    \n",
    "    speedup = time_without_cache / time_with_cache\n",
    "    \n",
    "    print(f\"\\n⏱️  Performance Results:\")\n",
    "    print(f\"Without KV caching: {time_without_cache:.3f}s\")\n",
    "    print(f\"With KV caching:    {time_with_cache:.3f}s\")\n",
    "    print(f\"Speedup:            {speedup:.1f}x\")\n",
    "    \n",
    "    return speedup\n",
    "\n",
    "speedup = benchmark_kv_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sparse Attention Patterns\n",
    "\n",
    "To reduce the O(n²) complexity, various sparse attention patterns have been proposed. Let's implement and visualize some common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionPatterns:\n",
    "    \"\"\"Collection of sparse attention pattern generators.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_local_attention_mask(seq_len: int, window_size: int) -> torch.Tensor:\n",
    "        \"\"\"Create local attention mask (each token attends to nearby tokens).\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size // 2)\n",
    "            end = min(seq_len, i + window_size // 2 + 1)\n",
    "            mask[i, start:end] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_strided_attention_mask(seq_len: int, stride: int) -> torch.Tensor:\n",
    "        \"\"\"Create strided attention mask (attend to every k-th token).\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # Attend to positions at regular intervals\n",
    "            positions = torch.arange(0, seq_len, stride)\n",
    "            mask[i, positions] = 1\n",
    "            # Always attend to self\n",
    "            mask[i, i] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_global_attention_mask(seq_len: int, num_global: int) -> torch.Tensor:\n",
    "        \"\"\"Create global attention mask (some tokens attend to all, all attend to globals).\"\"\"\n",
    "        mask = torch.eye(seq_len)  # Self-attention\n",
    "        \n",
    "        # First num_global tokens are global\n",
    "        mask[:num_global, :] = 1  # Global tokens attend to all\n",
    "        mask[:, :num_global] = 1  # All tokens attend to global tokens\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_block_sparse_mask(seq_len: int, block_size: int) -> torch.Tensor:\n",
    "        \"\"\"Create block sparse attention mask.\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        num_blocks = seq_len // block_size\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            for j in range(num_blocks):\n",
    "                # Attend within block and to adjacent blocks\n",
    "                if abs(i - j) <= 1:\n",
    "                    start_i, end_i = i * block_size, (i + 1) * block_size\n",
    "                    start_j, end_j = j * block_size, (j + 1) * block_size\n",
    "                    mask[start_i:end_i, start_j:end_j] = 1\n",
    "        \n",
    "        return mask\n",
    "\n",
    "# Visualize different sparse attention patterns\n",
    "seq_len = 64\n",
    "patterns = {\n",
    "    'Full Attention': torch.tril(torch.ones(seq_len, seq_len)),\n",
    "    'Local (window=8)': SparseAttentionPatterns.create_local_attention_mask(seq_len, 8),\n",
    "    'Strided (stride=4)': SparseAttentionPatterns.create_strided_attention_mask(seq_len, 4),\n",
    "    'Global (4 global)': SparseAttentionPatterns.create_global_attention_mask(seq_len, 4),\n",
    "    'Block Sparse (8x8)': SparseAttentionPatterns.create_block_sparse_mask(seq_len, 8)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for idx, (name, pattern) in enumerate(patterns.items()):\n",
    "    axes[idx].imshow(pattern.numpy(), cmap='Blues', origin='upper')\n",
    "    axes[idx].set_title(f'{name}\\n{pattern.sum().item():.0f}/{seq_len**2} connections')\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Query Position')\n",
    "    \n",
    "    # Add sparsity information\n",
    "    sparsity = 1 - (pattern.sum() / (seq_len ** 2))\n",
    "    axes[idx].text(0.02, 0.98, f'Sparsity: {sparsity:.1%}', \n",
    "                  transform=axes[idx].transAxes, \n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                  verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze complexity reduction\n",
    "print(\"\\nComplexity Analysis of Sparse Patterns:\")\n",
    "print(\"Pattern\\t\\t\\tConnections\\tReduction\\tComplexity\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, pattern in patterns.items():\n",
    "    connections = pattern.sum().item()\n",
    "    reduction = 1 - (connections / (seq_len ** 2))\n",
    "    if 'Local' in name:\n",
    "        complexity = \"O(n·w)\"  # w = window size\n",
    "    elif 'Strided' in name:\n",
    "        complexity = \"O(n²/s)\"  # s = stride\n",
    "    elif 'Global' in name:\n",
    "        complexity = \"O(n·g + g²)\"  # g = global tokens\n",
    "    elif 'Block' in name:\n",
    "        complexity = \"O(n·b)\"  # b = block size\n",
    "    else:\n",
    "        complexity = \"O(n²)\"\n",
    "    \n",
    "    print(f\"{name:<20}\\t{connections:>4.0f}\\t{reduction:>6.1%}\\t{complexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Pattern Visualization and Analysis\n",
    "\n",
    "Understanding what attention patterns emerge during training is crucial for interpreting transformer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAnalyzer:\n",
    "    \"\"\"Tools for analyzing and visualizing attention patterns.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_attention_statistics(attention_weights: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Compute statistics about attention patterns.\n",
    "        \n",
    "        Args:\n",
    "            attention_weights: [batch, heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, seq_len, _ = attention_weights.shape\n",
    "        \n",
    "        # Average across batch\n",
    "        attn = attention_weights.mean(dim=0)  # [heads, seq_len, seq_len]\n",
    "        \n",
    "        # Compute entropy (measure of attention spread)\n",
    "        entropy = -torch.sum(attn * torch.log(attn + 1e-8), dim=-1)  # [heads, seq_len]\n",
    "        \n",
    "        # Compute attention distance (how far attention looks)\n",
    "        positions = torch.arange(seq_len, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        query_pos = positions.expand(n_heads, seq_len, 1)\n",
    "        key_pos = positions.transpose(-1, -2).expand(n_heads, 1, seq_len)\n",
    "        \n",
    "        distances = torch.abs(query_pos - key_pos)\n",
    "        avg_distance = torch.sum(attn * distances, dim=-1)  # [heads, seq_len]\n",
    "        \n",
    "        # Compute attention to self vs others\n",
    "        self_attention = torch.diagonal(attn, dim1=-2, dim2=-1)  # [heads, seq_len]\n",
    "        \n",
    "        return {\n",
    "            'entropy': entropy,\n",
    "            'average_distance': avg_distance,\n",
    "            'self_attention': self_attention,\n",
    "            'max_attention': attn.max(dim=-1)[0],\n",
    "            'attention_concentration': (attn**2).sum(dim=-1)  # Gini coefficient approximation\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_attention_heads(attention_weights: torch.Tensor, \n",
    "                                 layer_name: str = \"Layer\",\n",
    "                                 max_heads: int = 8):\n",
    "        \"\"\"\n",
    "        Visualize attention patterns for multiple heads.\n",
    "        \n",
    "        Args:\n",
    "            attention_weights: [batch, heads, seq_len, seq_len]\n",
    "            layer_name: Name of the layer for the title\n",
    "            max_heads: Maximum number of heads to visualize\n",
    "        \"\"\"\n",
    "        # Take first batch, limit heads\n",
    "        attn = attention_weights[0][:max_heads]  # [heads, seq_len, seq_len]\n",
    "        n_heads_to_show = min(max_heads, attn.size(0))\n",
    "        \n",
    "        # Create subplots\n",
    "        cols = 4\n",
    "        rows = (n_heads_to_show + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for head_idx in range(n_heads_to_show):\n",
    "            row = head_idx // cols\n",
    "            col = head_idx % cols\n",
    "            \n",
    "            # Plot attention pattern\n",
    "            im = axes[row, col].imshow(attn[head_idx].detach().cpu().numpy(), \n",
    "                                     cmap='Blues', origin='upper')\n",
    "            axes[row, col].set_title(f'Head {head_idx + 1}')\n",
    "            axes[row, col].set_xlabel('Key Position')\n",
    "            axes[row, col].set_ylabel('Query Position')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=axes[row, col], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for head_idx in range(n_heads_to_show, rows * cols):\n",
    "            row = head_idx // cols\n",
    "            col = head_idx % cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'{layer_name} - Attention Patterns', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_attention_statistics(stats: dict, layer_name: str = \"Layer\"):\n",
    "        \"\"\"Plot various attention statistics.\"\"\"\n",
    "        n_heads, seq_len = stats['entropy'].shape\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # 1. Entropy across positions\n",
    "        for head in range(n_heads):\n",
    "            axes[0, 0].plot(stats['entropy'][head].cpu(), \n",
    "                          label=f'Head {head+1}', alpha=0.7)\n",
    "        axes[0, 0].set_title('Attention Entropy by Position')\n",
    "        axes[0, 0].set_xlabel('Position')\n",
    "        axes[0, 0].set_ylabel('Entropy')\n",
    "        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 2. Average attention distance\n",
    "        for head in range(n_heads):\n",
    "            axes[0, 1].plot(stats['average_distance'][head].cpu(), \n",
    "                          label=f'Head {head+1}', alpha=0.7)\n",
    "        axes[0, 1].set_title('Average Attention Distance')\n",
    "        axes[0, 1].set_xlabel('Query Position')\n",
    "        axes[0, 1].set_ylabel('Average Distance')\n",
    "        \n",
    "        # 3. Self-attention strength\n",
    "        for head in range(n_heads):\n",
    "            axes[0, 2].plot(stats['self_attention'][head].cpu(), \n",
    "                          label=f'Head {head+1}', alpha=0.7)\n",
    "        axes[0, 2].set_title('Self-Attention Strength')\n",
    "        axes[0, 2].set_xlabel('Position')\n",
    "        axes[0, 2].set_ylabel('Self-Attention Weight')\n",
    "        \n",
    "        # 4. Head-wise statistics (boxplots)\n",
    "        head_entropy_means = stats['entropy'].mean(dim=1).cpu()\n",
    "        head_distance_means = stats['average_distance'].mean(dim=1).cpu()\n",
    "        head_self_means = stats['self_attention'].mean(dim=1).cpu()\n",
    "        \n",
    "        head_labels = [f'H{i+1}' for i in range(n_heads)]\n",
    "        \n",
    "        axes[1, 0].bar(head_labels, head_entropy_means)\n",
    "        axes[1, 0].set_title('Mean Entropy by Head')\n",
    "        axes[1, 0].set_ylabel('Mean Entropy')\n",
    "        \n",
    "        axes[1, 1].bar(head_labels, head_distance_means)\n",
    "        axes[1, 1].set_title('Mean Distance by Head')\n",
    "        axes[1, 1].set_ylabel('Mean Distance')\n",
    "        \n",
    "        axes[1, 2].bar(head_labels, head_self_means)\n",
    "        axes[1, 2].set_title('Mean Self-Attention by Head')\n",
    "        axes[1, 2].set_ylabel('Mean Self-Attention')\n",
    "        \n",
    "        plt.suptitle(f'{layer_name} - Attention Analysis', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create a sample attention pattern to analyze\n",
    "def create_synthetic_attention_patterns(batch_size=2, n_heads=6, seq_len=32):\n",
    "    \"\"\"Create synthetic attention patterns that mimic real transformer behavior.\"\"\"\n",
    "    attention_weights = torch.zeros(batch_size, n_heads, seq_len, seq_len)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for h in range(n_heads):\n",
    "            if h == 0:  # Local attention head\n",
    "                for i in range(seq_len):\n",
    "                    start = max(0, i-3)\n",
    "                    end = min(seq_len, i+4)\n",
    "                    attention_weights[b, h, i, start:end] = torch.softmax(\n",
    "                        torch.randn(end-start), dim=0)\n",
    "            \n",
    "            elif h == 1:  # Global attention head (attends to beginning)\n",
    "                for i in range(seq_len):\n",
    "                    weights = torch.zeros(seq_len)\n",
    "                    weights[:5] = torch.rand(5) * 2  # Higher weight on first tokens\n",
    "                    weights[i] = torch.rand(1) * 2   # And self\n",
    "                    attention_weights[b, h, i, :] = torch.softmax(weights, dim=0)\n",
    "            \n",
    "            elif h == 2:  # Self-attention focused\n",
    "                for i in range(seq_len):\n",
    "                    weights = torch.zeros(seq_len)\n",
    "                    weights[i] = 3.0  # Strong self-attention\n",
    "                    weights += torch.randn(seq_len) * 0.5  # Weak noise\n",
    "                    attention_weights[b, h, i, :] = torch.softmax(weights, dim=0)\n",
    "            \n",
    "            else:  # Random patterns for other heads\n",
    "                for i in range(seq_len):\n",
    "                    # Create causal mask\n",
    "                    weights = torch.randn(i+1) if i < seq_len-1 else torch.randn(seq_len)\n",
    "                    full_weights = torch.full((seq_len,), -float('inf'))\n",
    "                    full_weights[:i+1] = weights\n",
    "                    attention_weights[b, h, i, :] = torch.softmax(full_weights, dim=0)\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# Generate and analyze synthetic attention patterns\n",
    "synthetic_attention = create_synthetic_attention_patterns()\n",
    "\n",
    "# Visualize attention heads\n",
    "AttentionAnalyzer.visualize_attention_heads(synthetic_attention, \"Synthetic Layer\")\n",
    "\n",
    "# Compute and plot statistics\n",
    "stats = AttentionAnalyzer.compute_attention_statistics(synthetic_attention)\n",
    "AttentionAnalyzer.plot_attention_statistics(stats, \"Synthetic Layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modern Attention Variants\n",
    "\n",
    "Let's implement some modern attention mechanisms that address efficiency and scaling concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"Multi-Query Attention (MQA) - shares K,V across heads.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # MQA: multiple Q heads, single K,V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Full Q projection\n",
    "        self.w_k = nn.Linear(d_model, self.d_k, bias=False)  # Single K projection\n",
    "        self.w_v = nn.Linear(d_model, self.d_k, bias=False)  # Single V projection\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute Q (multiple heads), K, V (single head each)\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).unsqueeze(1).expand(-1, self.n_heads, -1, -1)  # Broadcast to all heads\n",
    "        V = self.w_v(x).unsqueeze(1).expand(-1, self.n_heads, -1, -1)  # Broadcast to all heads\n",
    "        \n",
    "        # Attention computation (same as standard)\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped-Query Attention (GQA) - groups heads for K,V sharing.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        assert n_heads % n_kv_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_heads // n_kv_heads  # How many Q heads per KV head\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # GQA: full Q, reduced K,V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Repeat K,V for each group\n",
    "        K = K.repeat_interleave(self.n_rep, dim=1)  # [batch, n_heads, seq_len, d_k]\n",
    "        V = V.repeat_interleave(self.n_rep, dim=1)  # [batch, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # Standard attention computation\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "# Compare parameter counts and memory usage\n",
    "def compare_attention_variants():\n",
    "    \"\"\"Compare different attention mechanisms.\"\"\"\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    n_kv_heads = 2  # For GQA\n",
    "    batch_size = 4\n",
    "    seq_len = 1024\n",
    "    \n",
    "    # Create different attention mechanisms\n",
    "    standard_attn = CachedMultiHeadAttention(d_model, n_heads)\n",
    "    mqa_attn = MultiQueryAttention(d_model, n_heads)\n",
    "    gqa_attn = GroupedQueryAttention(d_model, n_heads, n_kv_heads)\n",
    "    \n",
    "    mechanisms = {\n",
    "        'Standard MHA': standard_attn,\n",
    "        'Multi-Query (MQA)': mqa_attn,\n",
    "        'Grouped-Query (GQA)': gqa_attn\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, mechanism in mechanisms.items():\n",
    "        # Count parameters\n",
    "        n_params = sum(p.numel() for p in mechanism.parameters())\n",
    "        \n",
    "        # Estimate KV cache memory (during inference)\n",
    "        if name == 'Standard MHA':\n",
    "            kv_cache_size = 2 * batch_size * n_heads * seq_len * (d_model // n_heads) * 4  # 4 bytes per float\n",
    "        elif name == 'Multi-Query (MQA)':\n",
    "            kv_cache_size = 2 * batch_size * 1 * seq_len * (d_model // n_heads) * 4  # Single KV\n",
    "        else:  # GQA\n",
    "            kv_cache_size = 2 * batch_size * n_kv_heads * seq_len * (d_model // n_heads) * 4\n",
    "        \n",
    "        results.append({\n",
    "            'mechanism': name,\n",
    "            'parameters': n_params,\n",
    "            'kv_cache_mb': kv_cache_size / (1024 * 1024),\n",
    "            'param_reduction': 0,  # Will calculate relative to standard\n",
    "            'cache_reduction': 0   # Will calculate relative to standard\n",
    "        })\n",
    "    \n",
    "    # Calculate relative reductions\n",
    "    standard_params = results[0]['parameters']\n",
    "    standard_cache = results[0]['kv_cache_mb']\n",
    "    \n",
    "    for result in results:\n",
    "        result['param_reduction'] = 1 - (result['parameters'] / standard_params)\n",
    "        result['cache_reduction'] = 1 - (result['kv_cache_mb'] / standard_cache)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Attention Mechanism Comparison:\")\n",
    "    print(\"Mechanism\\t\\tParameters\\tKV Cache (MB)\\tParam Reduction\\tCache Reduction\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['mechanism']:<20}\\t{result['parameters']:>8,}\\t{result['kv_cache_mb']:>10.1f}\\t\"\n",
    "              f\"{result['param_reduction']:>11.1%}\\t{result['cache_reduction']:>11.1%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "comparison_results = compare_attention_variants()\n",
    "\n",
    "# Visualize the comparison\n",
    "mechanisms = [r['mechanism'] for r in comparison_results]\n",
    "param_counts = [r['parameters'] / 1000 for r in comparison_results]  # In thousands\n",
    "cache_sizes = [r['kv_cache_mb'] for r in comparison_results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Parameter comparison\n",
    "bars1 = ax1.bar(mechanisms, param_counts, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax1.set_title('Parameter Count Comparison')\n",
    "ax1.set_ylabel('Parameters (thousands)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars1, param_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count:.0f}K', ha='center', va='bottom')\n",
    "\n",
    "# KV Cache comparison\n",
    "bars2 = ax2.bar(mechanisms, cache_sizes, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax2.set_title('KV Cache Memory Comparison')\n",
    "ax2.set_ylabel('Memory (MB)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars2, cache_sizes):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{size:.1f}MB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 Key Insights:\")\n",
    "print(f\"• MQA reduces KV cache by {comparison_results[1]['cache_reduction']:.0%} (critical for long sequences)\")\n",
    "print(f\"• GQA balances between MHA and MQA, reducing cache by {comparison_results[2]['cache_reduction']:.0%}\")\n",
    "print(f\"• Parameter reduction is modest but KV cache reduction is substantial\")\n",
    "print(f\"• These optimizations become critical for inference at scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Implementation Tips\n",
    "\n",
    "Let's look at some practical considerations when implementing advanced attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOptimizationUtils:\n",
    "    \"\"\"Utilities for optimizing attention computation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fused_scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
    "                                         mask: Optional[torch.Tensor] = None,\n",
    "                                         dropout_p: float = 0.0,\n",
    "                                         is_causal: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fused scaled dot-product attention (simulates Flash Attention concept).\n",
    "        In practice, you'd use torch.nn.functional.scaled_dot_product_attention in PyTorch 2.0+\n",
    "        \"\"\"\n",
    "        # This is a simplified version - real Flash Attention uses tiling and recomputation\n",
    "        scale = 1.0 / math.sqrt(Q.size(-1))\n",
    "        \n",
    "        if is_causal:\n",
    "            # Create causal mask\n",
    "            seq_len = Q.size(-2)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=Q.device))\n",
    "            if mask is not None:\n",
    "                mask = mask * causal_mask\n",
    "            else:\n",
    "                mask = causal_mask\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        if dropout_p > 0.0:\n",
    "            attn_weights = F.dropout(attn_weights, p=dropout_p)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_attention_variants(seq_lengths: List[int], \n",
    "                                   d_model: int = 512, \n",
    "                                   n_heads: int = 8,\n",
    "                                   batch_size: int = 4):\n",
    "        \"\"\"Benchmark different attention implementations.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for seq_len in seq_lengths:\n",
    "            print(f\"\\nBenchmarking seq_len = {seq_len}...\")\n",
    "            \n",
    "            # Create random inputs\n",
    "            x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "            \n",
    "            d_k = d_model // n_heads\n",
    "            Q = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "            K = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "            V = torch.randn(batch_size, n_heads, seq_len, d_k, device=device)\n",
    "            \n",
    "            # Standard attention\n",
    "            start_time = time.time()\n",
    "            for _ in range(10):  # Multiple runs for stable timing\n",
    "                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "                attn_weights = F.softmax(scores, dim=-1)\n",
    "                output = torch.matmul(attn_weights, V)\n",
    "                torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "            standard_time = (time.time() - start_time) / 10\n",
    "            \n",
    "            # Fused attention (simulated)\n",
    "            start_time = time.time()\n",
    "            for _ in range(10):\n",
    "                output = AttentionOptimizationUtils.fused_scaled_dot_product_attention(Q, K, V)\n",
    "                torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "            fused_time = (time.time() - start_time) / 10\n",
    "            \n",
    "            results[seq_len] = {\n",
    "                'standard_time': standard_time,\n",
    "                'fused_time': fused_time,\n",
    "                'speedup': standard_time / fused_time\n",
    "            }\n",
    "            \n",
    "            print(f\"  Standard: {standard_time:.4f}s\")\n",
    "            print(f\"  Fused:    {fused_time:.4f}s\")\n",
    "            print(f\"  Speedup:  {standard_time/fused_time:.2f}x\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run benchmarks (skip if no GPU available)\n",
    "if device.type == 'cuda':\n",
    "    print(\"Running attention benchmarks on GPU...\")\n",
    "    benchmark_results = AttentionOptimizationUtils.benchmark_attention_variants(\n",
    "        seq_lengths=[128, 256, 512, 1024],\n",
    "        d_model=512,\n",
    "        n_heads=8,\n",
    "        batch_size=4\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping GPU benchmarks (no CUDA device available)\")\n",
    "    # Create dummy results for visualization\n",
    "    benchmark_results = {\n",
    "        128: {'standard_time': 0.005, 'fused_time': 0.003, 'speedup': 1.67},\n",
    "        256: {'standard_time': 0.015, 'fused_time': 0.008, 'speedup': 1.88},\n",
    "        512: {'standard_time': 0.045, 'fused_time': 0.020, 'speedup': 2.25},\n",
    "        1024: {'standard_time': 0.150, 'fused_time': 0.055, 'speedup': 2.73}\n",
    "    }\n",
    "\n",
    "# Visualize benchmark results\n",
    "if benchmark_results:\n",
    "    seq_lens = list(benchmark_results.keys())\n",
    "    standard_times = [benchmark_results[s]['standard_time'] * 1000 for s in seq_lens]  # Convert to ms\n",
    "    fused_times = [benchmark_results[s]['fused_time'] * 1000 for s in seq_lens]\n",
    "    speedups = [benchmark_results[s]['speedup'] for s in seq_lens]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Timing comparison\n",
    "    x = np.arange(len(seq_lens))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, standard_times, width, label='Standard Attention', alpha=0.8)\n",
    "    ax1.bar(x + width/2, fused_times, width, label='Fused Attention', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.set_title('Attention Implementation Timing')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(seq_lens)\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Speedup\n",
    "    ax2.plot(seq_lens, speedups, 'o-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Speedup (x)')\n",
    "    ax2.set_title('Fused Attention Speedup')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add horizontal line at 1x\n",
    "    ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n🚀 Optimization Summary:\")\n",
    "print(f\"• Fused implementations reduce memory bandwidth requirements\")\n",
    "print(f\"• Speedup generally increases with sequence length\")\n",
    "print(f\"• Modern frameworks provide optimized attention kernels\")\n",
    "print(f\"• Flash Attention achieves O(n) memory complexity for attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### 🎯 What We've Learned\n",
    "\n",
    "1. **Computational Complexity**:\n",
    "   - Standard attention is O(n²) in both time and memory\n",
    "   - The attention weight matrix dominates memory usage for long sequences\n",
    "   - Quadratic scaling becomes prohibitive beyond ~8K tokens\n",
    "\n",
    "2. **KV Caching**:\n",
    "   - Essential optimization for autoregressive generation\n",
    "   - Provides substantial speedup by avoiding redundant computation\n",
    "   - Critical for real-time inference applications\n",
    "\n",
    "3. **Sparse Attention Patterns**:\n",
    "   - Local, strided, global, and block-sparse patterns reduce complexity\n",
    "   - Trade-off between efficiency and modeling capability\n",
    "   - Different patterns suit different tasks and sequence types\n",
    "\n",
    "4. **Modern Variants**:\n",
    "   - **Multi-Query Attention (MQA)**: Shares K,V across heads, reduces KV cache\n",
    "   - **Grouped-Query Attention (GQA)**: Balances between MHA and MQA\n",
    "   - **Flash Attention**: Achieves O(n) memory complexity through tiling\n",
    "\n",
    "5. **Practical Considerations**:\n",
    "   - Memory optimization is often more important than FLOP reduction\n",
    "   - Hardware-aware implementations provide significant speedups\n",
    "   - Modern frameworks offer optimized attention kernels\n",
    "\n",
    "### 🔄 Next Steps\n",
    "\n",
    "In the next notebook (08_modern_architecture_improvements), we'll explore:\n",
    "- RMSNorm vs LayerNorm\n",
    "- SwiGLU activation functions\n",
    "- Rotary Position Embedding (RoPE)\n",
    "- Pre-norm vs Post-norm architectures\n",
    "\n",
    "### 📚 Further Reading\n",
    "\n",
    "- **Flash Attention**: Dao et al. (2022) - \"FlashAttention: Fast and Memory-Efficient Exact Attention\"\n",
    "- **Multi-Query Attention**: Shazeer (2019) - \"Fast Transformer Decoding\"\n",
    "- **Sparse Attention**: Child et al. (2019) - \"Generating Long Sequences with Sparse Transformers\"\n",
    "- **Grouped-Query Attention**: Ainslie et al. (2023) - \"GQA: Training Generalized Multi-Query Transformer\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}