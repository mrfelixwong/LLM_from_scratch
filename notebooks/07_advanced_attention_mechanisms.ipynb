{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Attention Mechanisms\n\nThis notebook explores production-ready attention optimizations that make transformers efficient at scale: KV caching, sparse attention patterns, and modern variants like Multi-Query Attention."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom typing import Optional, Tuple, List\nimport time\nfrom dataclasses import dataclass\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Basic Multi-Head Attention for comparison\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n        \n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.w_o(attn_output)"
  },
  {
   "cell_type": "markdown",
   "source": "## KV Caching for Efficient Inference\n\nDuring autoregressive generation, we recompute K and V for all previous tokens at every step. KV caching stores these tensors and appends new ones, providing significant speedup.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class CachedMultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n        self.kv_cache = {}\n    \n    def forward(self, query, key, value, mask=None, use_cache=False, cache_key=\"default\"):\n        batch_size, seq_len, _ = query.shape\n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        if use_cache and cache_key in self.kv_cache:\n            cached_K, cached_V = self.kv_cache[cache_key]\n            new_K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            new_V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            K = torch.cat([cached_K, new_K], dim=2)\n            V = torch.cat([cached_V, new_V], dim=2)\n            self.kv_cache[cache_key] = (K, V)\n        else:\n            K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            if use_cache:\n                self.kv_cache[cache_key] = (K, V)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.w_o(attn_output)\n    \n    def clear_cache(self):\n        self.kv_cache.clear()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## KV Caching Performance Test\n\nLet's compare KV cached attention vs standard attention during autoregressive generation to see the speedup."
  },
  {
   "cell_type": "code",
   "source": "d_model, n_heads = 256, 8\nregular_attn = MultiHeadAttention(d_model, n_heads).to(device)\ncached_attn = CachedMultiHeadAttention(d_model, n_heads).to(device)\n\ndef simulate_autoregressive_generation(attention_module, use_cache=False, num_steps=20):\n    times = []\n    seq = torch.randn(1, 1, d_model).to(device)\n    \n    for step in range(num_steps):\n        start_time = time.time()\n        \n        if use_cache:\n            if step == 0:\n                output = attention_module(seq, seq, seq, use_cache=True, cache_key=\"gen\")\n            else:\n                new_token = torch.randn(1, 1, d_model).to(device)\n                output = attention_module(new_token, new_token, new_token, use_cache=True, cache_key=\"gen\")\n        else:\n            if step == 0:\n                current_seq = seq\n            else:\n                new_token = torch.randn(1, 1, d_model).to(device)\n                current_seq = torch.cat([current_seq, new_token], dim=1)\n            output = attention_module(current_seq, current_seq, current_seq)\n        \n        times.append((time.time() - start_time) * 1000)\n    return times\n\nregular_times = simulate_autoregressive_generation(regular_attn, use_cache=False, num_steps=10)\ncached_attn.clear_cache()\ncached_times = simulate_autoregressive_generation(cached_attn, use_cache=True, num_steps=10)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nsteps = list(range(1, len(regular_times) + 1))\nax1.plot(steps, regular_times, 'ro-', label='Regular Attention', linewidth=2, markersize=6)\nax1.plot(steps, cached_times, 'bo-', label='KV Cached Attention', linewidth=2, markersize=6)\nax1.set_xlabel('Generation Step')\nax1.set_ylabel('Time (ms)')\nax1.set_title('Per-Step Inference Time')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\ncumulative_regular = np.cumsum(regular_times)\ncumulative_cached = np.cumsum(cached_times)\nax2.plot(steps, cumulative_regular, 'ro-', label='Regular Attention', linewidth=2, markersize=6)\nax2.plot(steps, cumulative_cached, 'bo-', label='KV Cached Attention', linewidth=2, markersize=6)\nax2.set_xlabel('Generation Step')\nax2.set_ylabel('Cumulative Time (ms)')\nax2.set_title('Total Generation Time')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nspeedup = sum(regular_times) / sum(cached_times)\nprint(f\"KV caching provides {speedup:.1f}x speedup\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Sparse Attention Patterns\n\nStandard attention has O(nÂ²) complexity. Sparse patterns reduce this by having tokens attend to only a subset of positions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SparseAttentionPatterns:\n    @staticmethod\n    def create_local_attention_mask(seq_len: int, window_size: int) -> torch.Tensor:\n        mask = torch.zeros(seq_len, seq_len)\n        for i in range(seq_len):\n            start = max(0, i - window_size // 2)\n            end = min(seq_len, i + window_size // 2 + 1)\n            mask[i, start:end] = 1\n        return mask\n    \n    @staticmethod\n    def create_strided_attention_mask(seq_len: int, stride: int) -> torch.Tensor:\n        mask = torch.zeros(seq_len, seq_len)\n        for i in range(seq_len):\n            positions = torch.arange(0, seq_len, stride)\n            mask[i, positions] = 1\n            mask[i, i] = 1\n        return mask\n    \n    @staticmethod\n    def create_global_attention_mask(seq_len: int, num_global: int) -> torch.Tensor:\n        mask = torch.eye(seq_len)\n        mask[:num_global, :] = 1\n        mask[:, :num_global] = 1\n        return mask",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sparse Pattern Visualization\n\nLet's visualize different sparse attention patterns and analyze their complexity reduction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "seq_len = 64\npatterns = {\n    'Full': torch.tril(torch.ones(seq_len, seq_len)),\n    'Local': SparseAttentionPatterns.create_local_attention_mask(seq_len, 8),\n    'Strided': SparseAttentionPatterns.create_strided_attention_mask(seq_len, 4),\n    'Global': SparseAttentionPatterns.create_global_attention_mask(seq_len, 4)\n}\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor idx, (name, pattern) in enumerate(patterns.items()):\n    axes[idx].imshow(pattern.numpy(), cmap='Blues')\n    sparsity = 1 - (pattern.sum() / (seq_len ** 2))\n    axes[idx].set_title(f'{name}\\nSparsity: {sparsity:.1%}')\n    axes[idx].set_xlabel('Key Position')\n    if idx == 0:\n        axes[idx].set_ylabel('Query Position')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Complexity Analysis:\")\nprint(\"Pattern\\t\\tConnections\\tReduction\")\nfor name, pattern in patterns.items():\n    connections = pattern.sum().item()\n    reduction = 1 - (connections / (seq_len ** 2))\n    print(f\"{name}\\t\\t{connections:.0f}\\t\\t{reduction:.1%}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Modern Attention Variants\n\nMulti-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce parameters by sharing key/value heads across query heads."
  },
  {
   "cell_type": "code",
   "source": "class MultiQueryAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, self.d_k, bias=False)\n        self.w_v = nn.Linear(d_model, self.d_k, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n        \n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, 1, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, 1, self.d_k).transpose(1, 2)\n        \n        K = K.expand(-1, self.n_heads, -1, -1)\n        V = V.expand(-1, self.n_heads, -1, -1)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.w_o(attn_output)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        assert n_heads % n_kv_heads == 0\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_kv_heads = n_kv_heads\n        self.d_k = d_model // n_heads\n        self.group_size = n_heads // n_kv_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n        self.w_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n        \n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n        \n        K = K.repeat_interleave(self.group_size, dim=1)\n        V = V.repeat_interleave(self.group_size, dim=1)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.w_o(attn_output)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Attention Variants Comparison\n\nLet's compare parameter counts and performance of different attention mechanisms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "d_model, seq_len = 256, 32\nx = torch.randn(1, seq_len, d_model).to(device)\n\nmha = MultiHeadAttention(d_model, n_heads=8).to(device)\nmqa = MultiQueryAttention(d_model, n_heads=8).to(device)\ngqa = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=2).to(device)\n\nmha_params = sum(p.numel() for p in mha.parameters())\nmqa_params = sum(p.numel() for p in mqa.parameters())\ngqa_params = sum(p.numel() for p in gqa.parameters())\n\nprint(\"Parameter Comparison:\")\nprint(f\"Multi-Head Attention:    {mha_params:,} params\")\nprint(f\"Multi-Query Attention:   {mqa_params:,} params ({mha_params/mqa_params:.1f}x reduction)\")\nprint(f\"Grouped-Query Attention: {gqa_params:,} params ({mha_params/gqa_params:.1f}x reduction)\")\n\nmha_out = mha(x, x, x)\nmqa_out = mqa(x, x, x)\ngqa_out = gqa(x, x, x)\n\nprint(f\"\\nOutput shapes (all should be identical):\")\nprint(f\"MHA: {mha_out.shape}\")\nprint(f\"MQA: {mqa_out.shape}\")\nprint(f\"GQA: {gqa_out.shape}\")\n\nmechanisms = ['MHA', 'MQA', 'GQA']\nparameters = [mha_params, mqa_params, gqa_params]\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(mechanisms, parameters, color=['blue', 'orange', 'green'], alpha=0.7)\nplt.ylabel('Number of Parameters')\nplt.title('Parameter Count Comparison')\nplt.grid(True, alpha=0.3)\n\nfor i, v in enumerate(parameters):\n    plt.text(i, v + max(parameters) * 0.01, f'{v:,}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've explored three critical attention optimizations:\n\n- **KV Caching**: Speeds up autoregressive generation by 2-10x\n- **Sparse Attention**: Reduces O(nÂ²) complexity for long sequences  \n- **MQA/GQA**: Reduces parameters by 2-4x while maintaining quality\n\nThese techniques are essential for production transformer deployment in modern AI systems.",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Production-Ready Attention Optimizations ð¯\n\nCongratulations! You've mastered the essential attention optimizations that make transformers practical at scale.\n\n### ð§ What You've Learned\n\n**1. KV Caching** - The inference game-changer\n- **Problem**: Recomputing K,V for all previous tokens is wasteful\n- **Solution**: Cache K,V tensors, append new ones for new tokens\n- **Result**: ~2-10x speedup for autoregressive generation\n- **Usage**: Essential for all chatbots and language model inference\n\n**2. Sparse Attention** - Breaking the O(nÂ²) barrier\n- **Local Attention**: Each token attends to nearby tokens (O(nÂ·w))\n- **Strided Attention**: Attend to every k-th token (O(nÂ²/s))\n- **Global Attention**: Some tokens attend to all, all attend to globals\n- **Block Sparse**: Attend within blocks and to adjacent blocks\n- **Result**: Enable processing of 100K+ token sequences\n\n**3. Modern Variants** - Efficiency without quality loss\n- **Multi-Query Attention (MQA)**: 1 K,V head shared across all Q heads\n- **Grouped-Query Attention (GQA)**: Groups of Q heads share K,V heads\n- **Result**: 2-4x parameter reduction, faster inference, smaller KV cache\n\n### ð Real-World Impact\n\nThese aren't academic exercises - they're the backbone of modern AI:\n\n- **ChatGPT & GPT-4**: Use sophisticated caching and attention optimizations\n- **LLaMA-2**: Uses Grouped-Query Attention for efficiency\n- **PaLM & T5**: Pioneered Multi-Query Attention\n- **Longformer & BigBird**: Use sparse attention for long documents\n\n### ð Performance Benefits\n\nFrom our demonstrations:\n- **KV Caching**: Up to 10x faster autoregressive generation\n- **Sparse Attention**: 80-95% memory reduction for long sequences\n- **MQA/GQA**: 2-4x fewer parameters with minimal quality loss\n\n### ð¯ When to Use Each Technique\n\n**KV Caching**: \n- â Always use for autoregressive generation\n- â Text generation, chatbots, completion tasks\n- â Not needed for encoder-only models\n\n**Sparse Attention**:\n- â Long sequences (>8K tokens)\n- â Document processing, code analysis\n- â Short sequences where full attention is affordable\n\n**MQA/GQA**:\n- â Large-scale inference where memory matters\n- â Production deployments with cost constraints\n- â When you need to balance quality and efficiency\n\n### ð Next Steps\n\nYou now understand how to make attention mechanisms production-ready! These optimizations bridge the gap between research models and real-world applications.\n\n**Key Takeaway**: The best optimizations maintain model quality while dramatically improving efficiency. That's why these techniques are universally adopted in modern transformers.\n\nReady to explore complete model architectures and training! ðï¸"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}