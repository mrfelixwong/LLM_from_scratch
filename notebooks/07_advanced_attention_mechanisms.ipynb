{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Attention Mechanisms: Practical Optimizations\n\nIn the basic attention notebook, we learned the core mechanism. Now let's explore practical improvements that make transformers faster and more efficient in real applications.\n\n## What You'll Learn\n\n1. **KV Caching** - Speed up inference by caching key-value pairs\n2. **Sparse Attention** - Reduce complexity with smart attention patterns\n3. **Modern Variants** - Multi-Query and Grouped-Query Attention\n\nThese optimizations are used in production systems to make transformers practical at scale!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom typing import Optional, Tuple, List\nimport time\nfrom dataclasses import dataclass\n\n# Import our basic attention mechanism\nfrom src.model.attention import MultiHeadAttention\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Configure plotting\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(\"Environment setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. KV Caching for Efficient Inference\n\nDuring autoregressive generation, we can cache previously computed key and value vectors to avoid redundant computation. This is crucial for efficient text generation.\n\n**The Problem**: In normal autoregressive generation, we recompute K and V for all previous tokens at every step. This is wasteful!\n\n**The Solution**: Cache the K and V tensors and just append new ones for new tokens."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Sparse Attention Patterns\n\nTo reduce the O(nÂ²) complexity, various sparse attention patterns have been proposed. Let's implement and visualize some common patterns.\n\n**Why Sparse Attention?**\n- Standard attention is O(nÂ²) in memory and computation\n- Becomes prohibitive for long sequences (>8K tokens)\n- Many attention weights are close to zero anyway\n- Smart sparsity patterns can maintain model quality"
  },
  {
   "cell_type": "code",
   "source": "class CachedMultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention with KV caching for faster inference.\"\"\"\n    \n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        \n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Linear projections\n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        # Cache for key and value tensors\n        self.kv_cache = {}\n    \n    def forward(self, query, key, value, mask=None, use_cache=False, cache_key=\"default\"):\n        batch_size, seq_len, _ = query.shape\n        \n        # Linear projections\n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        if use_cache and cache_key in self.kv_cache:\n            # Use cached K, V and append new ones\n            cached_K, cached_V = self.kv_cache[cache_key]\n            \n            new_K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            new_V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            \n            K = torch.cat([cached_K, new_K], dim=2)  # Concatenate along sequence dimension\n            V = torch.cat([cached_V, new_V], dim=2)\n            \n            # Update cache\n            self.kv_cache[cache_key] = (K, V)\n        else:\n            # Fresh computation\n            K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n            \n            if use_cache:\n                self.kv_cache[cache_key] = (K, V)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        # Reshape and project\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        output = self.w_o(attn_output)\n        \n        return output\n    \n    def clear_cache(self):\n        \"\"\"Clear the KV cache.\"\"\"\n        self.kv_cache.clear()\n\n# Demonstrate KV caching benefits\nprint(\"ðŸš€ KV CACHING DEMONSTRATION\")\nprint(\"=\" * 40)\n\nd_model, n_heads = 256, 8\ncached_attn = CachedMultiHeadAttention(d_model, n_heads)\n\n# Simulate autoregressive generation\nseq_lens = [1, 2, 3, 4, 5]  # Growing sequence lengths\nx = torch.randn(1, 1, d_model)  # Start with one token\n\nprint(\"Simulating autoregressive generation:\")\nfor i, seq_len in enumerate(seq_lens):\n    if i == 0:\n        # First step - no cache\n        output = cached_attn(x, x, x, use_cache=True, cache_key=\"gen\")\n        print(f\"Step {i+1}: Added token, output shape: {output.shape}\")\n    else:\n        # Subsequent steps - use cache\n        new_token = torch.randn(1, 1, d_model)\n        output = cached_attn(new_token, new_token, new_token, use_cache=True, cache_key=\"gen\")\n        print(f\"Step {i+1}: Added token, output shape: {output.shape}\")\n\nprint(\"\\nâœ… KV caching reduces computation in autoregressive generation!\")\nprint(\"ðŸ’¡ Instead of recomputing all K,V pairs, we reuse cached ones\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionPatterns:\n",
    "    \"\"\"Collection of sparse attention pattern generators.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_local_attention_mask(seq_len: int, window_size: int) -> torch.Tensor:\n",
    "        \"\"\"Create local attention mask (each token attends to nearby tokens).\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size // 2)\n",
    "            end = min(seq_len, i + window_size // 2 + 1)\n",
    "            mask[i, start:end] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_strided_attention_mask(seq_len: int, stride: int) -> torch.Tensor:\n",
    "        \"\"\"Create strided attention mask (attend to every k-th token).\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # Attend to positions at regular intervals\n",
    "            positions = torch.arange(0, seq_len, stride)\n",
    "            mask[i, positions] = 1\n",
    "            # Always attend to self\n",
    "            mask[i, i] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_global_attention_mask(seq_len: int, num_global: int) -> torch.Tensor:\n",
    "        \"\"\"Create global attention mask (some tokens attend to all, all attend to globals).\"\"\"\n",
    "        mask = torch.eye(seq_len)  # Self-attention\n",
    "        \n",
    "        # First num_global tokens are global\n",
    "        mask[:num_global, :] = 1  # Global tokens attend to all\n",
    "        mask[:, :num_global] = 1  # All tokens attend to global tokens\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_block_sparse_mask(seq_len: int, block_size: int) -> torch.Tensor:\n",
    "        \"\"\"Create block sparse attention mask.\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        num_blocks = seq_len // block_size\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            for j in range(num_blocks):\n",
    "                # Attend within block and to adjacent blocks\n",
    "                if abs(i - j) <= 1:\n",
    "                    start_i, end_i = i * block_size, (i + 1) * block_size\n",
    "                    start_j, end_j = j * block_size, (j + 1) * block_size\n",
    "                    mask[start_i:end_i, start_j:end_j] = 1\n",
    "        \n",
    "        return mask\n",
    "\n",
    "# Visualize different sparse attention patterns\n",
    "seq_len = 64\n",
    "patterns = {\n",
    "    'Full Attention': torch.tril(torch.ones(seq_len, seq_len)),\n",
    "    'Local (window=8)': SparseAttentionPatterns.create_local_attention_mask(seq_len, 8),\n",
    "    'Strided (stride=4)': SparseAttentionPatterns.create_strided_attention_mask(seq_len, 4),\n",
    "    'Global (4 global)': SparseAttentionPatterns.create_global_attention_mask(seq_len, 4),\n",
    "    'Block Sparse (8x8)': SparseAttentionPatterns.create_block_sparse_mask(seq_len, 8)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for idx, (name, pattern) in enumerate(patterns.items()):\n",
    "    axes[idx].imshow(pattern.numpy(), cmap='Blues', origin='upper')\n",
    "    axes[idx].set_title(f'{name}\\n{pattern.sum().item():.0f}/{seq_len**2} connections')\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Query Position')\n",
    "    \n",
    "    # Add sparsity information\n",
    "    sparsity = 1 - (pattern.sum() / (seq_len ** 2))\n",
    "    axes[idx].text(0.02, 0.98, f'Sparsity: {sparsity:.1%}', \n",
    "                  transform=axes[idx].transAxes, \n",
    "                  bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                  verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze complexity reduction\n",
    "print(\"\\nComplexity Analysis of Sparse Patterns:\")\n",
    "print(\"Pattern\\t\\t\\tConnections\\tReduction\\tComplexity\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, pattern in patterns.items():\n",
    "    connections = pattern.sum().item()\n",
    "    reduction = 1 - (connections / (seq_len ** 2))\n",
    "    if 'Local' in name:\n",
    "        complexity = \"O(nÂ·w)\"  # w = window size\n",
    "    elif 'Strided' in name:\n",
    "        complexity = \"O(nÂ²/s)\"  # s = stride\n",
    "    elif 'Global' in name:\n",
    "        complexity = \"O(nÂ·g + gÂ²)\"  # g = global tokens\n",
    "    elif 'Block' in name:\n",
    "        complexity = \"O(nÂ·b)\"  # b = block size\n",
    "    else:\n",
    "        complexity = \"O(nÂ²)\"\n",
    "    \n",
    "    print(f\"{name:<20}\\t{connections:>4.0f}\\t{reduction:>6.1%}\\t{complexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Modern Attention Variants\n\nLet's implement some modern attention mechanisms that address efficiency and scaling concerns.\n\n**Key Ideas:**\n- **Multi-Query Attention (MQA)**: Share K,V across all heads\n- **Grouped-Query Attention (GQA)**: Share K,V within groups of heads\n- Both reduce KV cache size and improve inference speed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "class MultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention: One key/value head, multiple query heads.\"\"\"\n    \n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        \n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Multiple query heads, single key/value head\n        self.w_q = nn.Linear(d_model, d_model, bias=False)  # n_heads query heads\n        self.w_k = nn.Linear(d_model, self.d_k, bias=False)  # 1 key head\n        self.w_v = nn.Linear(d_model, self.d_k, bias=False)  # 1 value head\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n        \n        # Multiple query heads\n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        # Single key and value heads (broadcast to all query heads)\n        K = self.w_k(key).view(batch_size, seq_len, 1, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, 1, self.d_k).transpose(1, 2)\n        \n        # Expand K, V to match Q heads\n        K = K.expand(-1, self.n_heads, -1, -1)\n        V = V.expand(-1, self.n_heads, -1, -1)\n        \n        # Standard scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        # Reshape and project\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        output = self.w_o(attn_output)\n        \n        return output\n\n\nclass GroupedQueryAttention(nn.Module):\n    \"\"\"Grouped-Query Attention: Groups of query heads share key/value heads.\"\"\"\n    \n    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int):\n        super().__init__()\n        \n        assert d_model % n_heads == 0\n        assert n_heads % n_kv_heads == 0\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_kv_heads = n_kv_heads\n        self.d_k = d_model // n_heads\n        self.group_size = n_heads // n_kv_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n        self.w_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n        \n        # Query heads (full set)\n        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        # Fewer key/value heads\n        K = self.w_k(key).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n        \n        # Expand K, V to match Q heads by repeating each K,V head group_size times\n        K = K.repeat_interleave(self.group_size, dim=1)\n        V = V.repeat_interleave(self.group_size, dim=1)\n        \n        # Standard scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        # Reshape and project\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        output = self.w_o(attn_output)\n        \n        return output\n\n\n# Compare different attention mechanisms\nprint(\"ðŸ”„ MODERN ATTENTION VARIANTS COMPARISON\")\nprint(\"=\" * 50)\n\nd_model, seq_len = 256, 32\nbatch_size = 1\n\n# Create test input\nx = torch.randn(batch_size, seq_len, d_model)\n\n# Standard Multi-Head Attention\nmha = MultiHeadAttention(d_model, n_heads=8)\nmha_params = sum(p.numel() for p in mha.parameters())\n\n# Multi-Query Attention (8 query heads, 1 kv head)\nmqa = MultiQueryAttention(d_model, n_heads=8)\nmqa_params = sum(p.numel() for p in mqa.parameters())\n\n# Grouped-Query Attention (8 query heads, 2 kv heads)\ngqa = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=2)\ngqa_params = sum(p.numel() for p in gqa.parameters())\n\nprint(f\"Parameter comparison:\")\nprint(f\"Multi-Head Attention (MHA):     {mha_params:,} params\")\nprint(f\"Multi-Query Attention (MQA):    {mqa_params:,} params ({mha_params/mqa_params:.1f}x reduction)\")\nprint(f\"Grouped-Query Attention (GQA):  {gqa_params:,} params ({mha_params/gqa_params:.1f}x reduction)\")\n\n# Test forward passes\nmha_out = mha(x, x, x)\nmqa_out = mqa(x, x, x)\ngqa_out = gqa(x, x, x)\n\nprint(f\"\\nOutput shapes (all should be identical):\")\nprint(f\"MHA output: {mha_out.shape}\")\nprint(f\"MQA output: {mqa_out.shape}\")\nprint(f\"GQA output: {gqa_out.shape}\")\n\nprint(f\"\\nðŸŽ¯ Key Benefits:\")\nprint(f\"â€¢ MQA: Fewer parameters, faster inference\")\nprint(f\"â€¢ GQA: Balance between efficiency and quality\")\nprint(f\"â€¢ Both maintain same output dimensions as standard attention\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've learned the essential attention optimizations used in modern transformer systems!\n\n### Key Techniques:\n\n1. **KV Caching** - Cache key-value pairs during autoregressive generation for faster inference\n2. **Sparse Attention** - Use smart attention patterns to reduce O(nÂ²) complexity  \n3. **Multi-Query Attention (MQA)** - Share key/value heads across multiple query heads\n4. **Grouped-Query Attention (GQA)** - Balance between efficiency and quality\n\n### Real-World Impact:\n- **KV Caching**: Essential for fast text generation in chatbots and language models\n- **Sparse Attention**: Enables processing of longer sequences (Longformer, BigBird)\n- **MQA/GQA**: Used in modern models like PaLM, LLaMA for efficient inference\n\n### Key Takeaways:\n- Memory optimization often more important than FLOP reduction\n- Different sparsity patterns suit different tasks and sequence types\n- Modern variants maintain model quality while improving efficiency\n- These optimizations are crucial for production transformer systems\n\nThese techniques bridge the gap between basic attention and production-ready transformers! ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}