{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Optimization Deep Dive\n\nThis notebook teaches essential techniques for training transformers efficiently: learning rate scheduling, gradient clipping, mixed precision training, and memory optimization."
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, OneCycleLR\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List, Tuple, Optional\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n        \n        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.w_o(attn_output)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        attn_output = self.attention(x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + ff_output)\n        return x\n\nclass GPTModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n        self.pos_embedding = nn.Embedding(config['max_seq_len'], config['d_model'])\n        \n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(config['d_model'], config['n_heads'], config['d_ff'], config['dropout'])\n            for _ in range(config['n_layers'])\n        ])\n        \n        self.ln_f = nn.LayerNorm(config['d_model'])\n        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'])\n        self.dropout = nn.Dropout(config['dropout'])\n    \n    def forward(self, x):\n        seq_len = x.size(1)\n        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        \n        x = self.embedding(x) + self.pos_embedding(pos_ids)\n        x = self.dropout(x)\n        \n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        x = self.ln_f(x)\n        return self.lm_head(x)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Learning Rate Scheduling\n\nLearning rate scheduling controls how the learning rate changes during training. Warmup prevents early instability, while cosine decay provides smooth convergence. This is essential for transformer training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def warmup_cosine_schedule(step, warmup_steps, total_steps, base_lr=1e-4, min_lr=1e-6):\n    if step < warmup_steps:\n        return base_lr * (step + 1) / warmup_steps\n    else:\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return min_lr + (base_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n\ntotal_steps = 5000\nwarmup_steps = 500\nsteps = np.arange(total_steps)\n\nwarmup_cosine = [warmup_cosine_schedule(s, warmup_steps, total_steps) for s in steps]\n\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nonecycle_scheduler = OneCycleLR(optimizer, max_lr=5e-4, total_steps=total_steps)\nonecycle_lrs = []\nfor _ in range(total_steps):\n    onecycle_lrs.append(optimizer.param_groups[0]['lr'])\n    optimizer.step()\n    onecycle_scheduler.step()\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncosine_scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\ncosine_lrs = []\nfor _ in range(total_steps):\n    cosine_lrs.append(optimizer.param_groups[0]['lr'])\n    optimizer.step()\n    cosine_scheduler.step()\n\nplt.figure(figsize=(12, 6))\nplt.plot(steps, warmup_cosine, label='Warmup + Cosine (Recommended)', linewidth=3)\nplt.plot(steps, onecycle_lrs, label='OneCycle', linewidth=2)\nplt.plot(steps, cosine_lrs, label='Pure Cosine', linewidth=2)\nplt.axvline(x=warmup_steps, color='red', linestyle='--', alpha=0.7, label='Warmup End')\nplt.title('Learning Rate Schedules Comparison')\nplt.xlabel('Training Steps')\nplt.ylabel('Learning Rate')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Warmup + Cosine works best for transformers\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Gradient Clipping\n\nGradient clipping prevents gradient explosions by constraining the magnitude of gradients to a threshold. This is crucial for stable transformer training, typically using values between 0.5-2.0.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def calculate_gradient_norm(model):\n    total_norm = 0.0\n    for param in model.parameters():\n        if param.grad is not None:\n            total_norm += param.grad.data.norm(2).item() ** 2\n    return total_norm ** 0.5\n\nconfig = {\n    'vocab_size': 100,\n    'd_model': 64,\n    'n_heads': 4,\n    'n_layers': 2,\n    'd_ff': 128,\n    'max_seq_len': 32,\n    'dropout': 0.1\n}\n\nclip_values = [None, 1.0, 0.1]\n\nplt.figure(figsize=(15, 5))\n\nfor idx, clip_value in enumerate(clip_values):\n    model = GPTModel(config).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    \n    grad_norms = []\n    \n    for step in range(100):\n        x = torch.randint(0, config['vocab_size'], (4, 16), device=device)\n        targets = torch.randint(0, config['vocab_size'], (4, 16), device=device)\n        \n        optimizer.zero_grad()\n        logits = model(x)\n        loss = nn.CrossEntropyLoss()(logits.reshape(-1, config['vocab_size']), targets.reshape(-1))\n        loss.backward()\n        \n        grad_norm = calculate_gradient_norm(model)\n        grad_norms.append(grad_norm)\n        \n        if clip_value is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        \n        optimizer.step()\n    \n    plt.subplot(1, 3, idx + 1)\n    plt.plot(grad_norms, alpha=0.8, label='Gradient Norm')\n    if clip_value is not None:\n        plt.axhline(y=clip_value, color='red', linestyle='--', label=f'Clip: {clip_value}')\n        plt.title(f'Clipping: {clip_value}')\n    else:\n        plt.title('No Clipping')\n    \n    plt.xlabel('Training Steps')\n    plt.ylabel('Gradient Norm')\n    plt.yscale('log')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Gradient clipping prevents explosions and stabilizes training\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Mixed Precision Training\n\nMixed precision uses FP16 for forward pass and FP32 for parameter updates. This provides ~2x speedup and memory savings with minimal quality loss. Essential for modern GPU training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def benchmark_mixed_precision():\n    if not torch.cuda.is_available():\n        print(\"CUDA not available - skipping mixed precision demo\")\n        return\n    \n    config = {\n        'vocab_size': 1000,\n        'd_model': 256,\n        'n_heads': 8,\n        'n_layers': 4,\n        'd_ff': 512,\n        'max_seq_len': 128,\n        'dropout': 0.1\n    }\n    \n    batch_size, seq_len, num_steps = 8, 64, 30\n    results = {}\n    \n    for precision in ['FP32', 'FP16']:\n        print(f\"\\nTesting {precision}...\")\n        \n        model = GPTModel(config).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        scaler = torch.cuda.amp.GradScaler() if precision == 'FP16' else None\n        \n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n        start_time = time.time()\n        start_memory = torch.cuda.memory_allocated()\n        \n        losses = []\n        \n        for step in range(num_steps):\n            x = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            \n            optimizer.zero_grad()\n            \n            if precision == 'FP16':\n                with torch.cuda.amp.autocast():\n                    outputs = model(x)\n                    loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(x)\n                loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                loss.backward()\n                optimizer.step()\n            \n            losses.append(loss.item())\n        \n        torch.cuda.synchronize()\n        end_time = time.time()\n        peak_memory = torch.cuda.max_memory_allocated()\n        \n        results[precision] = {\n            'time': end_time - start_time,\n            'memory': (peak_memory - start_memory) / 1e9,\n            'final_loss': losses[-1]\n        }\n        \n        print(f\"  Time: {results[precision]['time']:.2f}s\")\n        print(f\"  Memory: {results[precision]['memory']:.2f}GB\")\n        print(f\"  Final loss: {results[precision]['final_loss']:.4f}\")\n        \n        del model, optimizer\n        torch.cuda.empty_cache()\n    \n    if 'FP32' in results and 'FP16' in results:\n        speedup = results['FP32']['time'] / results['FP16']['time']\n        memory_savings = (results['FP32']['memory'] - results['FP16']['memory']) / results['FP32']['memory'] * 100\n        \n        print(f\"\\nMixed Precision Benefits:\")\n        print(f\"  Speedup: {speedup:.1f}x faster\")\n        print(f\"  Memory savings: {memory_savings:.0f}%\")\n\nbenchmark_mixed_precision()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Gradient Accumulation\n\nGradient accumulation simulates large batch training with smaller memory footprint. Instead of processing large batches, we accumulate gradients over multiple mini-batches before updating parameters.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "config = {\n    'vocab_size': 1000,\n    'd_model': 128,\n    'n_heads': 8,\n    'n_layers': 4,\n    'd_ff': 256,\n    'max_seq_len': 64,\n    'dropout': 0.1\n}\n\nstrategies = {\n    'Large Batch': {'batch_size': 16, 'accum_steps': 1},\n    'Grad Accum 4x': {'batch_size': 4, 'accum_steps': 4},\n    'Grad Accum 8x': {'batch_size': 2, 'accum_steps': 8}\n}\n\nresults = {}\n\nfor name, params in strategies.items():\n    print(f\"\\nTesting {name}...\")\n    \n    model = GPTModel(config).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    \n    losses = []\n    peak_memory = 0\n    \n    for step in range(15):\n        optimizer.zero_grad()\n        step_loss = 0\n        \n        for accum_step in range(params['accum_steps']):\n            x = torch.randint(0, config['vocab_size'], (params['batch_size'], 32), device=device)\n            targets = torch.randint(0, config['vocab_size'], (params['batch_size'], 32), device=device)\n            \n            outputs = model(x)\n            loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n            loss = loss / params['accum_steps']\n            step_loss += loss.item()\n            loss.backward()\n            \n            if torch.cuda.is_available():\n                current_memory = torch.cuda.memory_allocated() / 1e9\n                peak_memory = max(peak_memory, current_memory)\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        losses.append(step_loss * params['accum_steps'])\n    \n    results[name] = {\n        'losses': losses,\n        'peak_memory': peak_memory,\n        'final_loss': losses[-1]\n    }\n    \n    print(f\"  Peak memory: {peak_memory:.2f}GB\")\n    print(f\"  Final loss: {losses[-1]:.4f}\")\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nfor name, data in results.items():\n    plt.plot(data['losses'], label=name, linewidth=2)\nplt.title('Training Loss (Same Effective Batch Size)')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nnames = list(results.keys())\nmemories = [results[name]['peak_memory'] for name in names]\ncolors = ['red', 'orange', 'green']\n\nbars = plt.bar(names, memories, color=colors, alpha=0.7)\nplt.title('Peak Memory Usage')\nplt.ylabel('Memory (GB)')\nplt.xticks(rotation=45)\n\nfor bar, memory in zip(bars, memories):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n            f'{memory:.2f}GB', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nGradient accumulation maintains quality with less memory\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Complete Optimized Training Setup\n\nThis demonstrates how to combine all optimization techniques into a production-ready training pipeline with AdamW optimizer, warmup+cosine scheduling, mixed precision, and gradient clipping.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def create_optimized_trainer(model, total_steps, warmup_steps=1000, lr=1e-4):\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n    \n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / warmup_steps\n        else:\n            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n            return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n    \n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n    \n    return optimizer, scheduler, scaler\n\ndef optimized_training_step(model, batch, scaler, gradient_accumulation_steps=1):\n    inputs, targets = batch\n    \n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = nn.CrossEntropyLoss()(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n            loss = loss / gradient_accumulation_steps\n        scaler.scale(loss).backward()\n    else:\n        outputs = model(inputs)\n        loss = nn.CrossEntropyLoss()(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n    \n    return loss.item() * gradient_accumulation_steps\n\ndef optimized_update_step(model, optimizer, scheduler, scaler, clip_value=1.0):\n    if scaler is not None:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        optimizer.step()\n    \n    scheduler.step()\n    optimizer.zero_grad()\n\nconfig = {\n    'vocab_size': 500,\n    'd_model': 128,\n    'n_heads': 4,\n    'n_layers': 2,\n    'd_ff': 256,\n    'max_seq_len': 32,\n    'dropout': 0.1\n}\n\nmodel = GPTModel(config).to(device)\ntotal_steps = 200\ngradient_accumulation_steps = 4\n\noptimizer, scheduler, scaler = create_optimized_trainer(model, total_steps)\n\nprint(\"Starting optimized training...\")\nprint(f\"\u2022 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"\u2022 Mixed precision: {'Enabled' if scaler else 'Disabled'}\")\nprint(f\"\u2022 Gradient accumulation: {gradient_accumulation_steps}x\")\n\nlosses = []\nlearning_rates = []\n\nfor step in range(total_steps):\n    step_loss = 0\n    \n    for accum_step in range(gradient_accumulation_steps):\n        x = torch.randint(0, config['vocab_size'], (2, 16), device=device)\n        targets = torch.randint(0, config['vocab_size'], (2, 16), device=device)\n        batch = (x, targets)\n        \n        loss = optimized_training_step(model, batch, scaler, gradient_accumulation_steps)\n        step_loss += loss / gradient_accumulation_steps\n    \n    optimized_update_step(model, optimizer, scheduler, scaler)\n    \n    losses.append(step_loss)\n    learning_rates.append(optimizer.param_groups[0]['lr'])\n    \n    if (step + 1) % 50 == 0:\n        print(f\"Step {step + 1}: Loss = {step_loss:.4f}, LR = {learning_rates[-1]:.6f}\")\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses, linewidth=2)\nplt.title('Training Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(learning_rates, linewidth=2, color='orange')\nplt.title('Learning Rate Schedule')\nplt.xlabel('Steps')\nplt.ylabel('Learning Rate')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTraining completed!\")\nprint(f\"\u2022 Final loss: {losses[-1]:.4f}\")\nprint(f\"\u2022 Loss reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": "## Summary\n\nEssential training optimization techniques for transformers:\n\n- **Learning Rate Scheduling**: Warmup + cosine decay prevents instability and ensures smooth convergence\n- **Gradient Clipping**: Threshold of 0.5-2.0 prevents gradient explosions\n- **Mixed Precision**: FP16/FP32 combination provides 2x speedup with minimal quality loss\n- **Gradient Accumulation**: Simulates large batches with limited memory\n- **AdamW Optimizer**: Superior to Adam for transformer training with proper weight decay\n\nThese techniques are essential for efficient transformer training at scale.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}