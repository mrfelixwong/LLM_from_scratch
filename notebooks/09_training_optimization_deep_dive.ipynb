{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Optimization Fundamentals\n\nThis notebook teaches you the essential techniques to train transformers efficiently and stably. We'll focus on the most impactful optimizations that every practitioner needs to know.\n\n## Why Training Optimization Matters\n\nWhen training neural networks, especially transformers, you'll encounter several fundamental challenges:\n\n1. **Learning Rate Problems**: Too high causes divergence, too low is inefficient\n2. **Gradient Issues**: Gradients can explode or vanish, destroying training\n3. **Memory Constraints**: Large models require clever memory management\n4. **Training Efficiency**: Modern techniques can provide 2x speedups\n\n## What You'll Learn\n\n1. **Learning Rate Scheduling**: Control learning rate changes over time\n2. **Gradient Clipping**: Prevent gradient explosions\n3. **Mixed Precision**: Train faster with 16-bit floats\n4. **Memory Optimization**: Train larger models with less memory\n\nLet's start with the foundations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import (\n",
    "    LambdaLR, CosineAnnealingLR, OneCycleLR, \n",
    "    ReduceLROnPlateau, StepLR\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our transformer components\n",
    "from src.model.transformer import GPTModel\n",
    "from src.data.tokenizer import CharacterTokenizer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Learning Rate Scheduling\n\n### The Problem with Fixed Learning Rates\n\nImagine you're looking for the lowest point in a valley while blindfolded. If you take huge steps, you'll overshoot and bounce around. If you take tiny steps, you'll barely move. Learning rate scheduling solves this by:\n\n1. **Starting small** (warmup): Prevents early instability\n2. **Increasing gradually**: Allows faster learning once stable\n3. **Decreasing over time**: Fine-tunes the solution\n\n### The Science Behind It\n\nNeural networks are sensitive to learning rates because:\n- **Too high**: Parameters oscillate wildly, loss explodes\n- **Too low**: Training is painfully slow, gets stuck in bad regions\n- **Just right**: Smooth convergence to good solutions\n\n### Essential Schedules\n\n1. **Warmup + Cosine**: Linear increase, then smooth decay (best for transformers)\n2. **OneCycle**: Single peak, good for fast training\n3. **Step Decay**: Sudden drops at intervals (simple but effective)\n\nNow let's see these in action:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Simple learning rate schedule implementations\nimport sys\nsys.path.append('..')\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef warmup_cosine_schedule(step, warmup_steps, total_steps, base_lr=1e-4, min_lr=1e-6):\n    \"\"\"The most important schedule for transformers.\"\"\"\n    if step < warmup_steps:\n        # Linear warmup: gradually increase from 0 to base_lr\n        return base_lr * (step + 1) / warmup_steps\n    else:\n        # Cosine decay: smooth decrease to min_lr\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return min_lr + (base_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n\n# Visualize the schedules\ntotal_steps = 5000\nwarmup_steps = 500\nsteps = np.arange(total_steps)\n\n# Calculate different schedules\nwarmup_cosine = [warmup_cosine_schedule(s, warmup_steps, total_steps) for s in steps]\n\n# Create a dummy model to get PyTorch scheduler curves\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# OneCycle schedule\nonecycle_scheduler = OneCycleLR(optimizer, max_lr=5e-4, total_steps=total_steps)\nonecycle_lrs = []\nfor _ in range(total_steps):\n    onecycle_lrs.append(optimizer.param_groups[0]['lr'])\n    optimizer.step()\n    onecycle_scheduler.step()\n\n# Reset for cosine\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncosine_scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\ncosine_lrs = []\nfor _ in range(total_steps):\n    cosine_lrs.append(optimizer.param_groups[0]['lr'])\n    optimizer.step()\n    cosine_scheduler.step()\n\n# Plot comparison\nplt.figure(figsize=(12, 6))\nplt.plot(steps, warmup_cosine, label='Warmup + Cosine (Recommended)', linewidth=3)\nplt.plot(steps, onecycle_lrs, label='OneCycle', linewidth=2)\nplt.plot(steps, cosine_lrs, label='Pure Cosine', linewidth=2)\n\nplt.axvline(x=warmup_steps, color='red', linestyle='--', alpha=0.7, label='Warmup End')\nplt.title('Learning Rate Schedules Comparison')\nplt.xlabel('Training Steps')\nplt.ylabel('Learning Rate')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Key Insights:\")\nprint(\"• Warmup + Cosine works best for transformers\")\nprint(\"• Warmup prevents early training instability\") \nprint(\"• Cosine decay provides smooth convergence\")\nprint(\"• OneCycle can be faster but less stable\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Gradient Clipping\n\n### The Exploding Gradient Problem\n\nDuring backpropagation, gradients can become extremely large, causing:\n- **Parameter updates that are too big**: Model parameters jump wildly\n- **Loss spikes**: Training loss suddenly shoots up to infinity\n- **Training collapse**: Model becomes impossible to train\n\nThink of it like driving a car: if you turn the steering wheel too hard, you'll crash.\n\n### How Gradient Clipping Works\n\nGradient clipping constrains the magnitude of gradients:\n\n1. **Calculate gradient norm**: √(sum of all squared gradients)\n2. **Check if too large**: Compare to threshold (e.g., 1.0)\n3. **Scale down if needed**: Multiply all gradients by (threshold / norm)\n\nThis preserves the direction but limits the magnitude.\n\n### Why It's Essential for Transformers\n\nTransformers are especially prone to gradient explosions because:\n- **Deep networks**: Gradients multiply through many layers\n- **Attention mechanism**: Can create very large gradients\n- **Residual connections**: Can amplify gradient flow\n\nLet's implement gradient monitoring and clipping:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "from src.model.transformer import GPTModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ndef calculate_gradient_norm(model):\n    \"\"\"Calculate the L2 norm of all gradients.\"\"\"\n    total_norm = 0.0\n    for param in model.parameters():\n        if param.grad is not None:\n            total_norm += param.grad.data.norm(2).item() ** 2\n    return total_norm ** 0.5\n\ndef demonstrate_gradient_clipping():\n    \"\"\"Show the effect of gradient clipping on training stability.\"\"\"\n    # Create a small transformer for demonstration\n    config = {\n        'vocab_size': 100,\n        'd_model': 64,\n        'n_heads': 4,\n        'n_layers': 2,\n        'd_ff': 128,\n        'max_seq_len': 32,\n        'dropout': 0.1\n    }\n    \n    clip_values = [None, 1.0, 0.1]  # No clipping, moderate clipping, strong clipping\n    \n    plt.figure(figsize=(15, 5))\n    \n    for idx, clip_value in enumerate(clip_values):\n        model = GPTModel(config).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3)  # High LR to cause instability\n        \n        grad_norms = []\n        losses = []\n        \n        for step in range(100):\n            # Create random batch\n            x = torch.randint(0, config['vocab_size'], (4, 16), device=device)\n            targets = torch.randint(0, config['vocab_size'], (4, 16), device=device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            logits = model(x)\n            loss = nn.CrossEntropyLoss()(logits.reshape(-1, config['vocab_size']), targets.reshape(-1))\n            \n            # Backward pass\n            loss.backward()\n            \n            # Record gradient norm before clipping\n            grad_norm = calculate_gradient_norm(model)\n            grad_norms.append(grad_norm)\n            losses.append(loss.item())\n            \n            # Apply clipping if specified\n            if clip_value is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n            \n            optimizer.step()\n        \n        # Plot results\n        plt.subplot(1, 3, idx + 1)\n        plt.plot(grad_norms, alpha=0.8, label='Gradient Norm')\n        if clip_value is not None:\n            plt.axhline(y=clip_value, color='red', linestyle='--', label=f'Clip threshold: {clip_value}')\n            plt.title(f'Clipping: {clip_value}')\n        else:\n            plt.title('No Clipping')\n        \n        plt.xlabel('Training Steps')\n        plt.ylabel('Gradient Norm')\n        plt.yscale('log')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Observations:\")\n    print(\"• Without clipping: Gradients can explode (>100)\")\n    print(\"• With clipping: Gradients stay bounded\") \n    print(\"• Too aggressive clipping can slow learning\")\n    print(\"• Typical values: 0.5-2.0 for transformers\")\n\ndemonstrate_gradient_clipping()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Mixed Precision Training\n\n### The Memory and Speed Problem\n\nTraining large transformers faces two major bottlenecks:\n1. **Memory**: Models can require 20+ GB of GPU memory\n2. **Speed**: Training can take days or weeks\n\n### What is Mixed Precision?\n\nMixed precision uses two number formats:\n- **FP32 (32-bit floats)**: High precision for critical operations\n- **FP16 (16-bit floats)**: Lower precision for most operations\n\nThis provides:\n- **2x memory reduction**: Store activations and gradients in 16-bit\n- **1.5-2x speed boost**: Modern GPUs have specialized 16-bit units\n- **Minimal accuracy loss**: Careful handling preserves model quality\n\n### How It Works\n\n1. **Forward pass**: Compute in FP16 (faster, less memory)\n2. **Loss scaling**: Multiply loss to prevent gradient underflow\n3. **Backward pass**: Gradients in FP16, but scaled up\n4. **Parameter updates**: Unscale and update in FP32 (precision)\n\n### The Challenge: Gradient Underflow\n\nFP16 has a much smaller range than FP32. Very small gradients can become zero, hurting training. Loss scaling solves this by multiplying the loss (and gradients) by a large number before backprop.\n\nLet's see mixed precision in action:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import time\n\ndef benchmark_mixed_precision():\n    \"\"\"Compare FP32 vs FP16 training.\"\"\"\n    if not torch.cuda.is_available():\n        print(\"CUDA not available - skipping mixed precision demo\")\n        return\n    \n    # Medium-sized model for noticeable differences\n    config = {\n        'vocab_size': 1000,\n        'd_model': 256,\n        'n_heads': 8,\n        'n_layers': 4,\n        'd_ff': 512,\n        'max_seq_len': 128,\n        'dropout': 0.1\n    }\n    \n    batch_size = 8\n    seq_len = 64\n    num_steps = 30\n    \n    results = {}\n    \n    for precision in ['FP32', 'FP16']:\n        print(f\"\\nTesting {precision}...\")\n        \n        # Create fresh model\n        model = GPTModel(config).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        \n        # Setup mixed precision if needed\n        scaler = torch.cuda.amp.GradScaler() if precision == 'FP16' else None\n        \n        # Warmup\n        for _ in range(3):\n            x = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            \n            optimizer.zero_grad()\n            if precision == 'FP16':\n                with torch.cuda.amp.autocast():\n                    outputs = model(x)\n                    loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(x)\n                loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                loss.backward()\n                optimizer.step()\n        \n        # Clear memory and start timing\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n        start_time = time.time()\n        start_memory = torch.cuda.memory_allocated()\n        \n        losses = []\n        \n        # Actual benchmark\n        for step in range(num_steps):\n            x = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n            \n            optimizer.zero_grad()\n            \n            if precision == 'FP16':\n                with torch.cuda.amp.autocast():\n                    outputs = model(x)\n                    loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(x)\n                loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                loss.backward()\n                optimizer.step()\n            \n            losses.append(loss.item())\n        \n        torch.cuda.synchronize()\n        end_time = time.time()\n        peak_memory = torch.cuda.max_memory_allocated()\n        \n        # Store results\n        results[precision] = {\n            'time': end_time - start_time,\n            'memory': (peak_memory - start_memory) / 1e9,  # GB\n            'final_loss': losses[-1]\n        }\n        \n        print(f\"  Time: {results[precision]['time']:.2f}s\")\n        print(f\"  Memory: {results[precision]['memory']:.2f}GB\")\n        print(f\"  Final loss: {results[precision]['final_loss']:.4f}\")\n        \n        # Cleanup\n        del model, optimizer\n        torch.cuda.empty_cache()\n    \n    # Calculate improvements\n    if 'FP32' in results and 'FP16' in results:\n        speedup = results['FP32']['time'] / results['FP16']['time']\n        memory_savings = (results['FP32']['memory'] - results['FP16']['memory']) / results['FP32']['memory'] * 100\n        \n        print(f\"\\n🚀 Mixed Precision Benefits:\")\n        print(f\"  Speedup: {speedup:.1f}x faster\")\n        print(f\"  Memory savings: {memory_savings:.0f}%\")\n        print(f\"  Quality: Similar final loss\")\n\nbenchmark_mixed_precision()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Memory Optimization\n\n### The Memory Crisis\n\nLarge transformer models require enormous amounts of GPU memory:\n- **Model parameters**: Billions of weights in FP32\n- **Activations**: Intermediate values stored for backpropagation  \n- **Gradients**: Same size as parameters\n- **Optimizer states**: Adam stores momentum and variance\n\nA 1B parameter model needs ~24GB just for the basics!\n\n### Gradient Accumulation: The Simple Solution\n\nInstead of processing large batches at once, accumulate gradients over multiple smaller batches:\n\n1. **Forward pass**: Process small batch (e.g., 2 samples)\n2. **Backward pass**: Compute gradients, but don't update yet\n3. **Accumulate**: Add gradients to running total\n4. **Update**: After N mini-batches, average gradients and update parameters\n\nThis simulates large batch training with less memory.\n\n### How Gradient Accumulation Works\n\n```\nNormal training (batch=8):\n  forward(8 samples) → backward → update\n\nGradient accumulation (batch=2, steps=4):\n  forward(2) → backward → accumulate\n  forward(2) → backward → accumulate  \n  forward(2) → backward → accumulate\n  forward(2) → backward → update (with average)\n```\n\nThe effective batch size is the same, but peak memory is much lower.\n\nLet's see this in practice:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def demonstrate_gradient_accumulation():\n    \"\"\"Show how gradient accumulation saves memory while maintaining quality.\"\"\"\n    \n    # Larger model to see memory differences  \n    config = {\n        'vocab_size': 1000,\n        'd_model': 128,\n        'n_heads': 8,\n        'n_layers': 4,\n        'd_ff': 256,\n        'max_seq_len': 64,\n        'dropout': 0.1\n    }\n    \n    strategies = {\n        'Large Batch': {'batch_size': 16, 'accum_steps': 1},\n        'Grad Accum 4x': {'batch_size': 4, 'accum_steps': 4},\n        'Grad Accum 8x': {'batch_size': 2, 'accum_steps': 8}\n    }\n    \n    results = {}\n    \n    for name, params in strategies.items():\n        print(f\"\\nTesting {name}...\")\n        \n        model = GPTModel(config).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        \n        losses = []\n        peak_memory = 0\n        \n        for step in range(15):  # Short run for demo\n            optimizer.zero_grad()\n            step_loss = 0\n            \n            # Gradient accumulation loop\n            for accum_step in range(params['accum_steps']):\n                # Create mini-batch\n                x = torch.randint(0, config['vocab_size'], (params['batch_size'], 32), device=device)\n                targets = torch.randint(0, config['vocab_size'], (params['batch_size'], 32), device=device)\n                \n                # Forward pass\n                outputs = model(x)\n                loss = nn.CrossEntropyLoss()(outputs.reshape(-1, config['vocab_size']), targets.reshape(-1))\n                \n                # Scale loss for accumulation\n                loss = loss / params['accum_steps']\n                step_loss += loss.item()\n                \n                # Backward pass\n                loss.backward()\n                \n                # Track memory usage\n                if torch.cuda.is_available():\n                    current_memory = torch.cuda.memory_allocated() / 1e9\n                    peak_memory = max(peak_memory, current_memory)\n            \n            # Update after accumulation\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            losses.append(step_loss * params['accum_steps'])  # Unscale for comparison\n        \n        results[name] = {\n            'losses': losses,\n            'peak_memory': peak_memory,\n            'final_loss': losses[-1]\n        }\n        \n        print(f\"  Peak memory: {peak_memory:.2f}GB\")\n        print(f\"  Final loss: {losses[-1]:.4f}\")\n    \n    # Plot comparison\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    for name, data in results.items():\n        plt.plot(data['losses'], label=name, linewidth=2)\n    plt.title('Training Loss (Same Effective Batch Size)')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    names = list(results.keys())\n    memories = [results[name]['peak_memory'] for name in names]\n    colors = ['red', 'orange', 'green']\n    \n    bars = plt.bar(names, memories, color=colors, alpha=0.7)\n    plt.title('Peak Memory Usage')\n    plt.ylabel('Memory (GB)')\n    plt.xticks(rotation=45)\n    \n    # Add value labels on bars\n    for bar, memory in zip(bars, memories):\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                f'{memory:.2f}GB', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n💡 Key Insights:\")\n    print(\"• Gradient accumulation maintains training quality\")\n    print(\"• Memory usage scales with mini-batch size, not effective batch size\")\n    print(\"• Essential technique for training large models\")\n\ndemonstrate_gradient_accumulation()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Essential Training Recipe\n\n### Putting It All Together\n\nNow you know the four pillars of efficient transformer training. Here's how to combine them into a production-ready training loop:\n\n### The Complete Recipe\n\n1. **Learning Rate**: Warmup + cosine decay\n2. **Gradient Clipping**: Clip to 1.0 \n3. **Mixed Precision**: Enable for speed and memory\n4. **Gradient Accumulation**: Use when memory is limited\n5. **Optimizer**: AdamW with weight decay\n\n### Why AdamW?\n\nAdamW (Adam with decoupled Weight decay) is the gold standard because:\n- **Adaptive learning rates**: Different rates for each parameter\n- **Momentum**: Smooths out noisy gradients\n- **Proper weight decay**: Regularizes without interfering with gradients\n- **Proven track record**: Used by GPT, BERT, and most successful models\n\nHere's the complete training template:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def create_optimized_trainer(model, total_steps, warmup_steps=1000, lr=1e-4):\n    \"\"\"Create an optimized training setup with all best practices.\"\"\"\n    \n    # 1. AdamW optimizer with weight decay\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n    \n    # 2. Learning rate scheduler (warmup + cosine)\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / warmup_steps\n        else:\n            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n            return 0.1 + 0.9 * 0.5 * (1 + np.cos(np.pi * progress))\n    \n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    \n    # 3. Mixed precision scaler\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n    \n    return optimizer, scheduler, scaler\n\ndef optimized_training_step(model, batch, optimizer, scheduler, scaler, \n                          gradient_accumulation_steps=1, clip_value=1.0):\n    \"\"\"A single optimized training step with all techniques.\"\"\"\n    \n    inputs, targets = batch\n    \n    # Mixed precision forward pass\n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = nn.CrossEntropyLoss()(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n            loss = loss / gradient_accumulation_steps  # Scale for accumulation\n        \n        # Mixed precision backward pass\n        scaler.scale(loss).backward()\n    else:\n        # Regular precision\n        outputs = model(inputs)\n        loss = nn.CrossEntropyLoss()(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n    \n    return loss.item() * gradient_accumulation_steps  # Return unscaled loss\n\ndef optimized_update_step(model, optimizer, scheduler, scaler, clip_value=1.0):\n    \"\"\"Update parameters with clipping and mixed precision handling.\"\"\"\n    \n    if scaler is not None:\n        # Mixed precision parameter update\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        # Regular precision parameter update\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        optimizer.step()\n    \n    scheduler.step()\n    optimizer.zero_grad()\n\n# Demonstrate the complete training loop\ndef demonstrate_optimized_training():\n    \"\"\"Show the complete optimized training setup.\"\"\"\n    \n    # Small model for quick demo\n    config = {\n        'vocab_size': 500,\n        'd_model': 128,\n        'n_heads': 4,\n        'n_layers': 2,\n        'd_ff': 256,\n        'max_seq_len': 32,\n        'dropout': 0.1\n    }\n    \n    model = GPTModel(config).to(device)\n    total_steps = 200\n    gradient_accumulation_steps = 4\n    \n    # Create optimized setup\n    optimizer, scheduler, scaler = create_optimized_trainer(model, total_steps)\n    \n    print(\"🚀 Starting optimized training...\")\n    print(f\"• Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"• Mixed precision: {'Enabled' if scaler else 'Disabled'}\")\n    print(f\"• Gradient accumulation: {gradient_accumulation_steps}x\")\n    \n    losses = []\n    learning_rates = []\n    \n    for step in range(total_steps):\n        step_loss = 0\n        \n        # Gradient accumulation loop\n        for accum_step in range(gradient_accumulation_steps):\n            # Create batch\n            x = torch.randint(0, config['vocab_size'], (2, 16), device=device)\n            targets = torch.randint(0, config['vocab_size'], (2, 16), device=device)\n            batch = (x, targets)\n            \n            # Forward and backward\n            loss = optimized_training_step(model, batch, optimizer, scheduler, scaler, \n                                         gradient_accumulation_steps)\n            step_loss += loss / gradient_accumulation_steps\n        \n        # Parameter update\n        optimized_update_step(model, optimizer, scheduler, scaler)\n        \n        # Record metrics\n        losses.append(step_loss)\n        learning_rates.append(optimizer.param_groups[0]['lr'])\n        \n        if (step + 1) % 50 == 0:\n            print(f\"Step {step + 1}: Loss = {step_loss:.4f}, LR = {learning_rates[-1]:.6f}\")\n    \n    # Plot results\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(losses, linewidth=2)\n    plt.title('Training Loss')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(learning_rates, linewidth=2, color='orange')\n    plt.title('Learning Rate Schedule')\n    plt.xlabel('Steps')\n    plt.ylabel('Learning Rate')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n✅ Training completed!\")\n    print(f\"• Final loss: {losses[-1]:.4f}\")\n    print(f\"• Loss reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n\ndemonstrate_optimized_training()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Your Training Optimization Checklist\n\n### ✅ Essential Techniques (Use Always)\n\n1. **Learning Rate Scheduling**\n   - Use warmup (1000-4000 steps) for stability\n   - Apply cosine decay for smooth convergence\n   - Typical values: 1e-4 base LR, 1e-6 minimum\n\n2. **Gradient Clipping** \n   - Always clip to prevent explosions\n   - Typical values: 0.5-2.0 for transformers\n   - Monitor gradient norms to set threshold\n\n3. **Mixed Precision**\n   - 1.5-2x speedup with minimal quality loss\n   - Essential for large models\n   - Use PyTorch's `autocast()` and `GradScaler()`\n\n4. **AdamW Optimizer**\n   - Better than Adam for transformers\n   - Use weight decay: 0.01-0.1\n   - Betas: (0.9, 0.95) for transformers\n\n### 💡 Memory Optimization (Use When Needed)\n\n5. **Gradient Accumulation**\n   - Simulate large batches with small memory\n   - Essential for training large models\n   - Accumulate 4-16 steps typically\n\n### 🚫 Common Mistakes to Avoid\n\n- **No warmup**: Causes early training instability\n- **Learning rate too high**: Loss explodes and training fails\n- **No gradient clipping**: Gradients explode, destroying training\n- **Ignoring memory limits**: OOM errors halt training\n- **Using Adam instead of AdamW**: Suboptimal convergence\n\n### 📝 Production Training Template\n\n```python\n# 1. Setup\nmodel = GPTModel(config)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps=1000, num_training_steps=10000)\nscaler = torch.cuda.amp.GradScaler()\n\n# 2. Training loop\nfor step in range(num_steps):\n    for accum_step in range(gradient_accumulation_steps):\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets) / gradient_accumulation_steps\n        \n        scaler.scale(loss).backward()\n    \n    # 3. Update with clipping\n    scaler.unscale_(optimizer)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    scaler.step(optimizer)\n    scaler.update()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n### 🎯 Key Results\n\nWith these optimizations, you can expect:\n- **2x faster training** (mixed precision)\n- **50% less memory usage** (gradient accumulation)\n- **More stable training** (gradient clipping + warmup)\n- **Better final performance** (AdamW + proper scheduling)\n\nYou now have the tools to train transformers efficiently and effectively!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        # pred: (batch_size * seq_len, vocab_size)\n",
    "        # target: (batch_size * seq_len,)\n",
    "        \n",
    "        vocab_size = pred.size(-1)\n",
    "        log_probs = torch.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot with smoothing\n",
    "        target_one_hot = torch.zeros_like(log_probs)\n",
    "        target_one_hot.fill_(self.smoothing / (vocab_size - 1))\n",
    "        target_one_hot.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -(target_one_hot * log_probs).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "class LayerwiseLROptimizer:\n",
    "    \"\"\"Optimizer with layer-wise learning rates.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, base_lr: float = 1e-4, \n",
    "                 decay_factor: float = 0.8):\n",
    "        self.model = model\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        \n",
    "        # Create parameter groups with different learning rates\n",
    "        param_groups = self._create_param_groups()\n",
    "        self.optimizer = optim.AdamW(param_groups, weight_decay=0.01)\n",
    "    \n",
    "    def _create_param_groups(self) -> List[Dict]:\n",
    "        \"\"\"Create parameter groups with layer-wise learning rates.\"\"\"\n",
    "        param_groups = []\n",
    "        \n",
    "        # Embedding layers - lowest LR\n",
    "        embedding_params = []\n",
    "        if hasattr(self.model, 'embedding'):\n",
    "            embedding_params.extend(self.model.embedding.parameters())\n",
    "        if hasattr(self.model, 'pos_embedding'):\n",
    "            embedding_params.extend(self.model.pos_embedding.parameters())\n",
    "        \n",
    "        if embedding_params:\n",
    "            param_groups.append({\n",
    "                'params': embedding_params,\n",
    "                'lr': self.base_lr * (self.decay_factor ** 3)\n",
    "            })\n",
    "        \n",
    "        # Transformer blocks - layer-wise decay\n",
    "        if hasattr(self.model, 'transformer_blocks'):\n",
    "            for i, block in enumerate(self.model.transformer_blocks):\n",
    "                layer_lr = self.base_lr * (self.decay_factor ** (len(self.model.transformer_blocks) - i - 1))\n",
    "                param_groups.append({\n",
    "                    'params': list(block.parameters()),\n",
    "                    'lr': layer_lr\n",
    "                })\n",
    "        \n",
    "        # Output layers - highest LR\n",
    "        if hasattr(self.model, 'ln_f'):\n",
    "            param_groups.append({\n",
    "                'params': list(self.model.ln_f.parameters()),\n",
    "                'lr': self.base_lr\n",
    "            })\n",
    "        if hasattr(self.model, 'lm_head'):\n",
    "            param_groups.append({\n",
    "                'params': list(self.model.lm_head.parameters()),\n",
    "                'lr': self.base_lr\n",
    "            })\n",
    "        \n",
    "        return param_groups\n",
    "    \n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "    def get_lr_info(self) -> Dict[str, float]:\n",
    "        \"\"\"Get learning rate information for each group.\"\"\"\n",
    "        lr_info = {}\n",
    "        group_names = ['embeddings'] + [f'block_{i}' for i in range(len(self.model.transformer_blocks))] + ['output']\n",
    "        \n",
    "        for i, group in enumerate(self.optimizer.param_groups):\n",
    "            if i < len(group_names):\n",
    "                lr_info[group_names[i]] = group['lr']\n",
    "        \n",
    "        return lr_info\n",
    "\n",
    "def demonstrate_advanced_techniques():\n",
    "    \"\"\"Demonstrate advanced training techniques.\"\"\"\n",
    "    print(\"Demonstrating advanced training techniques...\")\n",
    "    \n",
    "    # Model configuration\n",
    "    config = {\n",
    "        'vocab_size': 300,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 4,\n",
    "        'n_layers': 3,\n",
    "        'd_ff': 256,\n",
    "        'max_seq_len': 64,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    \n",
    "    # Compare different techniques\n",
    "    techniques = {\n",
    "        'Baseline': {\n",
    "            'loss_fn': nn.CrossEntropyLoss(),\n",
    "            'optimizer_type': 'standard'\n",
    "        },\n",
    "        'Label Smoothing': {\n",
    "            'loss_fn': LabelSmoothingCrossEntropy(smoothing=0.1),\n",
    "            'optimizer_type': 'standard'\n",
    "        },\n",
    "        'Layer-wise LR': {\n",
    "            'loss_fn': nn.CrossEntropyLoss(),\n",
    "            'optimizer_type': 'layerwise'\n",
    "        },\n",
    "        'Both': {\n",
    "            'loss_fn': LabelSmoothingCrossEntropy(smoothing=0.1),\n",
    "            'optimizer_type': 'layerwise'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    num_steps = 150\n",
    "    \n",
    "    for tech_name, tech_config in techniques.items():\n",
    "        print(f\"\\nTraining with {tech_name}...\")\n",
    "        \n",
    "        # Create model\n",
    "        model = GPTModel(config).to(device)\n",
    "        \n",
    "        # Create optimizer\n",
    "        if tech_config['optimizer_type'] == 'layerwise':\n",
    "            optimizer = LayerwiseLROptimizer(model)\n",
    "            print(f\"  Layer-wise learning rates: {optimizer.get_lr_info()}\")\n",
    "        else:\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "        \n",
    "        loss_fn = tech_config['loss_fn']\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Create batch\n",
    "            x = torch.randint(0, config['vocab_size'], (8, 32), device=device)\n",
    "            targets = torch.randint(0, config['vocab_size'], (8, 32), device=device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "            \n",
    "            # Compute accuracy\n",
    "            with torch.no_grad():\n",
    "                pred_tokens = outputs.argmax(dim=-1)\n",
    "                accuracy = (pred_tokens == targets).float().mean().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        results[tech_name] = {\n",
    "            'losses': losses,\n",
    "            'accuracies': accuracies\n",
    "        }\n",
    "        \n",
    "        print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "        print(f\"  Final accuracy: {accuracies[-1]:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    for tech_name, data in results.items():\n",
    "        axes[0, 0].plot(data['losses'], label=tech_name, linewidth=2)\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Accuracy curves\n",
    "    for tech_name, data in results.items():\n",
    "        axes[0, 1].plot(data['accuracies'], label=tech_name, linewidth=2)\n",
    "    axes[0, 1].set_title('Training Accuracy')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Smoothed loss (rolling average)\n",
    "    window = 20\n",
    "    for tech_name, data in results.items():\n",
    "        smoothed = np.convolve(data['losses'], np.ones(window)/window, mode='valid')\n",
    "        axes[1, 0].plot(range(window-1, len(data['losses'])), smoothed, \n",
    "                       label=tech_name, linewidth=2)\n",
    "    axes[1, 0].set_title(f'Smoothed Loss (window={window})')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final comparison\n",
    "    final_losses = [results[tech]['losses'][-1] for tech in techniques.keys()]\n",
    "    final_accuracies = [results[tech]['accuracies'][-1] for tech in techniques.keys()]\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    \n",
    "    ax2 = axes[1, 1]\n",
    "    color = 'tab:red'\n",
    "    ax2.set_xlabel('Technique')\n",
    "    ax2.set_ylabel('Final Loss', color=color)\n",
    "    bars1 = ax2.bar(x - 0.2, final_losses, 0.4, color=color, alpha=0.7)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    ax3 = ax2.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax3.set_ylabel('Final Accuracy', color=color)\n",
    "    bars2 = ax3.bar(x + 0.2, final_accuracies, 0.4, color=color, alpha=0.7)\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(techniques.keys(), rotation=45)\n",
    "    ax2.set_title('Final Performance Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTechnique Performance Summary:\")\n",
    "    print(f\"{'Technique':<15} {'Final Loss':<12} {'Final Acc':<12} {'Best Loss':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for tech_name, data in results.items():\n",
    "        final_loss = data['losses'][-1]\n",
    "        final_acc = data['accuracies'][-1]\n",
    "        best_loss = min(data['losses'])\n",
    "        \n",
    "        print(f\"{tech_name:<15} {final_loss:<12.4f} {final_acc:<12.4f} {best_loss:<12.4f}\")\n",
    "\n",
    "# Demonstrate advanced techniques\n",
    "demonstrate_advanced_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Best Practices\n",
    "\n",
    "### Training Optimization Checklist:\n",
    "\n",
    "#### Learning Rate Scheduling:\n",
    "- ✅ Use warmup (1000-4000 steps) for transformer training\n",
    "- ✅ Consider cosine annealing for smooth decay\n",
    "- ✅ OneCycle can be effective for faster convergence\n",
    "- ✅ Monitor learning rate vs loss relationship\n",
    "\n",
    "#### Gradient Management:\n",
    "- ✅ Always use gradient clipping (0.5-2.0 for transformers)\n",
    "- ✅ Monitor gradient norms regularly\n",
    "- ✅ Watch for exploding/vanishing gradients\n",
    "- ✅ Consider adaptive clipping strategies\n",
    "\n",
    "#### Memory Optimization:\n",
    "- ✅ Use mixed precision training when available\n",
    "- ✅ Implement gradient accumulation for large effective batch sizes\n",
    "- ✅ Consider gradient checkpointing for memory-limited scenarios\n",
    "- ✅ Monitor memory usage and optimize accordingly\n",
    "\n",
    "#### Optimizer Selection:\n",
    "- ✅ AdamW is generally the best starting point\n",
    "- ✅ Consider Lion for efficiency\n",
    "- ✅ Experiment with layer-wise learning rates\n",
    "- ✅ Use appropriate weight decay (0.01-0.1)\n",
    "\n",
    "#### Advanced Techniques:\n",
    "- ✅ Label smoothing can improve generalization\n",
    "- ✅ Curriculum learning for complex tasks\n",
    "- ✅ Adaptive dropout scheduling\n",
    "- ✅ Regular checkpointing and early stopping\n",
    "\n",
    "### Common Pitfalls to Avoid:\n",
    "1. **No warmup**: Can cause early training instability\n",
    "2. **Learning rate too high**: Causes loss spikes and divergence\n",
    "3. **No gradient clipping**: Exploding gradients destroy training\n",
    "4. **Ignoring memory usage**: OOM errors halt training\n",
    "5. **Not monitoring training**: Miss important signals about model health\n",
    "\n",
    "### Recommended Training Pipeline:\n",
    "```python\n",
    "# 1. Setup with proper initialization\n",
    "model = GPTModel(config)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps=1000, num_training_steps=10000)\n",
    "\n",
    "# 2. Enable optimizations\n",
    "scaler = GradScaler()  # Mixed precision\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# 3. Training loop with monitoring\n",
    "for step in range(num_steps):\n",
    "    # Forward pass with mixed precision\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets) / gradient_accumulation_steps\n",
    "    \n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        # Gradient clipping and optimization\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "This completes our deep dive into training optimization! These techniques will help you train transformers more efficiently and effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}