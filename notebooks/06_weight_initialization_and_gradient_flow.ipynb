{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Weight Initialization and Gradient Flow: The Foundation of Deep Learning\n",
    "\n",
    "Before we dive deeper into transformer training, we need to understand one of the most critical aspects that makes deep networks trainable: **proper weight initialization** and **gradient flow**.\n",
    "\n",
    "Poor initialization can make your transformer:\n",
    "- ❌ Never converge (gradients vanish or explode)\n",
    "- ❌ Train extremely slowly\n",
    "- ❌ Get stuck in poor local minima\n",
    "- ❌ Exhibit unstable training dynamics\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Why Initialization Matters** - The mathematical foundations\n",
    "2. **Xavier/Glorot Initialization** - The gold standard for deep networks\n",
    "3. **He Initialization** - For ReLU and modern activations\n",
    "4. **Gradient Flow Analysis** - Visualizing gradients through layers\n",
    "5. **Transformer-Specific Considerations** - Attention layers and residual connections\n",
    "6. **Common Failure Modes** - What goes wrong and how to fix it\n",
    "\n",
    "Let's build intuition through mathematics and code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import math\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Why Weight Initialization Matters: The Mathematics\n",
    "\n",
    "Let's understand why random initialization can make or break training. Consider a simple linear layer:\n",
    "\n",
    "$$y = Wx + b$$\n",
    "\n",
    "For deep networks, we need:\n",
    "1. **Forward pass**: Activations should have reasonable variance\n",
    "2. **Backward pass**: Gradients should flow without vanishing or exploding\n",
    "\n",
    "Let's see what happens with different initialization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_initialization_impact():\n",
    "    \"\"\"Demonstrate how initialization affects activation and gradient flow.\"\"\"\n",
    "    \n",
    "    # Network parameters\n",
    "    input_size = 512\n",
    "    hidden_size = 512\n",
    "    num_layers = 10\n",
    "    batch_size = 64\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, input_size)\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    initializations = {\n",
    "        'Too Small (0.01)': {'std': 0.01, 'description': 'Weights too small → vanishing'},\n",
    "        'Too Large (1.0)': {'std': 1.0, 'description': 'Weights too large → exploding'},\n",
    "        'Xavier/Glorot': {'xavier': True, 'description': 'Variance-preserving initialization'},\n",
    "        'He (ReLU)': {'he': True, 'description': 'For ReLU activations'}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for init_name, init_config in initializations.items():\n",
    "        print(f\"\\n🔍 Testing {init_name}:\")\n",
    "        print(f\"   {init_config['description']}\")\n",
    "        \n",
    "        # Create network\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nn.Linear(hidden_size if i > 0 else input_size, hidden_size)\n",
    "            \n",
    "            # Apply initialization\n",
    "            if 'std' in init_config:\n",
    "                nn.init.normal_(layer.weight, mean=0, std=init_config['std'])\n",
    "            elif 'xavier' in init_config:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif 'he' in init_config:\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            \n",
    "            nn.init.zeros_(layer.bias)\n",
    "            layers.append(layer)\n",
    "        \n",
    "        # Forward pass with activation recording\n",
    "        activations = []\n",
    "        current_input = x\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            current_input = layer(current_input)\n",
    "            if i < len(layers) - 1:  # Apply activation except last layer\n",
    "                current_input = torch.tanh(current_input)  # Use tanh for clear demonstration\n",
    "            activations.append(current_input.clone())\n",
    "        \n",
    "        # Backward pass for gradient analysis\n",
    "        output = activations[-1]\n",
    "        loss = output.mean()  # Simple loss for demonstration\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect statistics\n",
    "        activation_stats = []\n",
    "        gradient_stats = []\n",
    "        \n",
    "        for i, (activation, layer) in enumerate(zip(activations, layers)):\n",
    "            # Activation statistics\n",
    "            act_mean = activation.mean().item()\n",
    "            act_std = activation.std().item()\n",
    "            act_min = activation.min().item()\n",
    "            act_max = activation.max().item()\n",
    "            \n",
    "            activation_stats.append({\n",
    "                'layer': i,\n",
    "                'mean': act_mean,\n",
    "                'std': act_std,\n",
    "                'min': act_min,\n",
    "                'max': act_max\n",
    "            })\n",
    "            \n",
    "            # Gradient statistics\n",
    "            if layer.weight.grad is not None:\n",
    "                grad_mean = layer.weight.grad.mean().item()\n",
    "                grad_std = layer.weight.grad.std().item()\n",
    "                grad_norm = layer.weight.grad.norm().item()\n",
    "                \n",
    "                gradient_stats.append({\n",
    "                    'layer': i,\n",
    "                    'mean': grad_mean,\n",
    "                    'std': grad_std,\n",
    "                    'norm': grad_norm\n",
    "                })\n",
    "        \n",
    "        results[init_name] = {\n",
    "            'activations': activation_stats,\n",
    "            'gradients': gradient_stats\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        final_act_std = activation_stats[-1]['std']\n",
    "        avg_grad_norm = np.mean([g['norm'] for g in gradient_stats])\n",
    "        \n",
    "        print(f\"   Final activation std: {final_act_std:.4f}\")\n",
    "        print(f\"   Average gradient norm: {avg_grad_norm:.4f}\")\n",
    "        \n",
    "        # Clear gradients\n",
    "        for layer in layers:\n",
    "            layer.zero_grad()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze different initialization strategies\n",
    "init_results = analyze_initialization_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Visualizing Activation and Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_initialization_effects(results):\n",
    "    \"\"\"Visualize how initialization affects activation and gradient flow.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    colors = ['red', 'orange', 'green', 'blue']\n",
    "    \n",
    "    for i, (init_name, data) in enumerate(results.items()):\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Extract data\n",
    "        layers = [stat['layer'] for stat in data['activations']]\n",
    "        act_stds = [stat['std'] for stat in data['activations']]\n",
    "        act_means = [stat['mean'] for stat in data['activations']]\n",
    "        grad_norms = [stat['norm'] for stat in data['gradients']]\n",
    "        \n",
    "        # Plot activation standard deviations\n",
    "        ax1.plot(layers, act_stds, 'o-', color=color, label=init_name, linewidth=2, markersize=6)\n",
    "        \n",
    "        # Plot activation means\n",
    "        ax2.plot(layers, [abs(mean) for mean in act_means], 'o-', color=color, label=init_name, linewidth=2, markersize=6)\n",
    "        \n",
    "        # Plot gradient norms\n",
    "        ax3.plot(layers[:-1], grad_norms[:-1], 'o-', color=color, label=init_name, linewidth=2, markersize=6)\n",
    "    \n",
    "    # Activation standard deviations\n",
    "    ax1.set_xlabel('Layer Number')\n",
    "    ax1.set_ylabel('Activation Standard Deviation')\n",
    "    ax1.set_title('Activation Variance Through Layers')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Target (≈1.0)')\n",
    "    \n",
    "    # Activation means (should be close to 0)\n",
    "    ax2.set_xlabel('Layer Number')\n",
    "    ax2.set_ylabel('|Activation Mean|')\n",
    "    ax2.set_title('Activation Mean Drift')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Gradient norms\n",
    "    ax3.set_xlabel('Layer Number')\n",
    "    ax3.set_ylabel('Gradient Norm')\n",
    "    ax3.set_title('Gradient Flow Through Layers')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Weight distribution comparison\n",
    "    # Create sample weights for each initialization\n",
    "    sample_size = 10000\n",
    "    \n",
    "    for i, (init_name, _) in enumerate(results.items()):\n",
    "        color = colors[i]\n",
    "        \n",
    "        if 'Too Small' in init_name:\n",
    "            weights = torch.normal(0, 0.01, (sample_size,))\n",
    "        elif 'Too Large' in init_name:\n",
    "            weights = torch.normal(0, 1.0, (sample_size,))\n",
    "        elif 'Xavier' in init_name:\n",
    "            # Xavier: std = sqrt(2 / (fan_in + fan_out))\n",
    "            fan_in, fan_out = 512, 512\n",
    "            std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "            weights = torch.normal(0, std, (sample_size,))\n",
    "        elif 'He' in init_name:\n",
    "            # He: std = sqrt(2 / fan_in)\n",
    "            fan_in = 512\n",
    "            std = math.sqrt(2.0 / fan_in)\n",
    "            weights = torch.normal(0, std, (sample_size,))\n",
    "        \n",
    "        ax4.hist(weights.numpy(), bins=50, alpha=0.6, color=color, label=init_name, density=True)\n",
    "    \n",
    "    ax4.set_xlabel('Weight Value')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Weight Distribution Comparison')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis summary\n",
    "    print(\"\\n📊 ANALYSIS SUMMARY:\")\n",
    "    print(\"═\" * 50)\n",
    "    \n",
    "    for init_name, data in results.items():\n",
    "        final_act_std = data['activations'][-1]['std']\n",
    "        avg_grad_norm = np.mean([g['norm'] for g in data['gradients']])\n",
    "        \n",
    "        print(f\"\\n{init_name}:\")\n",
    "        print(f\"  Final activation std: {final_act_std:.4f}\")\n",
    "        print(f\"  Average gradient norm: {avg_grad_norm:.4f}\")\n",
    "        \n",
    "        # Diagnosis\n",
    "        if final_act_std < 0.1:\n",
    "            print(f\"  ⚠️  Vanishing activations - network may not learn\")\n",
    "        elif final_act_std > 10:\n",
    "            print(f\"  ⚠️  Exploding activations - unstable training\")\n",
    "        else:\n",
    "            print(f\"  ✅ Healthy activation variance\")\n",
    "        \n",
    "        if avg_grad_norm < 1e-6:\n",
    "            print(f\"  ⚠️  Vanishing gradients - slow/no learning\")\n",
    "        elif avg_grad_norm > 1:\n",
    "            print(f\"  ⚠️  Large gradients - may need clipping\")\n",
    "        else:\n",
    "            print(f\"  ✅ Healthy gradient flow\")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_initialization_effects(init_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Xavier/Glorot Initialization: The Mathematical Foundation\n",
    "\n",
    "Xavier initialization solves the variance preservation problem mathematically. For a linear layer with `fan_in` inputs and `fan_out` outputs:\n",
    "\n",
    "**Forward Pass Variance Preservation:**\n",
    "$$\\text{Var}(y) = \\text{fan\\_in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)$$\n",
    "\n",
    "**Backward Pass Gradient Preservation:**\n",
    "$$\\text{Var}(\\frac{\\partial L}{\\partial x}) = \\text{fan\\_out} \\cdot \\text{Var}(w) \\cdot \\text{Var}(\\frac{\\partial L}{\\partial y})$$\n",
    "\n",
    "For both to equal 1, we need: $\\text{Var}(w) = \\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_xavier_theory():\n",
    "    \"\"\"Demonstrate the mathematical theory behind Xavier initialization.\"\"\"\n",
    "    \n",
    "    print(\"🧮 XAVIER INITIALIZATION THEORY\")\n",
    "    print(\"═\" * 40)\n",
    "    \n",
    "    # Test different layer sizes\n",
    "    layer_configs = [\n",
    "        (100, 100, \"Square layer\"),\n",
    "        (512, 128, \"Bottleneck layer\"), \n",
    "        (128, 512, \"Expansion layer\"),\n",
    "        (1024, 1024, \"Large layer\")\n",
    "    ]\n",
    "    \n",
    "    batch_size = 1000\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for fan_in, fan_out, description in layer_configs:\n",
    "        print(f\"\\n📐 {description}: {fan_in} → {fan_out}\")\n",
    "        \n",
    "        # Xavier variance calculation\n",
    "        xavier_var = 2.0 / (fan_in + fan_out)\n",
    "        xavier_std = math.sqrt(xavier_var)\n",
    "        \n",
    "        print(f\"  Xavier variance: {xavier_var:.6f}\")\n",
    "        print(f\"  Xavier std: {xavier_std:.6f}\")\n",
    "        \n",
    "        # Create layer with Xavier initialization\n",
    "        layer = nn.Linear(fan_in, fan_out)\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        # Test forward pass variance preservation\n",
    "        x = torch.randn(batch_size, fan_in)\n",
    "        input_var = x.var().item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y = layer(x)\n",
    "            output_var = y.var().item()\n",
    "        \n",
    "        # Test backward pass (gradient preservation)\n",
    "        y.requires_grad_(True)\n",
    "        loss = y.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_var = layer.weight.grad.var().item()\n",
    "        \n",
    "        # Theoretical vs actual variance\n",
    "        theoretical_output_var = input_var  # Should preserve variance\n",
    "        variance_ratio = output_var / input_var\n",
    "        \n",
    "        print(f\"  Input variance: {input_var:.4f}\")\n",
    "        print(f\"  Output variance: {output_var:.4f}\")\n",
    "        print(f\"  Variance ratio: {variance_ratio:.4f} (target: 1.0)\")\n",
    "        print(f\"  Gradient variance: {grad_var:.6f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'config': description,\n",
    "            'fan_in': fan_in,\n",
    "            'fan_out': fan_out,\n",
    "            'xavier_var': xavier_var,\n",
    "            'variance_ratio': variance_ratio,\n",
    "            'gradient_var': grad_var\n",
    "        })\n",
    "    \n",
    "    # Visualize variance preservation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Variance preservation\n",
    "    configs = [r['config'] for r in results]\n",
    "    variance_ratios = [r['variance_ratio'] for r in results]\n",
    "    \n",
    "    ax1.bar(configs, variance_ratios, color='skyblue', alpha=0.7)\n",
    "    ax1.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Perfect Preservation')\n",
    "    ax1.set_ylabel('Output/Input Variance Ratio')\n",
    "    ax1.set_title('Xavier Initialization: Variance Preservation')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Xavier variance vs layer shape\n",
    "    fan_ratios = [r['fan_out'] / r['fan_in'] for r in results]\n",
    "    xavier_vars = [r['xavier_var'] for r in results]\n",
    "    \n",
    "    ax2.scatter(fan_ratios, xavier_vars, s=100, color='green', alpha=0.7)\n",
    "    for i, result in enumerate(results):\n",
    "        ax2.annotate(result['config'], (fan_ratios[i], xavier_vars[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Fan Out / Fan In Ratio')\n",
    "    ax2.set_ylabel('Xavier Variance')\n",
    "    ax2.set_title('Xavier Variance vs Layer Shape')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🎯 Key Insights:\")\n",
    "    print(\"• Xavier initialization keeps variance ratios close to 1.0\")\n",
    "    print(\"• Works regardless of layer shape (bottleneck, expansion, square)\")\n",
    "    print(\"• Preserves both forward and backward signal strength\")\n",
    "    print(\"• Essential for training deep networks (>6 layers)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demonstrate Xavier theory\n",
    "xavier_results = demonstrate_xavier_theory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. He Initialization: For ReLU and Modern Activations\n",
    "\n",
    "While Xavier works well for symmetric activations (tanh, sigmoid), ReLU activations break the symmetry assumption. He initialization accounts for this:\n",
    "\n",
    "**For ReLU:** $\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}}$ (since ReLU zeros out half the neurons)\n",
    "\n",
    "Let's compare Xavier vs He for different activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_initialization_with_activations():\n",
    "    \"\"\"Compare Xavier vs He initialization with different activation functions.\"\"\"\n",
    "    \n",
    "    print(\"⚡ INITIALIZATION VS ACTIVATION FUNCTIONS\")\n",
    "    print(\"═\" * 50)\n",
    "    \n",
    "    # Different activation functions\n",
    "    activations = {\n",
    "        'Tanh': torch.tanh,\n",
    "        'ReLU': torch.relu,\n",
    "        'GELU': torch.nn.functional.gelu,\n",
    "        'Swish/SiLU': torch.nn.functional.silu\n",
    "    }\n",
    "    \n",
    "    # Initialization methods\n",
    "    initializations = ['Xavier', 'He']\n",
    "    \n",
    "    # Network parameters\n",
    "    input_size = 512\n",
    "    hidden_size = 512\n",
    "    num_layers = 8\n",
    "    batch_size = 1000\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for act_name, activation_fn in activations.items():\n",
    "        results[act_name] = {}\n",
    "        \n",
    "        for init_name in initializations:\n",
    "            print(f\"\\n🧪 Testing {init_name} + {act_name}\")\n",
    "            \n",
    "            # Create network\n",
    "            layers = []\n",
    "            for i in range(num_layers):\n",
    "                layer = nn.Linear(hidden_size if i > 0 else input_size, hidden_size)\n",
    "                \n",
    "                # Apply initialization\n",
    "                if init_name == 'Xavier':\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                else:  # He\n",
    "                    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                \n",
    "                nn.init.zeros_(layer.bias)\n",
    "                layers.append(layer)\n",
    "            \n",
    "            # Forward pass\n",
    "            x = torch.randn(batch_size, input_size)\n",
    "            activations_list = []\n",
    "            current_input = x\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, layer in enumerate(layers):\n",
    "                    current_input = layer(current_input)\n",
    "                    if i < len(layers) - 1:  # Don't apply activation to final layer\n",
    "                        current_input = activation_fn(current_input)\n",
    "                    activations_list.append(current_input.clone())\n",
    "            \n",
    "            # Analyze activation statistics\n",
    "            layer_means = [act.mean().item() for act in activations_list]\n",
    "            layer_stds = [act.std().item() for act in activations_list]\n",
    "            layer_dead_neurons = []\n",
    "            \n",
    "            # Count dead neurons (for ReLU-like activations)\n",
    "            for act in activations_list[:-1]:  # Exclude final layer\n",
    "                if act_name in ['ReLU']:\n",
    "                    dead_ratio = (act == 0).float().mean().item()\n",
    "                    layer_dead_neurons.append(dead_ratio)\n",
    "                else:\n",
    "                    layer_dead_neurons.append(0.0)\n",
    "            \n",
    "            results[act_name][init_name] = {\n",
    "                'means': layer_means,\n",
    "                'stds': layer_stds,\n",
    "                'dead_neurons': layer_dead_neurons,\n",
    "                'final_std': layer_stds[-1]\n",
    "            }\n",
    "            \n",
    "            print(f\"   Final activation std: {layer_stds[-1]:.4f}\")\n",
    "            if act_name == 'ReLU' and layer_dead_neurons:\n",
    "                avg_dead = np.mean(layer_dead_neurons)\n",
    "                print(f\"   Average dead neurons: {avg_dead:.2%}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (act_name, act_results) in enumerate(results.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        for init_name, data in act_results.items():\n",
    "            layers = list(range(len(data['stds'])))\n",
    "            color = 'blue' if init_name == 'Xavier' else 'red'\n",
    "            linestyle = '-' if init_name == 'Xavier' else '--'\n",
    "            \n",
    "            ax.plot(layers, data['stds'], color=color, linestyle=linestyle, \n",
    "                   linewidth=2, marker='o', markersize=4, \n",
    "                   label=f\"{init_name} (final: {data['final_std']:.3f})\")\n",
    "        \n",
    "        ax.set_xlabel('Layer Number')\n",
    "        ax.set_ylabel('Activation Standard Deviation')\n",
    "        ax.set_title(f'{act_name} Activation')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # Add horizontal line at 1.0 for reference\n",
    "        ax.axhline(y=1.0, color='gray', linestyle=':', alpha=0.7, label='Target (1.0)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(\"\\n📊 INITIALIZATION RECOMMENDATIONS:\")\n",
    "    print(\"═\" * 40)\n",
    "    \n",
    "    for act_name, act_results in results.items():\n",
    "        xavier_final = act_results['Xavier']['final_std']\n",
    "        he_final = act_results['He']['final_std']\n",
    "        \n",
    "        print(f\"\\n{act_name}:\")\n",
    "        print(f\"  Xavier final std: {xavier_final:.4f}\")\n",
    "        print(f\"  He final std: {he_final:.4f}\")\n",
    "        \n",
    "        # Recommendation\n",
    "        if act_name in ['ReLU']:\n",
    "            if he_final > xavier_final:\n",
    "                print(f\"  ✅ Recommendation: Use He initialization\")\n",
    "            else:\n",
    "                print(f\"  ⚠️  He should work better, check implementation\")\n",
    "        elif act_name in ['Tanh']:\n",
    "            if abs(xavier_final - 1.0) < abs(he_final - 1.0):\n",
    "                print(f\"  ✅ Recommendation: Use Xavier initialization\")\n",
    "            else:\n",
    "                print(f\"  ⚠️  Xavier should work better\")\n",
    "        else:  # GELU, Swish\n",
    "            better = 'He' if he_final > xavier_final else 'Xavier'\n",
    "            print(f\"  ✅ Recommendation: {better} works better for {act_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare initialization methods with different activations\n",
    "activation_results = compare_initialization_with_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Transformer-Specific Initialization Considerations\n",
    "\n",
    "Transformers have unique architectural elements that require special initialization considerations:\n",
    "\n",
    "1. **Multi-head attention layers** - Multiple parallel projections\n",
    "2. **Residual connections** - Skip connections that affect gradient flow\n",
    "3. **Layer normalization** - Changes the activation statistics\n",
    "4. **Very deep networks** - Modern transformers have 24+ layers\n",
    "\n",
    "Let's see how to properly initialize transformer components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.attention import MultiHeadAttention\n",
    "from src.model.feedforward import FeedForward\n",
    "\n",
    "class TransformerBlockInitialized(nn.Module):\n",
    "    \"\"\"Transformer block with proper initialization strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, \n",
    "                 init_strategy: str = 'standard', dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.init_strategy = init_strategy\n",
    "        \n",
    "        # Components\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Apply initialization\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Apply transformer-specific weight initialization.\"\"\"\n",
    "        \n",
    "        if self.init_strategy == 'standard':\n",
    "            # Standard Xavier/Glorot for all linear layers\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "        \n",
    "        elif self.init_strategy == 'scaled':\n",
    "            # Scaled initialization for residual connections\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    # Scale down the output projection in attention and FFN\n",
    "                    if hasattr(module, '_is_output_projection'):\n",
    "                        module.weight.data *= 0.5  # Scale down for residual\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "        \n",
    "        elif self.init_strategy == 'small_init':\n",
    "            # Small initialization for very deep networks\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    # Smaller initial weights\n",
    "                    nn.init.normal_(module.weight, std=0.02)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Layer norm initialization (standard)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm transformer block\n",
    "        # Attention with residual\n",
    "        normed = self.norm1(x)\n",
    "        attn_out = self.attention(normed, normed, normed, mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        normed = self.norm2(x)\n",
    "        ff_out = self.feed_forward(normed)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def test_transformer_initialization():\n",
    "    \"\"\"Test different initialization strategies for transformer blocks.\"\"\"\n",
    "    \n",
    "    print(\"🏗️ TRANSFORMER INITIALIZATION STRATEGIES\")\n",
    "    print(\"═\" * 50)\n",
    "    \n",
    "    # Model parameters\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    num_layers = 12\n",
    "    batch_size = 32\n",
    "    seq_len = 128\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    strategies = ['standard', 'scaled', 'small_init']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n🧪 Testing {strategy} initialization:\")\n",
    "        \n",
    "        # Create transformer stack\n",
    "        blocks = nn.ModuleList([\n",
    "            TransformerBlockInitialized(d_model, n_heads, d_ff, strategy)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Input embeddings (also need initialization)\n",
    "        embedding = nn.Embedding(1000, d_model)\n",
    "        if strategy == 'small_init':\n",
    "            nn.init.normal_(embedding.weight, std=0.02)\n",
    "        else:\n",
    "            nn.init.normal_(embedding.weight, std=0.1)\n",
    "        \n",
    "        # Create input\n",
    "        input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "        x = embedding(input_ids)\n",
    "        \n",
    "        # Forward pass through all layers\n",
    "        layer_activations = []\n",
    "        current_input = x\n",
    "        \n",
    "        for i, block in enumerate(blocks):\n",
    "            current_input = block(current_input)\n",
    "            layer_activations.append(current_input.clone())\n",
    "        \n",
    "        # Backward pass for gradient analysis\n",
    "        output = layer_activations[-1]\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Analyze statistics\n",
    "        activation_stats = []\n",
    "        gradient_stats = []\n",
    "        \n",
    "        for i, (activation, block) in enumerate(zip(layer_activations, blocks)):\n",
    "            # Activation statistics\n",
    "            act_mean = activation.mean().item()\n",
    "            act_std = activation.std().item()\n",
    "            \n",
    "            activation_stats.append({\n",
    "                'layer': i,\n",
    "                'mean': act_mean,\n",
    "                'std': act_std\n",
    "            })\n",
    "            \n",
    "            # Gradient statistics (attention weights)\n",
    "            attn_grad_norm = 0\n",
    "            ff_grad_norm = 0\n",
    "            \n",
    "            for name, param in block.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if 'attention' in name:\n",
    "                        attn_grad_norm += param.grad.norm().item() ** 2\n",
    "                    elif 'feed_forward' in name:\n",
    "                        ff_grad_norm += param.grad.norm().item() ** 2\n",
    "            \n",
    "            gradient_stats.append({\n",
    "                'layer': i,\n",
    "                'attn_grad_norm': math.sqrt(attn_grad_norm),\n",
    "                'ff_grad_norm': math.sqrt(ff_grad_norm)\n",
    "            })\n",
    "        \n",
    "        results[strategy] = {\n",
    "            'activations': activation_stats,\n",
    "            'gradients': gradient_stats\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        final_std = activation_stats[-1]['std']\n",
    "        avg_attn_grad = np.mean([g['attn_grad_norm'] for g in gradient_stats])\n",
    "        avg_ff_grad = np.mean([g['ff_grad_norm'] for g in gradient_stats])\n",
    "        \n",
    "        print(f\"   Final activation std: {final_std:.4f}\")\n",
    "        print(f\"   Avg attention grad norm: {avg_attn_grad:.4f}\")\n",
    "        print(f\"   Avg feed-forward grad norm: {avg_ff_grad:.4f}\")\n",
    "        \n",
    "        # Clear gradients\n",
    "        for block in blocks:\n",
    "            block.zero_grad()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test transformer initialization\n",
    "transformer_results = test_transformer_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Visualizing Transformer Initialization Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_transformer_initialization(results):\n",
    "    \"\"\"Visualize the effects of different initialization strategies on transformers.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    colors = {'standard': 'blue', 'scaled': 'green', 'small_init': 'red'}\n",
    "    \n",
    "    for strategy, data in results.items():\n",
    "        color = colors[strategy]\n",
    "        \n",
    "        # Extract data\n",
    "        layers = [stat['layer'] for stat in data['activations']]\n",
    "        act_stds = [stat['std'] for stat in data['activations']]\n",
    "        act_means = [abs(stat['mean']) for stat in data['activations']]\n",
    "        attn_grads = [stat['attn_grad_norm'] for stat in data['gradients']]\n",
    "        ff_grads = [stat['ff_grad_norm'] for stat in data['gradients']]\n",
    "        \n",
    "        # Plot activation standard deviations\n",
    "        ax1.plot(layers, act_stds, 'o-', color=color, label=strategy, linewidth=2, markersize=4)\n",
    "        \n",
    "        # Plot activation means\n",
    "        ax2.plot(layers, act_means, 'o-', color=color, label=strategy, linewidth=2, markersize=4)\n",
    "        \n",
    "        # Plot attention gradient norms\n",
    "        ax3.plot(layers, attn_grads, 'o-', color=color, label=f'{strategy} (attn)', linewidth=2, markersize=4)\n",
    "        \n",
    "        # Plot feed-forward gradient norms\n",
    "        ax4.plot(layers, ff_grads, 's-', color=color, label=f'{strategy} (ff)', linewidth=2, markersize=4, alpha=0.7)\n",
    "    \n",
    "    # Activation standard deviations\n",
    "    ax1.set_xlabel('Layer Number')\n",
    "    ax1.set_ylabel('Activation Standard Deviation')\n",
    "    ax1.set_title('Activation Variance Through Transformer Layers')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Activation means\n",
    "    ax2.set_xlabel('Layer Number')\n",
    "    ax2.set_ylabel('|Activation Mean|')\n",
    "    ax2.set_title('Activation Mean Drift')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Attention gradient norms\n",
    "    ax3.set_xlabel('Layer Number')\n",
    "    ax3.set_ylabel('Gradient Norm')\n",
    "    ax3.set_title('Attention Layer Gradient Flow')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Feed-forward gradient norms\n",
    "    ax4.set_xlabel('Layer Number')\n",
    "    ax4.set_ylabel('Gradient Norm')\n",
    "    ax4.set_title('Feed-Forward Layer Gradient Flow')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis and recommendations\n",
    "    print(\"\\n🎯 TRANSFORMER INITIALIZATION ANALYSIS:\")\n",
    "    print(\"═\" * 50)\n",
    "    \n",
    "    for strategy, data in results.items():\n",
    "        final_std = data['activations'][-1]['std']\n",
    "        avg_attn_grad = np.mean([g['attn_grad_norm'] for g in data['gradients']])\n",
    "        avg_ff_grad = np.mean([g['ff_grad_norm'] for g in data['gradients']])\n",
    "        \n",
    "        print(f\"\\n{strategy.upper()} Strategy:\")\n",
    "        print(f\"  Final activation std: {final_std:.4f}\")\n",
    "        print(f\"  Avg attention grad: {avg_attn_grad:.4f}\")\n",
    "        print(f\"  Avg FF grad: {avg_ff_grad:.4f}\")\n",
    "        \n",
    "        # Health assessment\n",
    "        if 0.5 <= final_std <= 2.0:\n",
    "            print(f\"  ✅ Healthy activation variance\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Suboptimal activation variance\")\n",
    "        \n",
    "        if 1e-4 <= avg_attn_grad <= 1.0:\n",
    "            print(f\"  ✅ Good attention gradient flow\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Attention gradient issues\")\n",
    "    \n",
    "    print(\"\\n📋 RECOMMENDATIONS:\")\n",
    "    print(\"• Standard Xavier: Good baseline for most transformers\")\n",
    "    print(\"• Scaled initialization: Better for very deep networks (>24 layers)\")\n",
    "    print(\"• Small initialization: Use for large models (GPT-3 scale)\")\n",
    "    print(\"• Always use proper layer norm initialization (weight=1, bias=0)\")\n",
    "    print(\"• Monitor gradients during training - add gradient clipping if needed\")\n",
    "\n",
    "# Visualize transformer initialization results\n",
    "visualize_transformer_initialization(transformer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Common Initialization Failures and Debugging\n",
    "\n",
    "Let's explore common initialization problems and how to diagnose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_initialization_failures():\n",
    "    \"\"\"Show common initialization failures and their symptoms.\"\"\"\n",
    "    \n",
    "    print(\"🚨 COMMON INITIALIZATION FAILURES\")\n",
    "    print(\"═\" * 40)\n",
    "    \n",
    "    # Common failure modes\n",
    "    failure_modes = {\n",
    "        'Vanishing Gradients': {\n",
    "            'init_std': 0.001,\n",
    "            'description': 'Weights too small, gradients vanish in deep networks',\n",
    "            'symptoms': ['Loss barely decreases', 'Gradients near zero', 'Slow/no learning']\n",
    "        },\n",
    "        'Exploding Gradients': {\n",
    "            'init_std': 2.0,\n",
    "            'description': 'Weights too large, gradients explode',\n",
    "            'symptoms': ['Loss oscillates wildly', 'NaN values', 'Training instability']\n",
    "        },\n",
    "        'Dead ReLU': {\n",
    "            'init_std': 1.0,\n",
    "            'bias': -1.0,  # Negative bias kills ReLU neurons\n",
    "            'description': 'Neurons stuck at zero due to poor initialization',\n",
    "            'symptoms': ['Many zero activations', 'Poor learning', 'Reduced capacity']\n",
    "        },\n",
    "        'Saturation': {\n",
    "            'init_std': 3.0,\n",
    "            'activation': 'tanh',\n",
    "            'description': 'Activations saturate at extremes',\n",
    "            'symptoms': ['Gradients near zero', 'Slow learning', 'Poor convergence']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Network parameters\n",
    "    input_size = 256\n",
    "    hidden_size = 256\n",
    "    num_layers = 6\n",
    "    batch_size = 100\n",
    "    \n",
    "    failure_results = {}\n",
    "    \n",
    "    for failure_name, config in failure_modes.items():\n",
    "        print(f\"\\n🔍 Simulating {failure_name}:\")\n",
    "        print(f\"   {config['description']}\")\n",
    "        \n",
    "        # Create network with problematic initialization\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nn.Linear(hidden_size if i > 0 else input_size, hidden_size)\n",
    "            \n",
    "            # Apply problematic initialization\n",
    "            nn.init.normal_(layer.weight, mean=0, std=config['init_std'])\n",
    "            \n",
    "            # Set bias\n",
    "            if 'bias' in config:\n",
    "                nn.init.constant_(layer.bias, config['bias'])\n",
    "            else:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "            \n",
    "            layers.append(layer)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "        activations = []\n",
    "        current_input = x\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            current_input = layer(current_input)\n",
    "            \n",
    "            if i < len(layers) - 1:  # Apply activation\n",
    "                if config.get('activation') == 'tanh':\n",
    "                    current_input = torch.tanh(current_input)\n",
    "                else:  # Default ReLU\n",
    "                    current_input = torch.relu(current_input)\n",
    "            \n",
    "            activations.append(current_input.clone())\n",
    "        \n",
    "        # Backward pass\n",
    "        output = activations[-1]\n",
    "        loss = (output ** 2).mean()  # L2 loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Analyze failure symptoms\n",
    "        symptoms = {}\n",
    "        \n",
    "        # Gradient analysis\n",
    "        grad_norms = []\n",
    "        for layer in layers:\n",
    "            if layer.weight.grad is not None:\n",
    "                grad_norms.append(layer.weight.grad.norm().item())\n",
    "        \n",
    "        symptoms['avg_grad_norm'] = np.mean(grad_norms) if grad_norms else 0\n",
    "        symptoms['grad_ratio'] = max(grad_norms) / min(grad_norms) if len(grad_norms) > 1 and min(grad_norms) > 0 else float('inf')\n",
    "        \n",
    "        # Activation analysis\n",
    "        final_activation = activations[-1]\n",
    "        symptoms['final_mean'] = final_activation.mean().item()\n",
    "        symptoms['final_std'] = final_activation.std().item()\n",
    "        symptoms['has_nan'] = torch.isnan(final_activation).any().item()\n",
    "        symptoms['has_inf'] = torch.isinf(final_activation).any().item()\n",
    "        \n",
    "        # Dead neuron analysis (for ReLU)\n",
    "        if config.get('activation') != 'tanh':\n",
    "            dead_neurons = []\n",
    "            for act in activations[:-1]:  # Exclude final layer\n",
    "                dead_ratio = (act == 0).float().mean().item()\n",
    "                dead_neurons.append(dead_ratio)\n",
    "            symptoms['avg_dead_neurons'] = np.mean(dead_neurons) if dead_neurons else 0\n",
    "        \n",
    "        # Saturation analysis (for tanh)\n",
    "        if config.get('activation') == 'tanh':\n",
    "            saturated = []\n",
    "            for act in activations[:-1]:\n",
    "                # Consider neurons saturated if |activation| > 0.9\n",
    "                sat_ratio = (torch.abs(act) > 0.9).float().mean().item()\n",
    "                saturated.append(sat_ratio)\n",
    "            symptoms['avg_saturated'] = np.mean(saturated) if saturated else 0\n",
    "        \n",
    "        failure_results[failure_name] = {\n",
    "            'config': config,\n",
    "            'symptoms': symptoms,\n",
    "            'activations': [act.detach() for act in activations],\n",
    "            'grad_norms': grad_norms\n",
    "        }\n",
    "        \n",
    "        # Print diagnosis\n",
    "        print(f\"   Avg gradient norm: {symptoms['avg_grad_norm']:.2e}\")\n",
    "        print(f\"   Final activation std: {symptoms['final_std']:.4f}\")\n",
    "        \n",
    "        if symptoms['has_nan'] or symptoms['has_inf']:\n",
    "            print(f\"   ⚠️  NaN/Inf detected!\")\n",
    "        \n",
    "        if 'avg_dead_neurons' in symptoms and symptoms['avg_dead_neurons'] > 0.5:\n",
    "            print(f\"   ⚠️  {symptoms['avg_dead_neurons']:.1%} dead neurons\")\n",
    "        \n",
    "        if 'avg_saturated' in symptoms and symptoms['avg_saturated'] > 0.3:\n",
    "            print(f\"   ⚠️  {symptoms['avg_saturated']:.1%} saturated neurons\")\n",
    "        \n",
    "        # Clear gradients\n",
    "        for layer in layers:\n",
    "            layer.zero_grad()\n",
    "    \n",
    "    return failure_results\n",
    "\n",
    "def create_debugging_guide(failure_results):\n",
    "    \"\"\"Create a debugging guide for initialization problems.\"\"\"\n",
    "    \n",
    "    print(\"\\n🛠️ DEBUGGING GUIDE: INITIALIZATION PROBLEMS\")\n",
    "    print(\"═\" * 60)\n",
    "    \n",
    "    # Create diagnostic flowchart\n",
    "    diagnostics = {\n",
    "        \"Gradient norm < 1e-6\": {\n",
    "            \"Problem\": \"Vanishing gradients\",\n",
    "            \"Solutions\": [\n",
    "                \"Increase initialization scale (Xavier/He)\",\n",
    "                \"Use residual connections\",\n",
    "                \"Switch to pre-norm architecture\",\n",
    "                \"Reduce network depth\"\n",
    "            ]\n",
    "        },\n",
    "        \"Gradient norm > 10\": {\n",
    "            \"Problem\": \"Exploding gradients\",\n",
    "            \"Solutions\": [\n",
    "                \"Reduce initialization scale\",\n",
    "                \"Apply gradient clipping\",\n",
    "                \"Use smaller learning rate\",\n",
    "                \"Add more regularization\"\n",
    "            ]\n",
    "        },\n",
    "        \"Many zero activations (>50%)\": {\n",
    "            \"Problem\": \"Dead ReLU neurons\",\n",
    "            \"Solutions\": [\n",
    "                \"Use positive bias initialization\",\n",
    "                \"Switch to LeakyReLU or ELU\",\n",
    "                \"Reduce initialization scale\",\n",
    "                \"Use BatchNorm/LayerNorm\"\n",
    "            ]\n",
    "        },\n",
    "        \"Activations saturated (>30%)\": {\n",
    "            \"Problem\": \"Activation saturation\",\n",
    "            \"Solutions\": [\n",
    "                \"Reduce initialization scale\",\n",
    "                \"Use ReLU instead of tanh/sigmoid\",\n",
    "                \"Add normalization layers\",\n",
    "                \"Reduce network width\"\n",
    "            ]\n",
    "        },\n",
    "        \"NaN/Inf in activations\": {\n",
    "            \"Problem\": \"Numerical instability\",\n",
    "            \"Solutions\": [\n",
    "                \"Much smaller initialization\",\n",
    "                \"Add gradient clipping\",\n",
    "                \"Use lower learning rate\",\n",
    "                \"Check for bugs in forward pass\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for condition, info in diagnostics.items():\n",
    "        print(f\"\\n🔍 IF: {condition}\")\n",
    "        print(f\"   Problem: {info['Problem']}\")\n",
    "        print(f\"   Solutions:\")\n",
    "        for solution in info['Solutions']:\n",
    "            print(f\"     • {solution}\")\n",
    "    \n",
    "    print(\"\\n🎯 GENERAL BEST PRACTICES:\")\n",
    "    print(\"─\" * 30)\n",
    "    print(\"1. Always use Xavier/He initialization\")\n",
    "    print(\"2. Initialize biases to zero (except specific cases)\")\n",
    "    print(\"3. Monitor gradients during training\")\n",
    "    print(\"4. Use proper activation functions for your task\")\n",
    "    print(\"5. Add normalization layers (LayerNorm/BatchNorm)\")\n",
    "    print(\"6. Start with smaller networks and scale up\")\n",
    "    print(\"7. Test initialization on toy problems first\")\n",
    "\n",
    "# Demonstrate common failures\n",
    "failure_results = demonstrate_initialization_failures()\n",
    "\n",
    "# Create debugging guide\n",
    "create_debugging_guide(failure_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Practical Implementation: Initialization Utils\n",
    "\n",
    "Let's create practical utilities for proper transformer initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitializationUtils:\n",
    "    \"\"\"Utility class for proper weight initialization in transformers.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_linear_layer(layer: nn.Linear, method: str = 'xavier_uniform', gain: float = 1.0):\n",
    "        \"\"\"Initialize a linear layer with specified method.\"\"\"\n",
    "        if method == 'xavier_uniform':\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=gain)\n",
    "        elif method == 'xavier_normal':\n",
    "            nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "        elif method == 'kaiming_uniform':\n",
    "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        elif method == 'kaiming_normal':\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        elif method == 'small_normal':\n",
    "            nn.init.normal_(layer.weight, std=0.02)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown initialization method: {method}\")\n",
    "        \n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_transformer_weights(model: nn.Module, \n",
    "                                init_method: str = 'xavier_uniform',\n",
    "                                small_init_layers: List[str] = None,\n",
    "                                residual_scaling: bool = False):\n",
    "        \"\"\"Initialize all weights in a transformer model.\"\"\"\n",
    "        \n",
    "        small_init_layers = small_init_layers or []\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Check if this should use small initialization\n",
    "                use_small_init = any(layer_name in name for layer_name in small_init_layers)\n",
    "                \n",
    "                if use_small_init:\n",
    "                    InitializationUtils.init_linear_layer(module, 'small_normal')\n",
    "                    if residual_scaling:\n",
    "                        # Scale down output projections for residual connections\n",
    "                        module.weight.data *= 0.5\n",
    "                else:\n",
    "                    InitializationUtils.init_linear_layer(module, init_method)\n",
    "            \n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "            \n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_initialization(model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Analyze the initialization quality of a model.\"\"\"\n",
    "        \n",
    "        # Forward pass with hooks to capture activations\n",
    "        activations = {}\n",
    "        \n",
    "        def activation_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    activations[name] = output.detach().clone()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.MultiheadAttention)):\n",
    "                hook = module.register_forward_hook(activation_hook(name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        # Analyze activations\n",
    "        analysis = {}\n",
    "        for name, activation in activations.items():\n",
    "            analysis[name] = {\n",
    "                'mean': activation.mean().item(),\n",
    "                'std': activation.std().item(),\n",
    "                'min': activation.min().item(),\n",
    "                'max': activation.max().item(),\n",
    "                'has_nan': torch.isnan(activation).any().item(),\n",
    "                'has_inf': torch.isinf(activation).any().item()\n",
    "            }\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_initialization_report(analysis: Dict):\n",
    "        \"\"\"Print a detailed initialization analysis report.\"\"\"\n",
    "        \n",
    "        print(\"📊 INITIALIZATION ANALYSIS REPORT\")\n",
    "        print(\"═\" * 50)\n",
    "        \n",
    "        healthy_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for name, stats in analysis.items():\n",
    "            total_count += 1\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Mean: {stats['mean']:8.4f}\")\n",
    "            print(f\"  Std:  {stats['std']:8.4f}\")\n",
    "            print(f\"  Range: [{stats['min']:6.2f}, {stats['max']:6.2f}]\")\n",
    "            \n",
    "            # Health assessment\n",
    "            issues = []\n",
    "            \n",
    "            if stats['has_nan']:\n",
    "                issues.append(\"NaN values\")\n",
    "            if stats['has_inf']:\n",
    "                issues.append(\"Inf values\")\n",
    "            if abs(stats['mean']) > 0.1:\n",
    "                issues.append(\"Mean too large\")\n",
    "            if stats['std'] < 0.1 or stats['std'] > 10:\n",
    "                issues.append(\"Poor variance\")\n",
    "            \n",
    "            if issues:\n",
    "                print(f\"  ⚠️  Issues: {', '.join(issues)}\")\n",
    "            else:\n",
    "                print(f\"  ✅ Healthy\")\n",
    "                healthy_count += 1\n",
    "        \n",
    "        print(f\"\\n🎯 SUMMARY: {healthy_count}/{total_count} layers healthy\")\n",
    "        \n",
    "        if healthy_count == total_count:\n",
    "            print(\"✅ Initialization looks good!\")\n",
    "        elif healthy_count > total_count * 0.8:\n",
    "            print(\"⚠️  Mostly good, minor issues\")\n",
    "        else:\n",
    "            print(\"❌ Initialization needs improvement\")\n",
    "\n",
    "# Example usage\n",
    "def test_initialization_utils():\n",
    "    \"\"\"Test the initialization utilities.\"\"\"\n",
    "    \n",
    "    print(\"🧪 TESTING INITIALIZATION UTILITIES\")\n",
    "    print(\"═\" * 40)\n",
    "    \n",
    "    # Create a simple transformer-like model\n",
    "    class SimpleTransformer(nn.Module):\n",
    "        def __init__(self, d_model: int, n_heads: int, d_ff: int, n_layers: int):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(1000, d_model)\n",
    "            \n",
    "            self.layers = nn.ModuleList()\n",
    "            for _ in range(n_layers):\n",
    "                layer = nn.ModuleDict({\n",
    "                    'attention': nn.MultiheadAttention(d_model, n_heads, batch_first=True),\n",
    "                    'ff1': nn.Linear(d_model, d_ff),\n",
    "                    'ff2': nn.Linear(d_ff, d_model),\n",
    "                    'norm1': nn.LayerNorm(d_model),\n",
    "                    'norm2': nn.LayerNorm(d_model)\n",
    "                })\n",
    "                self.layers.append(layer)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                # Attention\n",
    "                normed = layer['norm1'](x)\n",
    "                attn_out, _ = layer['attention'](normed, normed, normed)\n",
    "                x = x + attn_out\n",
    "                \n",
    "                # Feed-forward\n",
    "                normed = layer['norm2'](x)\n",
    "                ff_out = torch.relu(layer['ff1'](normed))\n",
    "                ff_out = layer['ff2'](ff_out)\n",
    "                x = x + ff_out\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleTransformer(d_model=256, n_heads=8, d_ff=1024, n_layers=6)\n",
    "    \n",
    "    # Test different initialization strategies\n",
    "    strategies = [\n",
    "        {\n",
    "            'name': 'Standard Xavier',\n",
    "            'method': 'xavier_uniform',\n",
    "            'small_init_layers': [],\n",
    "            'residual_scaling': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'He + Residual Scaling',\n",
    "            'method': 'kaiming_uniform',\n",
    "            'small_init_layers': ['ff2', 'attention'],\n",
    "            'residual_scaling': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test input\n",
    "    input_ids = torch.randint(0, 1000, (8, 32))  # batch_size=8, seq_len=32\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n🎯 Testing {strategy['name']}:\")\n",
    "        \n",
    "        # Initialize weights\n",
    "        InitializationUtils.init_transformer_weights(\n",
    "            model,\n",
    "            init_method=strategy['method'],\n",
    "            small_init_layers=strategy['small_init_layers'],\n",
    "            residual_scaling=strategy['residual_scaling']\n",
    "        )\n",
    "        \n",
    "        # Analyze initialization\n",
    "        analysis = InitializationUtils.analyze_initialization(model, input_ids)\n",
    "        InitializationUtils.print_initialization_report(analysis)\n",
    "\n",
    "# Test the utilities\n",
    "test_initialization_utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You now understand the critical foundation that makes deep transformers trainable.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Initialization is Critical** - Poor initialization can make models untrainable\n",
    "2. **Xavier/Glorot Rules** - Preserve variance through forward and backward passes\n",
    "3. **He Initialization** - Better for ReLU and modern activations\n",
    "4. **Transformer Specifics** - Residual connections and deep networks need special care\n",
    "5. **Common Failures** - Know how to diagnose vanishing/exploding gradients\n",
    "\n",
    "### Mathematical Foundation:\n",
    "- **Xavier variance**: $\\text{Var}(w) = \\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}$\n",
    "- **He variance**: $\\text{Var}(w) = \\frac{2}{\\text{fan\\_in}}$ (for ReLU)\n",
    "- **Gradient flow**: Monitor $\\|\\nabla_w L\\|$ across layers\n",
    "\n",
    "### Practical Guidelines:\n",
    "- ✅ Use Xavier/He initialization for linear layers\n",
    "- ✅ Initialize biases to zero (usually)\n",
    "- ✅ Use small initialization (0.02 std) for very large models\n",
    "- ✅ Scale down residual projections for deep networks\n",
    "- ✅ Monitor gradients during training\n",
    "- ✅ Add gradient clipping if needed\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand the foundation, you're ready for:\n",
    "- **Advanced attention mechanisms** (KV caching, sparse attention)\n",
    "- **Modern architecture improvements** (RMSNorm, SwiGLU, RoPE)\n",
    "- **Training optimization** (learning rate schedules, gradient clipping)\n",
    "\n",
    "Proper initialization is the invisible foundation that makes everything else possible! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}