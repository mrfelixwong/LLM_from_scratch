{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Generation: The Art and Science of Language\n",
    "\n",
    "Training a model is only half the battle. The real magic happens during generation - turning trained weights into coherent, creative, and controllable text.\n",
    "\n",
    "## The Generation Challenge\n",
    "\n",
    "Your model outputs probabilities over 50,000+ tokens. How do you turn this into meaningful text? The naive approach - always picking the highest probability token - produces boring, repetitive output.\n",
    "\n",
    "## The Physics of Language Generation\n",
    "\n",
    "Text generation is fundamentally about **sampling from probability distributions**. But not all sampling is equal:\n",
    "\n",
    "**Deterministic Sampling**: Always pick highest probability\n",
    "- Predictable but boring\n",
    "- Often gets stuck in loops\n",
    "- No creativity or surprise\n",
    "\n",
    "**Random Sampling**: Pick tokens randomly according to probabilities\n",
    "- Creative but often incoherent\n",
    "- Can generate nonsense\n",
    "- Hard to control quality\n",
    "\n",
    "**Smart Sampling**: Balance creativity with coherence\n",
    "- Temperature scaling for controlled randomness\n",
    "- Top-k and nucleus sampling for quality control\n",
    "- Beam search for structured exploration\n",
    "\n",
    "## The Information Theory Foundation\n",
    "\n",
    "Good text generation requires understanding **entropy** and **surprise**:\n",
    "\n",
    "**Entropy H(p) = -Σ p(x) log p(x)**\n",
    "- Low entropy: Model is confident (deterministic)\n",
    "- High entropy: Model is uncertain (random)\n",
    "- Sweet spot: Controlled uncertainty for creativity\n",
    "\n",
    "**Perplexity = exp(H)**: Measures how \"surprised\" the model is\n",
    "- Lower perplexity = better predictions\n",
    "- But some surprise is needed for interesting text\n",
    "\n",
    "## What You'll Master\n",
    "\n",
    "1. **Temperature scaling** for controlling creativity\n",
    "2. **Top-k and nucleus sampling** for quality control\n",
    "3. **Beam search** for structured exploration\n",
    "4. **Quality metrics** for evaluating generated text\n",
    "5. **Controllable generation** for specific outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Advanced generation laboratory ready! 🎨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temperature: Controlling the Heat of Creativity\n",
    "\n",
    "Temperature is the most fundamental knob for controlling text generation. It transforms the raw probability distribution from your model.\n",
    "\n",
    "### The Mathematical Foundation\n",
    "\n",
    "**Softmax with Temperature**:\n",
    "```\n",
    "p_i = exp(logit_i / T) / Σ_j exp(logit_j / T)\n",
    "```\n",
    "\n",
    "Where T is temperature:\n",
    "- **T = 1**: Normal softmax (no modification)\n",
    "- **T → 0**: Becomes deterministic (always picks highest logit)\n",
    "- **T → ∞**: Becomes uniform random (all tokens equally likely)\n",
    "\n",
    "### The Physics Analogy\n",
    "\n",
    "Temperature comes from statistical mechanics:\n",
    "\n",
    "**Low Temperature (T < 1)**:\n",
    "- Like a cold system with low kinetic energy\n",
    "- Particles settle into lowest energy states\n",
    "- Text becomes more deterministic and predictable\n",
    "\n",
    "**High Temperature (T > 1)**:\n",
    "- Like a hot system with high kinetic energy\n",
    "- Particles can access higher energy states\n",
    "- Text becomes more random and creative\n",
    "\n",
    "### Practical Effects\n",
    "\n",
    "**T = 0.1**: Very conservative, often repetitive\n",
    "**T = 0.7**: Good balance for most applications\n",
    "**T = 1.0**: Raw model probabilities\n",
    "**T = 1.2**: More creative and diverse\n",
    "**T = 2.0**: Often incoherent but surprising\n",
    "\n",
    "Let's implement and visualize temperature effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pre-trained model for generation experiments\n",
    "config = {\n",
    "    'vocab_size': 1000,\n",
    "    'd_model': 128,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 4,\n",
    "    'd_ff': 256,\n",
    "    'max_seq_len': 64,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = GPTModel(**config).to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "def apply_temperature(logits, temperature):\n",
    "    \"\"\"Apply temperature scaling to logits.\"\"\"\n",
    "    if temperature == 0:\n",
    "        # Special case: make it very low temperature instead of zero\n",
    "        temperature = 1e-10\n",
    "    return logits / temperature\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"Sample next token from logits with various strategies.\"\"\"\n",
    "    # Apply temperature\n",
    "    scaled_logits = apply_temperature(logits, temperature)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Apply top-k if specified\n",
    "    if top_k is not None:\n",
    "        # Get top-k indices\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "        # Create mask for top-k\n",
    "        mask = torch.zeros_like(probs)\n",
    "        mask.scatter_(-1, top_k_indices, 1)\n",
    "        probs = probs * mask\n",
    "        # Renormalize\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Apply nucleus (top-p) sampling if specified\n",
    "    if top_p is not None:\n",
    "        # Sort probabilities in descending order\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        # Calculate cumulative probabilities\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        # Create mask for nucleus\n",
    "        nucleus_mask = cumsum_probs <= top_p\n",
    "        # Include at least one token\n",
    "        nucleus_mask[..., 0] = True\n",
    "        # Apply mask\n",
    "        sorted_probs[~nucleus_mask] = 0\n",
    "        # Renormalize\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        # Scatter back to original order\n",
    "        probs = torch.zeros_like(probs)\n",
    "        probs.scatter_(-1, sorted_indices, sorted_probs)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token, probs\n",
    "\n",
    "def visualize_temperature_effects():\n",
    "    \"\"\"Visualize how temperature affects probability distributions.\"\"\"\n",
    "    \n",
    "    # Create example logits (before softmax)\n",
    "    logits = torch.tensor([2.0, 1.5, 1.0, 0.5, 0.2, -0.5, -1.0, -2.0])\n",
    "    temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    token_labels = [f'Token_{i}' for i in range(len(logits))]\n",
    "    \n",
    "    for i, temp in enumerate(temperatures):\n",
    "        if i < len(axes):\n",
    "            # Apply temperature and softmax\n",
    "            scaled_logits = apply_temperature(logits, temp)\n",
    "            probs = F.softmax(scaled_logits, dim=-1)\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = axes[i].bar(token_labels, probs.numpy(), \n",
    "                              color=plt.cm.viridis(i / len(temperatures)), \n",
    "                              alpha=0.8, edgecolor='black', linewidth=1)\n",
    "            \n",
    "            # Add probability values on bars\n",
    "            for bar, prob in zip(bars, probs.numpy()):\n",
    "                if prob > 0.01:  # Only show significant probabilities\n",
    "                    axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                                f'{prob:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            axes[i].set_title(f'Temperature = {temp}\\n(Entropy: {-torch.sum(probs * torch.log(probs + 1e-10)):.2f})', \n",
    "                             fontsize=12, weight='bold')\n",
    "            axes[i].set_ylabel('Probability')\n",
    "            axes[i].set_ylim(0, 1.0)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add entropy vs temperature plot\n",
    "    temp_range = np.linspace(0.1, 3.0, 50)\n",
    "    entropies = []\n",
    "    \n",
    "    for temp in temp_range:\n",
    "        scaled_logits = apply_temperature(logits, temp)\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    axes[5].plot(temp_range, entropies, linewidth=3, color='red', marker='o', markersize=4)\n",
    "    axes[5].set_title('Entropy vs Temperature\\n(Creativity Control)', fontsize=12, weight='bold')\n",
    "    axes[5].set_xlabel('Temperature')\n",
    "    axes[5].set_ylabel('Entropy (bits)')\n",
    "    axes[5].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for practical ranges\n",
    "    axes[5].axvspan(0.7, 1.2, alpha=0.2, color='green', label='Sweet Spot')\n",
    "    axes[5].axvspan(0.1, 0.5, alpha=0.2, color='blue', label='Conservative')\n",
    "    axes[5].axvspan(1.5, 3.0, alpha=0.2, color='orange', label='Creative')\n",
    "    axes[5].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🌡️ TEMPERATURE INSIGHTS:\")\n",
    "    print(\"• Low temp (0.1-0.5): Predictable, often repetitive\")\n",
    "    print(\"• Medium temp (0.7-1.2): Good balance of coherence and creativity\")\n",
    "    print(\"• High temp (1.5+): Creative but potentially incoherent\")\n",
    "    print(\"• Entropy measures 'surprise' - key for controlling creativity\")\n",
    "\n",
    "visualize_temperature_effects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate temperature effects with actual text generation\n",
    "\n",
    "def generate_text(model, prompt_tokens, max_length=20, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"Generate text using specified sampling parameters.\"\"\"\n",
    "    model.eval()\n",
    "    generated = prompt_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            outputs = model(generated)\n",
    "            next_token_logits = outputs[0, -1, :]  # Last token predictions\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token, probs = sample_from_logits(\n",
    "                next_token_logits, temperature=temperature, top_k=top_k, top_p=top_p\n",
    "            )\n",
    "            \n",
    "            # Append to sequence\n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we hit max sequence length\n",
    "            if generated.size(1) >= model.max_seq_len:\n",
    "                break\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Create a simple prompt (random tokens for demonstration)\n",
    "prompt = torch.randint(0, config['vocab_size'], (1, 5), device=device)\n",
    "print(f\"Starting prompt tokens: {prompt[0].tolist()}\")\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0.1, 0.7, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"\\n🎨 TEMPERATURE COMPARISON:\")\n",
    "print(\"Generating text with different temperature settings...\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(model, prompt, max_length=15, temperature=temp)\n",
    "    new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "    \n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  Generated tokens: {new_tokens}\")\n",
    "    print(f\"  Uniqueness: {len(set(new_tokens))}/{len(new_tokens)} unique tokens\")\n",
    "    \n",
    "    # Calculate repetition rate\n",
    "    if len(new_tokens) > 1:\n",
    "        repetitions = sum(1 for i in range(1, len(new_tokens)) if new_tokens[i] == new_tokens[i-1])\n",
    "        repetition_rate = repetitions / (len(new_tokens) - 1)\n",
    "        print(f\"  Repetition rate: {repetition_rate:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"🔍 OBSERVATIONS:\")\n",
    "print(\"• Low temperature: More repetitive, predictable patterns\")\n",
    "print(\"• High temperature: More diverse, potentially chaotic patterns\")\n",
    "print(\"• Medium temperature: Best balance for most applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top-k and Nucleus Sampling: Quality Control\n",
    "\n",
    "Temperature alone isn't enough. Even with perfect temperature, you might sample a very low-probability token that ruins coherence. Top-k and nucleus (top-p) sampling provide quality control.\n",
    "\n",
    "### Top-k Sampling: Fixed Vocabulary Filtering\n",
    "\n",
    "**Algorithm**:\n",
    "1. Sort tokens by probability (highest first)\n",
    "2. Keep only the top k most likely tokens\n",
    "3. Set all other probabilities to zero\n",
    "4. Renormalize and sample\n",
    "\n",
    "**Effect**: Prevents sampling from the \"long tail\" of unlikely tokens\n",
    "\n",
    "**Typical values**: k = 40-100 for most applications\n",
    "\n",
    "### Nucleus (Top-p) Sampling: Adaptive Vocabulary\n",
    "\n",
    "**Algorithm**:\n",
    "1. Sort tokens by probability (highest first)\n",
    "2. Add tokens to \"nucleus\" until cumulative probability ≥ p\n",
    "3. Sample only from the nucleus\n",
    "4. Adapt vocabulary size based on probability distribution\n",
    "\n",
    "**Effect**: Dynamic vocabulary size - smaller when model is confident, larger when uncertain\n",
    "\n",
    "**Typical values**: p = 0.9-0.95 for most applications\n",
    "\n",
    "### The Information Theory Perspective\n",
    "\n",
    "Both methods are about **controlling the effective vocabulary size**:\n",
    "\n",
    "**High Confidence Distributions** (low entropy):\n",
    "- Few tokens have significant probability\n",
    "- Top-k might be too restrictive\n",
    "- Nucleus adapts to use smaller vocabulary\n",
    "\n",
    "**Low Confidence Distributions** (high entropy):\n",
    "- Many tokens have similar probability\n",
    "- Top-k provides consistent filtering\n",
    "- Nucleus adapts to use larger vocabulary\n",
    "\n",
    "Let's implement and compare these approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sampling_strategies():\n",
    "    \"\"\"Compare different sampling strategies with various probability distributions.\"\"\"\n",
    "    \n",
    "    # Create different types of probability distributions\n",
    "    vocab_size = 100\n",
    "    \n",
    "    # High confidence (peaked distribution)\n",
    "    high_conf_logits = torch.zeros(vocab_size)\n",
    "    high_conf_logits[0] = 3.0  # Very likely token\n",
    "    high_conf_logits[1] = 1.0  # Somewhat likely\n",
    "    high_conf_logits[2:10] = 0.2  # Slightly likely\n",
    "    high_conf_logits[10:] = -2.0  # Very unlikely\n",
    "    \n",
    "    # Low confidence (flat distribution)\n",
    "    low_conf_logits = torch.randn(vocab_size) * 0.5\n",
    "    \n",
    "    # Medium confidence (moderate peak)\n",
    "    med_conf_logits = torch.zeros(vocab_size)\n",
    "    med_conf_logits[0] = 1.5\n",
    "    med_conf_logits[1:5] = 1.0\n",
    "    med_conf_logits[5:20] = 0.5\n",
    "    med_conf_logits[20:] = -1.0\n",
    "    \n",
    "    distributions = {\n",
    "        'High Confidence': high_conf_logits,\n",
    "        'Medium Confidence': med_conf_logits,\n",
    "        'Low Confidence': low_conf_logits\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    \n",
    "    for row, (dist_name, logits) in enumerate(distributions.items()):\n",
    "        # Original distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "        \n",
    "        # Plot original\n",
    "        axes[row, 0].bar(range(min(20, vocab_size)), probs[:20].numpy(), \n",
    "                        alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[row, 0].set_title(f'{dist_name}\\nOriginal (H={entropy:.2f})', weight='bold')\n",
    "        axes[row, 0].set_ylabel('Probability')\n",
    "        \n",
    "        # Top-k = 10\n",
    "        _, top_k_probs = sample_from_logits(logits, temperature=1.0, top_k=10)\n",
    "        k_entropy = -torch.sum(top_k_probs * torch.log(top_k_probs + 1e-10)).item()\n",
    "        axes[row, 1].bar(range(min(20, vocab_size)), top_k_probs[:20].numpy(), \n",
    "                        alpha=0.7, color='green', edgecolor='black')\n",
    "        axes[row, 1].set_title(f'Top-k=10\\n(H={k_entropy:.2f})', weight='bold')\n",
    "        \n",
    "        # Nucleus p=0.9\n",
    "        _, nucleus_probs = sample_from_logits(logits, temperature=1.0, top_p=0.9)\n",
    "        n_entropy = -torch.sum(nucleus_probs * torch.log(nucleus_probs + 1e-10)).item()\n",
    "        axes[row, 2].bar(range(min(20, vocab_size)), nucleus_probs[:20].numpy(), \n",
    "                        alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[row, 2].set_title(f'Nucleus p=0.9\\n(H={n_entropy:.2f})', weight='bold')\n",
    "        \n",
    "        # Combined: Top-k + Nucleus\n",
    "        _, combined_probs = sample_from_logits(logits, temperature=1.0, top_k=10, top_p=0.9)\n",
    "        c_entropy = -torch.sum(combined_probs * torch.log(combined_probs + 1e-10)).item()\n",
    "        axes[row, 3].bar(range(min(20, vocab_size)), combined_probs[:20].numpy(), \n",
    "                        alpha=0.7, color='red', edgecolor='black')\n",
    "        axes[row, 3].set_title(f'Top-k + Nucleus\\n(H={c_entropy:.2f})', weight='bold')\n",
    "        \n",
    "        # Calculate effective vocabulary size (tokens with prob > 0.01)\n",
    "        orig_vocab = (probs > 0.01).sum().item()\n",
    "        k_vocab = (top_k_probs > 0.01).sum().item()\n",
    "        n_vocab = (nucleus_probs > 0.01).sum().item()\n",
    "        c_vocab = (combined_probs > 0.01).sum().item()\n",
    "        \n",
    "        print(f\"\\n{dist_name}:\")\n",
    "        print(f\"  Original entropy: {entropy:.3f}, effective vocab: {orig_vocab}\")\n",
    "        print(f\"  Top-k entropy: {k_entropy:.3f}, effective vocab: {k_vocab}\")\n",
    "        print(f\"  Nucleus entropy: {n_entropy:.3f}, effective vocab: {n_vocab}\")\n",
    "        print(f\"  Combined entropy: {c_entropy:.3f}, effective vocab: {c_vocab}\")\n",
    "    \n",
    "    # Set common formatting\n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            axes[i, j].set_xlabel('Token Index')\n",
    "            axes[i, j].grid(True, alpha=0.3)\n",
    "            axes[i, j].set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "print(\"🎯 ANALYZING SAMPLING STRATEGIES\")\n",
    "print(\"Comparing how different methods handle various probability distributions...\")\n",
    "\n",
    "distributions = analyze_sampling_strategies()\n",
    "\n",
    "print(\"\\n🔍 KEY INSIGHTS:\")\n",
    "print(\"• Top-k: Fixed vocabulary size, good for consistent filtering\")\n",
    "print(\"• Nucleus: Adaptive vocabulary, matches model confidence\")\n",
    "print(\"• Combined: Best of both - consistent bounds with adaptivity\")\n",
    "print(\"• Entropy measures randomness - lower = more focused sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical comparison of sampling strategies\n",
    "\n",
    "def compare_sampling_methods():\n",
    "    \"\"\"Generate text with different sampling methods and compare results.\"\"\"\n",
    "    \n",
    "    # Generation settings\n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 3), device=device)\n",
    "    generation_length = 20\n",
    "    num_samples = 5  # Generate multiple samples for each method\n",
    "    \n",
    "    methods = {\n",
    "        'Greedy (T=0.1)': {'temperature': 0.1, 'top_k': None, 'top_p': None},\n",
    "        'Pure Random (T=2.0)': {'temperature': 2.0, 'top_k': None, 'top_p': None},\n",
    "        'Top-k=40': {'temperature': 1.0, 'top_k': 40, 'top_p': None},\n",
    "        'Nucleus p=0.9': {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
    "        'Balanced (T=0.8, k=50, p=0.95)': {'temperature': 0.8, 'top_k': 50, 'top_p': 0.95}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"🎲 SAMPLING METHOD COMPARISON\")\n",
    "    print(f\"Starting with prompt tokens: {prompt[0].tolist()}\")\n",
    "    print(f\"Generating {generation_length} tokens with each method...\\n\")\n",
    "    \n",
    "    for method_name, params in methods.items():\n",
    "        print(f\"📝 {method_name}:\")\n",
    "        \n",
    "        samples = []\n",
    "        diversities = []\n",
    "        repetition_rates = []\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            # Generate text\n",
    "            generated = generate_text(\n",
    "                model, prompt, \n",
    "                max_length=generation_length, \n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            # Extract new tokens\n",
    "            new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "            samples.append(new_tokens)\n",
    "            \n",
    "            # Calculate diversity (unique tokens / total tokens)\n",
    "            diversity = len(set(new_tokens)) / len(new_tokens) if new_tokens else 0\n",
    "            diversities.append(diversity)\n",
    "            \n",
    "            # Calculate repetition rate (consecutive identical tokens)\n",
    "            if len(new_tokens) > 1:\n",
    "                repetitions = sum(1 for i in range(1, len(new_tokens)) \n",
    "                                if new_tokens[i] == new_tokens[i-1])\n",
    "                repetition_rate = repetitions / (len(new_tokens) - 1)\n",
    "            else:\n",
    "                repetition_rate = 0\n",
    "            repetition_rates.append(repetition_rate)\n",
    "            \n",
    "            # Show first few samples\n",
    "            if sample_idx < 2:\n",
    "                print(f\"  Sample {sample_idx + 1}: {new_tokens[:10]}{'...' if len(new_tokens) > 10 else ''}\")\n",
    "        \n",
    "        # Calculate statistics across samples\n",
    "        avg_diversity = np.mean(diversities)\n",
    "        avg_repetition = np.mean(repetition_rates)\n",
    "        \n",
    "        # Calculate inter-sample diversity (how different samples are from each other)\n",
    "        all_tokens = set()\n",
    "        for sample in samples:\n",
    "            all_tokens.update(sample)\n",
    "        inter_sample_diversity = len(all_tokens)\n",
    "        \n",
    "        results[method_name] = {\n",
    "            'avg_diversity': avg_diversity,\n",
    "            'avg_repetition': avg_repetition,\n",
    "            'inter_sample_diversity': inter_sample_diversity,\n",
    "            'samples': samples\n",
    "        }\n",
    "        \n",
    "        print(f\"  Avg diversity: {avg_diversity:.3f}\")\n",
    "        print(f\"  Avg repetition rate: {avg_repetition:.3f}\")\n",
    "        print(f\"  Inter-sample diversity: {inter_sample_diversity} unique tokens\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "sampling_results = compare_sampling_methods()\n",
    "\n",
    "print(\"🏆 SAMPLING METHOD ANALYSIS:\")\n",
    "print(\"\\nRanking by diversity (creativity):\")\n",
    "diversity_ranking = sorted(sampling_results.items(), \n",
    "                          key=lambda x: x[1]['avg_diversity'], reverse=True)\n",
    "for i, (method, stats) in enumerate(diversity_ranking, 1):\n",
    "    print(f\"  {i}. {method}: {stats['avg_diversity']:.3f}\")\n",
    "\n",
    "print(\"\\nRanking by coherence (low repetition):\")\n",
    "coherence_ranking = sorted(sampling_results.items(), \n",
    "                          key=lambda x: x[1]['avg_repetition'])\n",
    "for i, (method, stats) in enumerate(coherence_ranking, 1):\n",
    "    print(f\"  {i}. {method}: {stats['avg_repetition']:.3f} repetition rate\")\n",
    "\n",
    "print(\"\\n🎯 PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"• Greedy: Best for consistency, worst for creativity\")\n",
    "print(\"• Pure Random: Most creative, often incoherent\")\n",
    "print(\"• Top-k: Good balance, consistent vocabulary control\")\n",
    "print(\"• Nucleus: Adaptive, matches model confidence\")\n",
    "print(\"• Balanced: Often optimal - combines multiple techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Beam Search: Structured Exploration\n",
    "\n",
    "All previous methods are **local** - they only consider the next token. Beam search is **global** - it considers entire sequences and their cumulative probabilities.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "**Beam Search maintains k \"beams\" (partial sequences):**\n",
    "\n",
    "1. **Initialize**: Start with k copies of the prompt\n",
    "2. **Expand**: For each beam, generate all possible next tokens\n",
    "3. **Score**: Calculate cumulative log-probability for each sequence\n",
    "4. **Prune**: Keep only the k highest-scoring sequences\n",
    "5. **Repeat**: Until all beams end or max length reached\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "**Sequence Score**: S(x₁, x₂, ..., xₙ) = Σᵢ log P(xᵢ | x₁, ..., xᵢ₋₁)\n",
    "\n",
    "**Length Normalization**: Often divide by sequence length to avoid bias toward shorter sequences\n",
    "\n",
    "**Diversity Penalties**: Subtract penalties for similar beams to encourage exploration\n",
    "\n",
    "### Why Beam Search Works\n",
    "\n",
    "**Global Optimization**: Unlike sampling, beam search optimizes the entire sequence\n",
    "\n",
    "**Structured Exploration**: Systematically explores the most promising paths\n",
    "\n",
    "**Quality Control**: High-probability sequences are more likely to be coherent\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Advantages**:\n",
    "- Higher quality, more coherent output\n",
    "- Deterministic (same input → same output)\n",
    "- Good for tasks requiring consistency\n",
    "\n",
    "**Disadvantages**:\n",
    "- Less creative/diverse than sampling\n",
    "- Computationally expensive (k forward passes per step)\n",
    "- Can get stuck in repetitive patterns\n",
    "\n",
    "Let's implement beam search and compare it with sampling methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    \"\"\"Implement beam search for text generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, beam_size=5, max_length=50, length_penalty=1.0, diversity_penalty=0.0):\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        self.length_penalty = length_penalty\n",
    "        self.diversity_penalty = diversity_penalty\n",
    "    \n",
    "    def generate(self, prompt_tokens):\n",
    "        \"\"\"Generate text using beam search.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize beams: (sequence, cumulative_score, finished)\n",
    "        beams = [(prompt_tokens.clone(), 0.0, False)]\n",
    "        finished_beams = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(self.max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                # Expand each beam\n",
    "                for beam_seq, beam_score, is_finished in beams:\n",
    "                    if is_finished:\n",
    "                        candidates.append((beam_seq, beam_score, True))\n",
    "                        continue\n",
    "                    \n",
    "                    # Get model predictions for this beam\n",
    "                    outputs = self.model(beam_seq)\n",
    "                    next_token_logits = outputs[0, -1, :]\n",
    "                    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    \n",
    "                    # Get top tokens to consider (limit search space)\n",
    "                    top_probs, top_indices = torch.topk(next_token_probs, \n",
    "                                                       min(self.beam_size * 2, len(next_token_probs)))\n",
    "                    \n",
    "                    # Create candidate sequences\n",
    "                    for prob, token_id in zip(top_probs, top_indices):\n",
    "                        new_seq = torch.cat([beam_seq, token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                        new_score = beam_score + torch.log(prob).item()\n",
    "                        \n",
    "                        # Apply length penalty\n",
    "                        length_normalized_score = new_score / (new_seq.size(1) ** self.length_penalty)\n",
    "                        \n",
    "                        # Check if sequence should be finished (you could add end-of-sequence logic here)\n",
    "                        is_finished = new_seq.size(1) >= self.max_length\n",
    "                        \n",
    "                        candidates.append((new_seq, length_normalized_score, is_finished))\n",
    "                \n",
    "                # Apply diversity penalty\n",
    "                if self.diversity_penalty > 0:\n",
    "                    candidates = self._apply_diversity_penalty(candidates)\n",
    "                \n",
    "                # Sort candidates by score and keep top beams\n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Separate finished and unfinished beams\n",
    "                new_beams = []\n",
    "                for seq, score, finished in candidates:\n",
    "                    if finished:\n",
    "                        finished_beams.append((seq, score))\n",
    "                    else:\n",
    "                        new_beams.append((seq, score, finished))\n",
    "                    \n",
    "                    if len(new_beams) >= self.beam_size:\n",
    "                        break\n",
    "                \n",
    "                beams = new_beams\n",
    "                \n",
    "                # Stop if all beams are finished\n",
    "                if not beams:\n",
    "                    break\n",
    "        \n",
    "        # Add remaining beams to finished beams\n",
    "        for seq, score, _ in beams:\n",
    "            finished_beams.append((seq, score))\n",
    "        \n",
    "        # Sort finished beams by score\n",
    "        finished_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return finished_beams\n",
    "    \n",
    "    def _apply_diversity_penalty(self, candidates):\n",
    "        \"\"\"Apply diversity penalty to encourage different sequences.\"\"\"\n",
    "        # Simple diversity penalty: reduce score if sequence is too similar to higher-scoring ones\n",
    "        penalized_candidates = []\n",
    "        \n",
    "        for i, (seq, score, finished) in enumerate(candidates):\n",
    "            penalty = 0.0\n",
    "            \n",
    "            # Compare with higher-scoring candidates\n",
    "            for j in range(i):\n",
    "                other_seq, _, _ = candidates[j]\n",
    "                # Calculate similarity (simple: number of matching tokens)\n",
    "                min_len = min(seq.size(1), other_seq.size(1))\n",
    "                matches = (seq[0, :min_len] == other_seq[0, :min_len]).sum().item()\n",
    "                similarity = matches / min_len if min_len > 0 else 0\n",
    "                penalty += similarity * self.diversity_penalty\n",
    "            \n",
    "            penalized_score = score - penalty\n",
    "            penalized_candidates.append((seq, penalized_score, finished))\n",
    "        \n",
    "        return penalized_candidates\n",
    "\n",
    "# Test beam search\n",
    "def compare_beam_search_vs_sampling():\n",
    "    \"\"\"Compare beam search with sampling methods.\"\"\"\n",
    "    \n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 4), device=device)\n",
    "    generation_length = 15\n",
    "    \n",
    "    print(f\"🔍 BEAM SEARCH vs SAMPLING COMPARISON\")\n",
    "    print(f\"Starting prompt: {prompt[0].tolist()}\")\n",
    "    print(f\"Generating {generation_length} additional tokens...\\n\")\n",
    "    \n",
    "    # Beam search with different beam sizes\n",
    "    beam_sizes = [1, 3, 5]\n",
    "    \n",
    "    for beam_size in beam_sizes:\n",
    "        print(f\"🔬 Beam Search (size={beam_size}):\")\n",
    "        \n",
    "        generator = BeamSearchGenerator(\n",
    "            model, beam_size=beam_size, max_length=generation_length, \n",
    "            length_penalty=1.0, diversity_penalty=0.0\n",
    "        )\n",
    "        \n",
    "        beams = generator.generate(prompt)\n",
    "        \n",
    "        for i, (sequence, score) in enumerate(beams[:3]):\n",
    "            new_tokens = sequence[0, len(prompt[0]):].tolist()\n",
    "            print(f\"  Beam {i+1}: {new_tokens} (score: {score:.3f})\")\n",
    "        \n",
    "        # Calculate diversity within beams\n",
    "        if len(beams) > 1:\n",
    "            all_tokens = set()\n",
    "            for seq, _ in beams[:beam_size]:\n",
    "                tokens = seq[0, len(prompt[0]):].tolist()\n",
    "                all_tokens.update(tokens)\n",
    "            print(f\"  Beam diversity: {len(all_tokens)} unique tokens across top {beam_size} beams\")\n",
    "        print()\n",
    "    \n",
    "    # Compare with sampling methods\n",
    "    print(\"🎲 Sampling Methods (for comparison):\")\n",
    "    \n",
    "    sampling_methods = {\n",
    "        'Nucleus (p=0.9)': {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
    "        'Top-k (k=50)': {'temperature': 1.0, 'top_k': 50, 'top_p': None},\n",
    "    }\n",
    "    \n",
    "    for method_name, params in sampling_methods.items():\n",
    "        print(f\"  {method_name}:\")\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(3):\n",
    "            generated = generate_text(model, prompt, max_length=generation_length, **params)\n",
    "            new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "            samples.append(new_tokens)\n",
    "            print(f\"    Sample {i+1}: {new_tokens}\")\n",
    "        \n",
    "        # Calculate diversity\n",
    "        all_tokens = set()\n",
    "        for sample in samples:\n",
    "            all_tokens.update(sample)\n",
    "        print(f\"    Sample diversity: {len(all_tokens)} unique tokens across 3 samples\")\n",
    "        print()\n",
    "    \n",
    "    return beams\n",
    "\n",
    "beam_results = compare_beam_search_vs_sampling()\n",
    "\n",
    "print(\"📊 BEAM SEARCH INSIGHTS:\")\n",
    "print(\"• Beam size 1 = greedy search (deterministic)\")\n",
    "print(\"• Larger beam sizes explore more possibilities\")\n",
    "print(\"• Beam search optimizes entire sequences, not just next tokens\")\n",
    "print(\"• Generally more coherent but less diverse than sampling\")\n",
    "print(\"• Good for tasks requiring consistency (translation, summarization)\")\n",
    "print(\"• Sampling better for creative tasks (story writing, dialogue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Metrics: Measuring Generation Success\n",
    "\n",
    "How do you know if your generation is good? Unlike training loss, there's no single metric for generation quality. You need multiple perspectives.\n",
    "\n",
    "### Automatic Metrics\n",
    "\n",
    "**Perplexity**: How \"surprised\" the model is by the generated text\n",
    "- Lower = more predictable (potentially boring)\n",
    "- Higher = more surprising (potentially incoherent)\n",
    "- Formula: PP = exp(-1/N Σ log P(token_i))\n",
    "\n",
    "**Diversity Metrics**:\n",
    "- **Type-Token Ratio (TTR)**: Unique words / Total words\n",
    "- **Entropy**: Information content of the distribution\n",
    "- **Self-BLEU**: How similar generated samples are to each other (lower = more diverse)\n",
    "\n",
    "**Repetition Metrics**:\n",
    "- **n-gram Repetition**: Percentage of repeated n-grams\n",
    "- **Longest Repeated Sequence**: Maximum length of repeated subsequence\n",
    "\n",
    "### Semantic Metrics\n",
    "\n",
    "**Coherence**: Does the text make sense?\n",
    "- Sentence-level: Grammar and syntax\n",
    "- Document-level: Logical flow and consistency\n",
    "\n",
    "**Relevance**: Does it match the prompt/context?\n",
    "- Semantic similarity to prompt\n",
    "- Topic consistency\n",
    "\n",
    "**Factuality**: Is the information correct?\n",
    "- Fact-checking against knowledge bases\n",
    "- Consistency with known facts\n",
    "\n",
    "### The Evaluation Challenge\n",
    "\n",
    "**No Single \"Best\" Metric**: Different tasks need different evaluation criteria\n",
    "\n",
    "**Human Evaluation**: Often the gold standard, but expensive and subjective\n",
    "\n",
    "**Task-Specific Metrics**: Translation has BLEU, summarization has ROUGE, etc.\n",
    "\n",
    "Let's implement a comprehensive evaluation suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationMetrics:\n",
    "    \"\"\"Comprehensive metrics for evaluating text generation quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def calculate_perplexity(self, text_tokens):\n",
    "        \"\"\"Calculate perplexity of generated text under the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if len(text_tokens.shape) == 1:\n",
    "                text_tokens = text_tokens.unsqueeze(0)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = self.model(text_tokens)\n",
    "            \n",
    "            # Calculate cross-entropy loss (negative log likelihood)\n",
    "            # Shift tokens for teacher forcing\n",
    "            logits = outputs[:, :-1, :].contiguous()\n",
    "            targets = text_tokens[:, 1:].contiguous()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            \n",
    "            return perplexity\n",
    "    \n",
    "    def calculate_diversity_metrics(self, token_sequences):\n",
    "        \"\"\"Calculate various diversity metrics for a set of generated sequences.\"\"\"\n",
    "        if not token_sequences:\n",
    "            return {}\n",
    "        \n",
    "        # Convert to lists if needed\n",
    "        if isinstance(token_sequences[0], torch.Tensor):\n",
    "            token_sequences = [seq.tolist() if isinstance(seq, torch.Tensor) else seq \n",
    "                             for seq in token_sequences]\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Type-Token Ratio (TTR) for each sequence\n",
    "        ttrs = []\n",
    "        for seq in token_sequences:\n",
    "            if len(seq) > 0:\n",
    "                ttr = len(set(seq)) / len(seq)\n",
    "                ttrs.append(ttr)\n",
    "        \n",
    "        metrics['avg_ttr'] = np.mean(ttrs) if ttrs else 0\n",
    "        metrics['std_ttr'] = np.std(ttrs) if ttrs else 0\n",
    "        \n",
    "        # Inter-sequence diversity (unique tokens across all sequences)\n",
    "        all_tokens = set()\n",
    "        total_tokens = 0\n",
    "        for seq in token_sequences:\n",
    "            all_tokens.update(seq)\n",
    "            total_tokens += len(seq)\n",
    "        \n",
    "        metrics['inter_seq_ttr'] = len(all_tokens) / total_tokens if total_tokens > 0 else 0\n",
    "        metrics['vocab_size'] = len(all_tokens)\n",
    "        \n",
    "        # Self-BLEU (how similar sequences are to each other - lower is more diverse)\n",
    "        if len(token_sequences) > 1:\n",
    "            self_bleu_scores = []\n",
    "            for i, seq1 in enumerate(token_sequences):\n",
    "                bleu_sum = 0\n",
    "                count = 0\n",
    "                for j, seq2 in enumerate(token_sequences):\n",
    "                    if i != j:\n",
    "                        # Simple n-gram overlap (proxy for BLEU)\n",
    "                        overlap = self._calculate_ngram_overlap(seq1, seq2, n=2)\n",
    "                        bleu_sum += overlap\n",
    "                        count += 1\n",
    "                if count > 0:\n",
    "                    self_bleu_scores.append(bleu_sum / count)\n",
    "            \n",
    "            metrics['self_bleu'] = np.mean(self_bleu_scores) if self_bleu_scores else 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_repetition_metrics(self, token_sequence):\n",
    "        \"\"\"Calculate repetition metrics for a single sequence.\"\"\"\n",
    "        if isinstance(token_sequence, torch.Tensor):\n",
    "            token_sequence = token_sequence.tolist()\n",
    "        \n",
    "        if len(token_sequence) <= 1:\n",
    "            return {'repetition_rate': 0, 'max_repeated_ngram': 0, 'ngram_repetition_2': 0}\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Immediate repetition rate (consecutive identical tokens)\n",
    "        repetitions = sum(1 for i in range(1, len(token_sequence)) \n",
    "                         if token_sequence[i] == token_sequence[i-1])\n",
    "        metrics['repetition_rate'] = repetitions / (len(token_sequence) - 1)\n",
    "        \n",
    "        # N-gram repetition (for n=2, 3)\n",
    "        for n in [2, 3]:\n",
    "            if len(token_sequence) >= n:\n",
    "                ngrams = [tuple(token_sequence[i:i+n]) for i in range(len(token_sequence)-n+1)]\n",
    "                unique_ngrams = len(set(ngrams))\n",
    "                total_ngrams = len(ngrams)\n",
    "                repetition = 1 - (unique_ngrams / total_ngrams) if total_ngrams > 0 else 0\n",
    "                metrics[f'ngram_repetition_{n}'] = repetition\n",
    "        \n",
    "        # Longest repeated subsequence\n",
    "        max_repeat_len = self._find_longest_repeated_subsequence(token_sequence)\n",
    "        metrics['max_repeated_ngram'] = max_repeat_len\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_ngram_overlap(self, seq1, seq2, n=2):\n",
    "        \"\"\"Calculate n-gram overlap between two sequences (simple BLEU approximation).\"\"\"\n",
    "        if len(seq1) < n or len(seq2) < n:\n",
    "            return 0.0\n",
    "        \n",
    "        ngrams1 = set(tuple(seq1[i:i+n]) for i in range(len(seq1)-n+1))\n",
    "        ngrams2 = set(tuple(seq2[i:i+n]) for i in range(len(seq2)-n+1))\n",
    "        \n",
    "        if len(ngrams1) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(ngrams1.intersection(ngrams2))\n",
    "        return overlap / len(ngrams1)\n",
    "    \n",
    "    def _find_longest_repeated_subsequence(self, sequence):\n",
    "        \"\"\"Find the length of the longest repeated subsequence.\"\"\"\n",
    "        max_length = 0\n",
    "        n = len(sequence)\n",
    "        \n",
    "        # Check for repeated subsequences of increasing length\n",
    "        for length in range(1, n // 2 + 1):\n",
    "            for i in range(n - length):\n",
    "                subseq = sequence[i:i+length]\n",
    "                # Look for this subsequence later in the sequence\n",
    "                for j in range(i + length, n - length + 1):\n",
    "                    if sequence[j:j+length] == subseq:\n",
    "                        max_length = max(max_length, length)\n",
    "                        break\n",
    "        \n",
    "        return max_length\n",
    "    \n",
    "    def comprehensive_evaluation(self, generated_sequences, prompts=None):\n",
    "        \"\"\"Run comprehensive evaluation on generated text.\"\"\"\n",
    "        results = {\n",
    "            'perplexities': [],\n",
    "            'diversity_metrics': {},\n",
    "            'repetition_metrics': [],\n",
    "            'quality_scores': []\n",
    "        }\n",
    "        \n",
    "        # Calculate per-sequence metrics\n",
    "        for seq in generated_sequences:\n",
    "            # Perplexity\n",
    "            if isinstance(seq, list):\n",
    "                seq_tensor = torch.tensor([seq], device=device)\n",
    "            else:\n",
    "                seq_tensor = seq\n",
    "            \n",
    "            try:\n",
    "                perplexity = self.calculate_perplexity(seq_tensor)\n",
    "                results['perplexities'].append(perplexity)\n",
    "            except:\n",
    "                results['perplexities'].append(float('inf'))\n",
    "            \n",
    "            # Repetition metrics\n",
    "            rep_metrics = self.calculate_repetition_metrics(seq)\n",
    "            results['repetition_metrics'].append(rep_metrics)\n",
    "        \n",
    "        # Calculate diversity metrics across all sequences\n",
    "        results['diversity_metrics'] = self.calculate_diversity_metrics(generated_sequences)\n",
    "        \n",
    "        # Calculate overall quality score (lower is better)\n",
    "        avg_perplexity = np.mean([p for p in results['perplexities'] if p != float('inf')])\n",
    "        avg_repetition = np.mean([m['repetition_rate'] for m in results['repetition_metrics']])\n",
    "        diversity_score = results['diversity_metrics'].get('avg_ttr', 0)\n",
    "        \n",
    "        # Composite quality score (you can adjust weights)\n",
    "        quality_score = {\n",
    "            'perplexity_score': avg_perplexity,\n",
    "            'repetition_penalty': avg_repetition * 100,  # Higher repetition = worse\n",
    "            'diversity_bonus': diversity_score * 100,    # Higher diversity = better\n",
    "            'composite_score': avg_perplexity + (avg_repetition * 50) - (diversity_score * 20)\n",
    "        }\n",
    "        results['quality_scores'] = quality_score\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"📊 Generation metrics toolkit ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation of different generation methods\n",
    "\n",
    "def evaluate_generation_methods():\n",
    "    \"\"\"Evaluate different generation methods using comprehensive metrics.\"\"\"\n",
    "    \n",
    "    metrics_calculator = GenerationMetrics(model)\n",
    "    \n",
    "    # Test different generation methods\n",
    "    methods = {\n",
    "        'Greedy': {'temperature': 0.1, 'top_k': None, 'top_p': None},\n",
    "        'High Temperature': {'temperature': 2.0, 'top_k': None, 'top_p': None},\n",
    "        'Top-k (k=40)': {'temperature': 1.0, 'top_k': 40, 'top_p': None},\n",
    "        'Nucleus (p=0.9)': {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
    "        'Balanced': {'temperature': 0.8, 'top_k': 50, 'top_p': 0.95}\n",
    "    }\n",
    "    \n",
    "    # Generate multiple samples for each method\n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 4), device=device)\n",
    "    num_samples = 5\n",
    "    generation_length = 20\n",
    "    \n",
    "    print(f\"🔬 COMPREHENSIVE GENERATION EVALUATION\")\n",
    "    print(f\"Generating {num_samples} samples of {generation_length} tokens each...\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for method_name, params in methods.items():\n",
    "        print(f\"📝 Evaluating {method_name}...\")\n",
    "        \n",
    "        # Generate samples\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            generated = generate_text(model, prompt, max_length=generation_length, **params)\n",
    "            # Extract only the newly generated tokens\n",
    "            new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "            samples.append(new_tokens)\n",
    "        \n",
    "        # Run comprehensive evaluation\n",
    "        results = metrics_calculator.comprehensive_evaluation(samples)\n",
    "        all_results[method_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Avg Perplexity: {results['quality_scores']['perplexity_score']:.2f}\")\n",
    "        print(f\"  Avg TTR (diversity): {results['diversity_metrics']['avg_ttr']:.3f}\")\n",
    "        print(f\"  Repetition Rate: {np.mean([m['repetition_rate'] for m in results['repetition_metrics']]):.3f}\")\n",
    "        print(f\"  Composite Score: {results['quality_scores']['composite_score']:.2f} (lower = better)\")\n",
    "        print()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_generation_methods()\n",
    "\n",
    "# Create comparison visualization\n",
    "def visualize_evaluation_results(results):\n",
    "    \"\"\"Visualize the evaluation results across different methods.\"\"\"\n",
    "    \n",
    "    methods = list(results.keys())\n",
    "    \n",
    "    # Extract metrics\n",
    "    perplexities = [results[method]['quality_scores']['perplexity_score'] for method in methods]\n",
    "    diversities = [results[method]['diversity_metrics']['avg_ttr'] for method in methods]\n",
    "    repetitions = [np.mean([m['repetition_rate'] for m in results[method]['repetition_metrics']]) \n",
    "                  for method in methods]\n",
    "    composite_scores = [results[method]['quality_scores']['composite_score'] for method in methods]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Perplexity (lower is better)\n",
    "    bars = axes[0, 0].bar(methods, perplexities, color='skyblue', alpha=0.8, edgecolor='black')\n",
    "    axes[0, 0].set_title('Perplexity (Lower = Better)', fontsize=14, weight='bold')\n",
    "    axes[0, 0].set_ylabel('Perplexity')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, val in zip(bars, perplexities):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                       f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Diversity (higher is better)\n",
    "    bars = axes[0, 1].bar(methods, diversities, color='lightgreen', alpha=0.8, edgecolor='black')\n",
    "    axes[0, 1].set_title('Diversity (TTR, Higher = Better)', fontsize=14, weight='bold')\n",
    "    axes[0, 1].set_ylabel('Type-Token Ratio')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, val in zip(bars, diversities):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                       f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Repetition Rate (lower is better)\n",
    "    bars = axes[1, 0].bar(methods, repetitions, color='salmon', alpha=0.8, edgecolor='black')\n",
    "    axes[1, 0].set_title('Repetition Rate (Lower = Better)', fontsize=14, weight='bold')\n",
    "    axes[1, 0].set_ylabel('Repetition Rate')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, val in zip(bars, repetitions):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                       f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Composite Score (lower is better)\n",
    "    bars = axes[1, 1].bar(methods, composite_scores, color='gold', alpha=0.8, edgecolor='black')\n",
    "    axes[1, 1].set_title('Composite Quality Score (Lower = Better)', fontsize=14, weight='bold')\n",
    "    axes[1, 1].set_ylabel('Composite Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, val in zip(bars, composite_scores):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print rankings\n",
    "    print(\"🏆 GENERATION METHOD RANKINGS:\")\n",
    "    \n",
    "    # Best perplexity (lowest)\n",
    "    perp_ranking = sorted(zip(methods, perplexities), key=lambda x: x[1])\n",
    "    print(\"\\n📉 Best Perplexity (Coherence):\")\n",
    "    for i, (method, score) in enumerate(perp_ranking, 1):\n",
    "        print(f\"  {i}. {method}: {score:.2f}\")\n",
    "    \n",
    "    # Best diversity (highest)\n",
    "    div_ranking = sorted(zip(methods, diversities), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\n🌈 Best Diversity (Creativity):\")\n",
    "    for i, (method, score) in enumerate(div_ranking, 1):\n",
    "        print(f\"  {i}. {method}: {score:.3f}\")\n",
    "    \n",
    "    # Best composite score (lowest)\n",
    "    comp_ranking = sorted(zip(methods, composite_scores), key=lambda x: x[1])\n",
    "    print(\"\\n🎯 Best Overall Quality:\")\n",
    "    for i, (method, score) in enumerate(comp_ranking, 1):\n",
    "        print(f\"  {i}. {method}: {score:.2f}\")\n",
    "\n",
    "visualize_evaluation_results(evaluation_results)\n",
    "\n",
    "print(\"\\n🔍 EVALUATION INSIGHTS:\")\n",
    "print(\"• Perplexity measures predictability - lower usually means more coherent\")\n",
    "print(\"• Diversity (TTR) measures creativity - higher means more varied vocabulary\")\n",
    "print(\"• Repetition rate measures quality - lower means less repetitive text\")\n",
    "print(\"• Composite score balances all factors - find the sweet spot for your task\")\n",
    "print(\"• No single 'best' method - choose based on your specific requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Master Advanced Text Generation\n",
    "\n",
    "You've learned the complete toolkit for sophisticated text generation - from mathematical foundations to practical implementation.\n",
    "\n",
    "### 🎨 The Four Pillars of Generation\n",
    "\n",
    "**1. Temperature Scaling**\n",
    "```python\n",
    "# Control creativity through probability sharpening/flattening\n",
    "scaled_logits = logits / temperature\n",
    "probs = F.softmax(scaled_logits, dim=-1)\n",
    "```\n",
    "- **T < 1**: Conservative, predictable (good for formal text)\n",
    "- **T = 1**: Raw model probabilities\n",
    "- **T > 1**: Creative, diverse (good for creative writing)\n",
    "\n",
    "**2. Quality Control Sampling**\n",
    "```python\n",
    "# Top-k: Fixed vocabulary filtering\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "\n",
    "# Nucleus: Adaptive vocabulary based on cumulative probability\n",
    "sorted_probs = torch.sort(probs, descending=True)\n",
    "nucleus = cumsum_probs <= p\n",
    "```\n",
    "\n",
    "**3. Beam Search**\n",
    "```python\n",
    "# Global optimization of entire sequences\n",
    "for each beam:\n",
    "    expand all possible next tokens\n",
    "    score by cumulative log-probability\n",
    "    keep top-k sequences\n",
    "```\n",
    "\n",
    "**4. Quality Metrics**\n",
    "```python\n",
    "# Multi-dimensional evaluation\n",
    "perplexity = exp(cross_entropy_loss)\n",
    "diversity = unique_tokens / total_tokens\n",
    "repetition = consecutive_duplicates / (total_tokens - 1)\n",
    "```\n",
    "\n",
    "### 🎯 Method Selection Guide\n",
    "\n",
    "**For Creative Writing**:\n",
    "- Temperature: 0.8-1.2\n",
    "- Nucleus sampling: p=0.9-0.95\n",
    "- Avoid beam search (too deterministic)\n",
    "\n",
    "**For Technical Documentation**:\n",
    "- Temperature: 0.3-0.7\n",
    "- Top-k: k=20-40\n",
    "- Consider beam search for consistency\n",
    "\n",
    "**For Dialogue/Conversation**:\n",
    "- Temperature: 0.7-1.0\n",
    "- Nucleus: p=0.85-0.9\n",
    "- Balance creativity with coherence\n",
    "\n",
    "**For Code Generation**:\n",
    "- Temperature: 0.1-0.5\n",
    "- Top-k: k=10-30\n",
    "- Beam search often beneficial\n",
    "\n",
    "### 📊 Quality Assessment Framework\n",
    "\n",
    "**Automatic Metrics**:\n",
    "- **Perplexity**: Model confidence (lower = more coherent)\n",
    "- **TTR**: Vocabulary diversity (higher = more creative)\n",
    "- **Repetition Rate**: Text quality (lower = better)\n",
    "- **Self-BLEU**: Inter-sample similarity (lower = more diverse)\n",
    "\n",
    "**Task-Specific Evaluation**:\n",
    "- **Summarization**: ROUGE, factual consistency\n",
    "- **Translation**: BLEU, semantic similarity\n",
    "- **QA**: Exact match, F1 score\n",
    "- **Creative writing**: Human evaluation, engagement\n",
    "\n",
    "### 🔧 Advanced Techniques\n",
    "\n",
    "**Controllable Generation**:\n",
    "```python\n",
    "# Prompt engineering for control\n",
    "prompt = \"Write a formal business email about [topic]:\"\n",
    "\n",
    "# Classifier-guided generation\n",
    "if classifier(generated_text) != target_style:\n",
    "    apply_style_penalty(logits)\n",
    "```\n",
    "\n",
    "**Multi-objective Optimization**:\n",
    "```python\n",
    "# Balance multiple criteria\n",
    "score = coherence_weight * coherence_score + \\\n",
    "        creativity_weight * diversity_score - \\\n",
    "        repetition_penalty * repetition_rate\n",
    "```\n",
    "\n",
    "### 💡 Best Practices\n",
    "\n",
    "**1. Start with Proven Baselines**:\n",
    "- Temperature 0.8, nucleus p=0.9 works for most tasks\n",
    "- Adjust based on specific requirements\n",
    "\n",
    "**2. Use Multiple Metrics**:\n",
    "- No single metric captures generation quality\n",
    "- Combine automatic and human evaluation\n",
    "\n",
    "**3. Task-Specific Tuning**:\n",
    "- Different tasks need different strategies\n",
    "- A/B test with real users when possible\n",
    "\n",
    "**4. Monitor Edge Cases**:\n",
    "- Watch for repetition loops\n",
    "- Check for factual errors\n",
    "- Validate semantic coherence\n",
    "\n",
    "### 🚀 Advanced Applications\n",
    "\n",
    "**Interactive Generation**:\n",
    "- Real-time adjustment of parameters\n",
    "- User feedback incorporation\n",
    "- Dynamic style adaptation\n",
    "\n",
    "**Multi-modal Generation**:\n",
    "- Text conditioned on images\n",
    "- Audio-to-text generation\n",
    "- Cross-modal consistency\n",
    "\n",
    "**Personalized Generation**:\n",
    "- User-specific style adaptation\n",
    "- Contextual preference learning\n",
    "- Progressive refinement\n",
    "\n",
    "### 🎭 The Art and Science Balance\n",
    "\n",
    "Great text generation combines:\n",
    "\n",
    "**Science**: Mathematical foundations, rigorous evaluation, systematic optimization\n",
    "\n",
    "**Art**: Creative intuition, aesthetic judgment, human-centered design\n",
    "\n",
    "**Engineering**: Robust implementation, efficient algorithms, production scalability\n",
    "\n",
    "You now have the complete toolkit to generate high-quality, controllable, and creative text. Use these techniques to build AI systems that truly understand and create human language! 🎨📚🤖"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}