{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Generation\n",
    "\n",
    "Transform model probabilities into coherent, creative text through sophisticated sampling techniques.\n",
    "\n",
    "## Generation Challenge\n",
    "\n",
    "Models output probabilities over 50,000+ tokens. Naive approaches (always picking highest probability) produce boring, repetitive text.\n",
    "\n",
    "## Core Techniques\n",
    "\n",
    "**Temperature**: Controls creativity vs coherence\n",
    "**Top-k/Nucleus**: Quality control sampling  \n",
    "**Beam Search**: Global sequence optimization\n",
    "**Quality Metrics**: Multi-dimensional evaluation\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Temperature scaling for creativity control\n",
    "2. Top-k and nucleus sampling for quality\n",
    "3. Beam search for structured exploration  \n",
    "4. Comprehensive quality evaluation\n",
    "5. Method selection for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This code imports necessary libraries for advanced text generation experiments including PyTorch for deep learning, visualization tools, and our transformer model components. It sets up device detection, random seeds for reproducibility, and prepares the experimental environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "config = create_model_config('tiny')\n",
    "model = GPTModel(**config).to(device)\n",
    "\n",
    "print(\"Environment ready for advanced generation experiments! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temperature: Controlling the Heat of Creativity\n",
    "\n",
    "Temperature is the most fundamental knob for controlling text generation. It transforms the raw probability distribution from your model.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Softmax with Temperature**: `p_i = exp(logit_i / T) / Œ£ exp(logit_j / T)`\n",
    "\n",
    "- **T < 1**: Conservative, predictable\n",
    "- **T = 1**: Raw model probabilities  \n",
    "- **T > 1**: Creative, diverse\n",
    "\n",
    "### Practical Effects\n",
    "\n",
    "- **T = 0.3**: Very conservative\n",
    "- **T = 0.8**: Good balance\n",
    "- **T = 1.5**: More creative\n",
    "- **T = 2.0**: Often incoherent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Implementation\n",
    "\n",
    "This code implements temperature scaling and generates text samples with different temperature settings. It demonstrates how temperature affects the sharpness of probability distributions and resulting text creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits, temperature=1.0):\n",
    "    return logits / temperature\n",
    "\n",
    "def generate_text(model, prompt, max_length=20, temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(generated)\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            \n",
    "            scaled_logits = apply_temperature(next_token_logits, temperature)\n",
    "            \n",
    "            if top_k is not None:\n",
    "                top_k_logits, top_k_indices = torch.topk(scaled_logits, min(top_k, scaled_logits.size(-1)))\n",
    "                scaled_logits = torch.full_like(scaled_logits, float('-inf'))\n",
    "                scaled_logits.scatter_(0, top_k_indices, top_k_logits)\n",
    "            \n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                sorted_indices_to_remove[0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                scaled_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            probs = F.softmax(scaled_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def demonstrate_temperature_effects():\n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 3), device=device)\n",
    "    temperatures = [0.1, 0.7, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    print(\"üå°Ô∏è TEMPERATURE EFFECTS DEMONSTRATION\")\n",
    "    print(f\"Starting prompt: {prompt[0].tolist()}\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(model, prompt, max_length=15, temperature=temp)\n",
    "        new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "        print(f\"T={temp}: {new_tokens}\")\n",
    "\n",
    "demonstrate_temperature_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top-k and Nucleus Sampling: Quality Control\n",
    "\n",
    "Temperature alone isn't enough. Top-k and nucleus sampling provide quality control by filtering low-probability tokens.\n",
    "\n",
    "### Top-k Sampling\n",
    "\n",
    "1. Sort tokens by probability\n",
    "2. Keep only top k tokens\n",
    "3. Renormalize and sample\n",
    "\n",
    "**Effect**: Fixed vocabulary filtering\n",
    "**Typical values**: k = 40-100\n",
    "\n",
    "### Nucleus (Top-p) Sampling\n",
    "\n",
    "1. Sort tokens by probability\n",
    "2. Add tokens until cumulative probability ‚â• p  \n",
    "3. Sample from this \"nucleus\"\n",
    "\n",
    "**Effect**: Adaptive vocabulary size\n",
    "**Typical values**: p = 0.9-0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Strategy Analysis\n",
    "\n",
    "This code analyzes and compares top-k and nucleus sampling strategies across different probability distributions. It demonstrates how each method adapts to high-confidence vs low-confidence model predictions, showing effective vocabulary sizes and entropy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sampling_strategies():\n",
    "    vocab_size = 1000\n",
    "    logits = torch.randn(vocab_size)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    k_values = [10, 20, 50, 100]\n",
    "    p_values = [0.8, 0.9, 0.95, 0.99]\n",
    "    \n",
    "    print(\"üìä SAMPLING STRATEGY ANALYSIS\\n\")\n",
    "    \n",
    "    print(\"Top-k Sampling Analysis:\")\n",
    "    for k in k_values:\n",
    "        top_k_probs, _ = torch.topk(probs, k)\n",
    "        effective_vocab = k\n",
    "        entropy = -torch.sum(top_k_probs * torch.log(top_k_probs + 1e-10))\n",
    "        print(f\"  k={k}: Effective vocab={effective_vocab}, Entropy={entropy:.3f}\")\n",
    "    \n",
    "    print(\"\\nNucleus (Top-p) Sampling Analysis:\")\n",
    "    for p in p_values:\n",
    "        sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        nucleus_size = (cumsum_probs <= p).sum().item()\n",
    "        nucleus_probs = sorted_probs[:nucleus_size]\n",
    "        entropy = -torch.sum(nucleus_probs * torch.log(nucleus_probs + 1e-10))\n",
    "        print(f\"  p={p}: Effective vocab={nucleus_size}, Entropy={entropy:.3f}\")\n",
    "\n",
    "analyze_sampling_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Sampling Comparison\n",
    "\n",
    "This code generates actual text samples using different sampling methods and measures their diversity, repetition rates, and inter-sample variation. It provides concrete metrics to compare greedy, random, top-k, nucleus, and balanced approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sampling_methods():\n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 3), device=device)\n",
    "    generation_length = 20\n",
    "    num_samples = 5\n",
    "    \n",
    "    methods = {\n",
    "        'Greedy (T=0.1)': {'temperature': 0.1, 'top_k': None, 'top_p': None},\n",
    "        'Pure Random (T=2.0)': {'temperature': 2.0, 'top_k': None, 'top_p': None},\n",
    "        'Top-k=40': {'temperature': 1.0, 'top_k': 40, 'top_p': None},\n",
    "        'Nucleus p=0.9': {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
    "        'Balanced': {'temperature': 0.8, 'top_k': 50, 'top_p': 0.95}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    print(f\"üé≤ SAMPLING METHOD COMPARISON\")\n",
    "    print(f\"Starting with prompt tokens: {prompt[0].tolist()}\")\n",
    "    \n",
    "    for method_name, params in methods.items():\n",
    "        print(f\"üìù {method_name}:\")\n",
    "        samples = []\n",
    "        diversities = []\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            generated = generate_text(model, prompt, max_length=generation_length, **params)\n",
    "            new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "            samples.append(new_tokens)\n",
    "            \n",
    "            diversity = len(set(new_tokens)) / len(new_tokens) if new_tokens else 0\n",
    "            diversities.append(diversity)\n",
    "            \n",
    "            if sample_idx < 2:\n",
    "                print(f\"  Sample {sample_idx + 1}: {new_tokens[:10]}{'...' if len(new_tokens) > 10 else ''}\")\n",
    "        \n",
    "        avg_diversity = np.mean(diversities)\n",
    "        print(f\"  Avg diversity: {avg_diversity:.3f}\\n\")\n",
    "        \n",
    "        results[method_name] = {'avg_diversity': avg_diversity, 'samples': samples}\n",
    "    \n",
    "    return results\n",
    "\n",
    "sampling_results = compare_sampling_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Beam Search: Structured Exploration\n",
    "\n",
    "Beam search optimizes entire sequences rather than individual tokens by maintaining multiple candidate sequences.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize**: Start with k copies of prompt\n",
    "2. **Expand**: Generate all possible next tokens for each beam\n",
    "3. **Score**: Calculate cumulative log-probability\n",
    "4. **Prune**: Keep only k highest-scoring sequences\n",
    "5. **Repeat**: Until completion\n",
    "\n",
    "### Key Formula\n",
    "\n",
    "**Sequence Score**: `S(x‚ÇÅ...x‚Çô) = Œ£·µ¢ log P(x·µ¢ | x‚ÇÅ...x·µ¢‚Çã‚ÇÅ)`\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Advantages**: Higher quality, deterministic, coherent\n",
    "**Disadvantages**: Less creative, computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Implementation\n",
    "\n",
    "This code implements a complete beam search generator with length normalization and diversity penalties. It demonstrates how beam search optimizes entire sequences rather than individual tokens, comparing results with sampling methods to show the coherence vs creativity trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(self, model, beam_size=5, max_length=50, length_penalty=1.0):\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        self.length_penalty = length_penalty\n",
    "    \n",
    "    def generate(self, prompt_tokens):\n",
    "        self.model.eval()\n",
    "        beams = [(prompt_tokens.clone(), 0.0, False)]\n",
    "        finished_beams = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(self.max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                for beam_seq, beam_score, is_finished in beams:\n",
    "                    if is_finished:\n",
    "                        candidates.append((beam_seq, beam_score, True))\n",
    "                        continue\n",
    "                    \n",
    "                    outputs = self.model(beam_seq)\n",
    "                    next_token_logits = outputs[0, -1, :]\n",
    "                    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    \n",
    "                    top_probs, top_indices = torch.topk(next_token_probs, min(self.beam_size * 2, len(next_token_probs)))\n",
    "                    \n",
    "                    for prob, token_id in zip(top_probs, top_indices):\n",
    "                        new_seq = torch.cat([beam_seq, token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                        new_score = beam_score + torch.log(prob).item()\n",
    "                        length_normalized_score = new_score / (new_seq.size(1) ** self.length_penalty)\n",
    "                        is_finished = new_seq.size(1) >= self.max_length\n",
    "                        candidates.append((new_seq, length_normalized_score, is_finished))\n",
    "                \n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                new_beams = []\n",
    "                for seq, score, finished in candidates:\n",
    "                    if finished:\n",
    "                        finished_beams.append((seq, score))\n",
    "                    else:\n",
    "                        new_beams.append((seq, score, finished))\n",
    "                    if len(new_beams) >= self.beam_size:\n",
    "                        break\n",
    "                \n",
    "                beams = new_beams\n",
    "                if not beams:\n",
    "                    break\n",
    "        \n",
    "        for seq, score, _ in beams:\n",
    "            finished_beams.append((seq, score))\n",
    "        \n",
    "        finished_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        return finished_beams\n",
    "\n",
    "def compare_beam_search_vs_sampling():\n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 4), device=device)\n",
    "    generation_length = 15\n",
    "    \n",
    "    print(f\"üîç BEAM SEARCH vs SAMPLING\")\n",
    "    print(f\"Starting prompt: {prompt[0].tolist()}\\n\")\n",
    "    \n",
    "    generator = BeamSearchGenerator(model, beam_size=3, max_length=generation_length)\n",
    "    beams = generator.generate(prompt)\n",
    "    \n",
    "    print(\"üî¨ Beam Search (size=3):\")\n",
    "    for i, (sequence, score) in enumerate(beams[:3]):\n",
    "        new_tokens = sequence[0, len(prompt[0]):].tolist()\n",
    "        print(f\"  Beam {i+1}: {new_tokens} (score: {score:.3f})\")\n",
    "    \n",
    "    return beams\n",
    "\n",
    "beam_results = compare_beam_search_vs_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Metrics: Measuring Generation Success\n",
    "\n",
    "Evaluating text generation requires multiple metrics since there's no single \"correct\" output.\n",
    "\n",
    "### Automatic Metrics\n",
    "\n",
    "**Perplexity**: Model surprise at generated text\n",
    "- Lower = more predictable (coherent)\n",
    "- Formula: `PP = exp(-1/N Œ£ log P(token_i))`\n",
    "\n",
    "**Diversity Metrics**:\n",
    "- **TTR**: Unique tokens / Total tokens\n",
    "- **Self-BLEU**: Similarity between samples (lower = more diverse)\n",
    "\n",
    "**Repetition Metrics**:\n",
    "- **n-gram repetition**: Repeated subsequences\n",
    "- **Consecutive repetition**: Immediate token repeats\n",
    "\n",
    "### Evaluation Challenge\n",
    "\n",
    "No single metric captures all aspects of quality. Combine automatic metrics with human evaluation for comprehensive assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Metrics Implementation\n",
    "\n",
    "This code implements comprehensive evaluation metrics for text generation including perplexity calculation, diversity measurements, and repetition analysis. It provides a complete toolkit for assessing generation quality across multiple dimensions with both automatic and semantic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationMetrics:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def calculate_perplexity(self, text_tokens):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if len(text_tokens.shape) == 1:\n",
    "                text_tokens = text_tokens.unsqueeze(0)\n",
    "            \n",
    "            outputs = self.model(text_tokens)\n",
    "            logits = outputs[:, :-1, :].contiguous()\n",
    "            targets = text_tokens[:, 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return perplexity\n",
    "    \n",
    "    def calculate_diversity_metrics(self, token_sequences):\n",
    "        if not token_sequences:\n",
    "            return {}\n",
    "        \n",
    "        if isinstance(token_sequences[0], torch.Tensor):\n",
    "            token_sequences = [seq.tolist() for seq in token_sequences]\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        ttrs = []\n",
    "        for seq in token_sequences:\n",
    "            if len(seq) > 0:\n",
    "                ttr = len(set(seq)) / len(seq)\n",
    "                ttrs.append(ttr)\n",
    "        \n",
    "        metrics['avg_ttr'] = np.mean(ttrs) if ttrs else 0\n",
    "        \n",
    "        all_tokens = set()\n",
    "        total_tokens = 0\n",
    "        for seq in token_sequences:\n",
    "            all_tokens.update(seq)\n",
    "            total_tokens += len(seq)\n",
    "        \n",
    "        metrics['inter_seq_ttr'] = len(all_tokens) / total_tokens if total_tokens > 0 else 0\n",
    "        metrics['vocab_size'] = len(all_tokens)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_repetition_metrics(self, token_sequence):\n",
    "        if isinstance(token_sequence, torch.Tensor):\n",
    "            token_sequence = token_sequence.tolist()\n",
    "        \n",
    "        if len(token_sequence) <= 1:\n",
    "            return {'repetition_rate': 0, 'ngram_repetition_2': 0}\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        repetitions = sum(1 for i in range(1, len(token_sequence)) \n",
    "                         if token_sequence[i] == token_sequence[i-1])\n",
    "        metrics['repetition_rate'] = repetitions / (len(token_sequence) - 1)\n",
    "        \n",
    "        if len(token_sequence) >= 2:\n",
    "            ngrams = [tuple(token_sequence[i:i+2]) for i in range(len(token_sequence)-1)]\n",
    "            unique_ngrams = len(set(ngrams))\n",
    "            total_ngrams = len(ngrams)\n",
    "            repetition = 1 - (unique_ngrams / total_ngrams) if total_ngrams > 0 else 0\n",
    "            metrics['ngram_repetition_2'] = repetition\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def comprehensive_evaluation(self, generated_sequences):\n",
    "        results = {\n",
    "            'perplexities': [],\n",
    "            'diversity_metrics': {},\n",
    "            'repetition_metrics': [],\n",
    "            'quality_scores': []\n",
    "        }\n",
    "        \n",
    "        for seq in generated_sequences:\n",
    "            if isinstance(seq, list):\n",
    "                seq_tensor = torch.tensor([seq], device=device)\n",
    "            else:\n",
    "                seq_tensor = seq\n",
    "            \n",
    "            try:\n",
    "                perplexity = self.calculate_perplexity(seq_tensor)\n",
    "                results['perplexities'].append(perplexity)\n",
    "            except:\n",
    "                results['perplexities'].append(float('inf'))\n",
    "            \n",
    "            rep_metrics = self.calculate_repetition_metrics(seq)\n",
    "            results['repetition_metrics'].append(rep_metrics)\n",
    "        \n",
    "        results['diversity_metrics'] = self.calculate_diversity_metrics(generated_sequences)\n",
    "        \n",
    "        avg_perplexity = np.mean([p for p in results['perplexities'] if p != float('inf')])\n",
    "        avg_repetition = np.mean([m['repetition_rate'] for m in results['repetition_metrics']])\n",
    "        diversity_score = results['diversity_metrics'].get('avg_ttr', 0)\n",
    "        \n",
    "        quality_score = {\n",
    "            'perplexity_score': avg_perplexity,\n",
    "            'repetition_penalty': avg_repetition * 100,\n",
    "            'diversity_bonus': diversity_score * 100,\n",
    "            'composite_score': avg_perplexity + (avg_repetition * 50) - (diversity_score * 20)\n",
    "        }\n",
    "        results['quality_scores'] = quality_score\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"üìä Generation metrics toolkit ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Method Evaluation\n",
    "\n",
    "This code evaluates all generation methods using the complete metrics suite, providing detailed comparisons across perplexity, diversity, repetition, and composite quality scores. It includes visualization and ranking to identify optimal methods for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation_methods():\n",
    "    metrics_calculator = GenerationMetrics(model)\n",
    "    \n",
    "    methods = {\n",
    "        'Greedy': {'temperature': 0.1, 'top_k': None, 'top_p': None},\n",
    "        'High Temperature': {'temperature': 2.0, 'top_k': None, 'top_p': None},\n",
    "        'Top-k (k=40)': {'temperature': 1.0, 'top_k': 40, 'top_p': None},\n",
    "        'Nucleus (p=0.9)': {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
    "        'Balanced': {'temperature': 0.8, 'top_k': 50, 'top_p': 0.95}\n",
    "    }\n",
    "    \n",
    "    prompt = torch.randint(0, config['vocab_size'], (1, 4), device=device)\n",
    "    num_samples = 5\n",
    "    generation_length = 20\n",
    "    \n",
    "    print(f\"üî¨ COMPREHENSIVE GENERATION EVALUATION\")\n",
    "    print(f\"Generating {num_samples} samples of {generation_length} tokens each...\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for method_name, params in methods.items():\n",
    "        print(f\"üìù Evaluating {method_name}...\")\n",
    "        \n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            generated = generate_text(model, prompt, max_length=generation_length, **params)\n",
    "            new_tokens = generated[0, len(prompt[0]):].tolist()\n",
    "            samples.append(new_tokens)\n",
    "        \n",
    "        results = metrics_calculator.comprehensive_evaluation(samples)\n",
    "        all_results[method_name] = results\n",
    "        \n",
    "        print(f\"  Avg Perplexity: {results['quality_scores']['perplexity_score']:.2f}\")\n",
    "        print(f\"  Avg TTR (diversity): {results['diversity_metrics']['avg_ttr']:.3f}\")\n",
    "        print(f\"  Repetition Rate: {np.mean([m['repetition_rate'] for m in results['repetition_metrics']]):.3f}\")\n",
    "        print(f\"  Composite Score: {results['quality_scores']['composite_score']:.2f} (lower = better)\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def visualize_evaluation_results(results):\n",
    "    methods = list(results.keys())\n",
    "    \n",
    "    perplexities = [results[method]['quality_scores']['perplexity_score'] for method in methods]\n",
    "    diversities = [results[method]['diversity_metrics']['avg_ttr'] for method in methods]\n",
    "    composite_scores = [results[method]['quality_scores']['composite_score'] for method in methods]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].bar(methods, perplexities, color='skyblue', alpha=0.8)\n",
    "    axes[0].set_title('Perplexity (Lower = Better)', weight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    axes[1].bar(methods, diversities, color='lightgreen', alpha=0.8)\n",
    "    axes[1].set_title('Diversity (TTR, Higher = Better)', weight='bold')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    axes[2].bar(methods, composite_scores, color='gold', alpha=0.8)\n",
    "    axes[2].set_title('Composite Quality Score (Lower = Better)', weight='bold')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üèÜ GENERATION METHOD RANKINGS:\")\n",
    "    \n",
    "    comp_ranking = sorted(zip(methods, composite_scores), key=lambda x: x[1])\n",
    "    print(\"\\nüéØ Best Overall Quality:\")\n",
    "    for i, (method, score) in enumerate(comp_ranking, 1):\n",
    "        print(f\"  {i}. {method}: {score:.2f}\")\n",
    "\n",
    "evaluation_results = evaluate_generation_methods()\n",
    "visualize_evaluation_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Advanced Text Generation Mastery\n",
    "\n",
    "Transform raw model probabilities into high-quality text through sophisticated sampling and evaluation.\n",
    "\n",
    "### Core Techniques\n",
    "\n",
    "**Temperature Scaling**: `logits / T` - Controls creativity vs coherence\n",
    "- T < 1: Conservative, predictable\n",
    "- T > 1: Creative, diverse  \n",
    "- Sweet spot: 0.7-1.2\n",
    "\n",
    "**Quality Control Sampling**:\n",
    "- **Top-k**: Fixed vocabulary filtering (k=40-100)\n",
    "- **Nucleus**: Adaptive vocabulary (p=0.9-0.95)\n",
    "- **Combined**: Best of both approaches\n",
    "\n",
    "**Beam Search**: Global sequence optimization\n",
    "- Maintains k candidate sequences\n",
    "- Optimizes cumulative probability\n",
    "- Better coherence, less creativity\n",
    "\n",
    "### Method Selection Guide\n",
    "\n",
    "**Creative Writing**: T=0.8-1.2, nucleus p=0.9, avoid beam search\n",
    "**Technical Docs**: T=0.3-0.7, top-k=20-40, consider beam search  \n",
    "**Dialogue**: T=0.7-1.0, nucleus p=0.85-0.9\n",
    "**Code Generation**: T=0.1-0.5, top-k=10-30, beam search beneficial\n",
    "\n",
    "### Quality Assessment\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Perplexity**: Model confidence (lower = more coherent)\n",
    "- **TTR**: Vocabulary diversity (higher = more creative)\n",
    "- **Repetition Rate**: Text quality (lower = better)\n",
    "- **Self-BLEU**: Inter-sample similarity (lower = more diverse)\n",
    "\n",
    "**Best Practice**: Combine automatic metrics with human evaluation for comprehensive quality assessment.\n",
    "\n",
    "### Implementation Framework\n",
    "\n",
    "```python\n",
    "# Balanced generation (good default)\n",
    "temperature = 0.8\n",
    "top_k = 50  \n",
    "top_p = 0.95\n",
    "\n",
    "# Apply techniques\n",
    "scaled_logits = logits / temperature\n",
    "filtered_probs = apply_top_k_nucleus(scaled_logits, top_k, top_p)\n",
    "next_token = torch.multinomial(filtered_probs, 1)\n",
    "```\n",
    "\n",
    "You now have the complete toolkit for sophisticated, controllable text generation across any domain or application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}