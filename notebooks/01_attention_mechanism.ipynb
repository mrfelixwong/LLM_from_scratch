{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Understanding Attention: The Heart of Transformers\n\nAttention allows models to focus on relevant parts of input when processing each position. Instead of processing sequences step-by-step like RNNs, attention connects any two positions directly.\n\n## Core Formula\n`Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V`\n\nThink of it like a database lookup:\n- **Queries (Q)**: What you're searching for\n- **Keys (K)**: Search index \n- **Values (V)**: Actual data returned\n- **Attention weights**: How relevant each key is to each query"
  },
  {
   "cell_type": "markdown",
   "source": "## Environment Setup\n\nImport required libraries for attention implementation and visualization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test Basic Attention\n\nCreate simple test data where attention patterns should be interpretable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Attention Visualization\n\nVisualize attention patterns on a realistic word sequence.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def scaled_dot_product_attention(Q, K, V, mask=None):\n    d_k = Q.size(-1)\n    \n    # Step 1: Compute similarity scores (dot product)\n    scores = torch.matmul(Q, K.transpose(-2, -1))\n    \n    # Step 2: Scale for numerical stability\n    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n    \n    # Step 3: Apply mask if provided (for causal attention)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    # Step 4: Convert to probabilities\n    attention_weights = F.softmax(scores, dim=-1)\n    \n    # Step 5: Apply to values\n    output = torch.matmul(attention_weights, V)\n    \n    return output, attention_weights"
  },
  {
   "cell_type": "markdown",
   "source": "def create_causal_mask(seq_len):\n    return torch.tril(torch.ones(seq_len, seq_len))\n\nseq_len = 4\ncausal_mask = create_causal_mask(seq_len)\nprint(\"Causal mask (1 = can attend, 0 = masked):\")\nprint(causal_mask.numpy())\n\n# Apply causal mask\noutput_causal, attn_causal = scaled_dot_product_attention(\n    embeddings, embeddings, embeddings, mask=causal_mask.unsqueeze(0)\n)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nsns.heatmap(attn_weights[0].numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax1)\nax1.set_title('Regular Self-Attention')\n\nsns.heatmap(attn_causal[0].numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax2)\nax2.set_title('Causal Self-Attention')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Causal attention: each position can only attend to previous positions.\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Head Attention Implementation\n\nMultiple attention heads capture different types of relationships in parallel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Summary\n\nYou've mastered attention - the core mechanism that makes transformers work!\n\n**Key Concepts**:\n- **Attention formula**: `softmax(QK^T / âˆšd_k)V` - similarity-based weighted combination\n- **Scaling**: `âˆšd_k` prevents softmax saturation for numerical stability\n- **Causal masking**: Prevents future information leakage in language modeling  \n- **Multi-head**: Parallel attention heads capture different relationship types\n\n**Why Revolutionary**: Direct connections between any positions enable parallelization and long-range dependencies.\n\n**Next**: Combine attention with feed-forward networks to build complete transformer blocks!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create word embeddings for \"The cat sat down\"\nembeddings = torch.tensor([\n    [1.0, 0.5, 0.0, 0.5],  # \"The\"\n    [0.0, 1.0, 1.0, 0.0],  # \"cat\" \n    [0.0, 0.0, 0.5, 1.0],  # \"sat\"\n    [0.5, 0.0, 0.0, 1.0],  # \"down\"\n]).unsqueeze(0)\n\nwords = [\"The\", \"cat\", \"sat\", \"down\"]\noutput, attn_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    attn_weights[0].detach().numpy(),\n    annot=True, fmt='.3f',\n    xticklabels=words, yticklabels=words,\n    cmap='Blues'\n)\nplt.title('Self-Attention Weights')\nplt.xlabel('Keys (attending to)')\nplt.ylabel('Queries (attending from)')\nplt.show()\n\nprint(\"Each row shows how much each word attends to other words.\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Head Attention Implementation\n\nMultiple attention heads capture different types of relationships in parallel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_sequence_data():\n",
    "    \"\"\"\n",
    "    Create a simple sequence where attention patterns should be interpretable.\n",
    "    \"\"\"\n",
    "    # Create embeddings for words: \"The\", \"cat\", \"sat\", \"down\"\n",
    "    seq_len, d_model = 4, 6\n",
    "    \n",
    "    # Manually create embeddings that should have interesting attention patterns\n",
    "    embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.0, 0.5, 0.0, 0.0],  # \"The\" - article\n",
    "        [0.0, 1.0, 1.0, 0.0, 0.5, 0.0],  # \"cat\" - noun\n",
    "        [0.0, 0.0, 0.5, 1.0, 1.0, 0.5],  # \"sat\" - verb\n",
    "        [0.5, 0.0, 0.0, 0.5, 1.0, 1.0],  # \"down\" - adverb\n",
    "    ]).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    words = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "    \n",
    "    return embeddings, words\n",
    "\n",
    "# Create interpretable data\n",
    "embeddings, words = create_simple_sequence_data()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "# Use embeddings as Q, K, V for self-attention\n",
    "output, attn_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attn_weights[0].detach().numpy(),\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    cmap='Blues',\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Keys (attending to)')\n",
    "plt.ylabel('Queries (attending from)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each row shows how much each word attends to other words.\")\n",
    "print(\"Higher values (darker blue) indicate stronger attention.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_causal_mask(seq_len):\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask\n\nseq_len = 4\ncausal_mask = create_causal_mask(seq_len)\nprint(\"Causal mask (1 = can attend, 0 = cannot attend):\")\nprint(causal_mask.numpy())\n\noutput_causal, attn_weights_causal = scaled_dot_product_attention(\n    embeddings, embeddings, embeddings, mask=causal_mask.unsqueeze(0), show_steps=False\n)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nsns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax1)\nax1.set_title('Regular Self-Attention')\n\nsns.heatmap(attn_weights_causal[0].detach().numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax2)\nax2.set_title('Causal Self-Attention')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Causal attention: each word can only attend to itself and previous words.\")\nprint(\"Notice the upper triangle is zero - no peeking at future!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Attention Mastery\n\nYou've learned the core mechanism that powers all transformers!\n\n### Key Concepts\n- **Attention formula**: `softmax(QK^T / âˆšd_k)V` - weighted average based on similarity\n- **Scaling factor**: `âˆšd_k` prevents softmax saturation for numerical stability  \n- **Causal masking**: Prevents future information leakage in language modeling\n- **Multi-head attention**: Parallel heads capture different relationship types\n\n### Why Attention is Revolutionary\n**Before**: RNNs processed sequences step-by-step, limiting parallelization\n**After**: Attention connects any two positions directly, enabling parallelization\n\n### Applications\n- **Self-attention**: Each position attends to all positions in same sequence\n- **Cross-attention**: Queries from one sequence, keys/values from another (e.g., translation)\n- **Causal attention**: For autoregressive language modeling\n\n### Next Steps\nNow you understand attention! Next, we'll see how it combines with other components to build complete transformer blocks.\n\nThe foundation is solid - let's build transformers! ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}