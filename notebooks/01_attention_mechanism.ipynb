{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Attention Mechanism\n",
    "\n",
    "This notebook explores the core attention mechanism that powers transformer models. We'll build intuition about how attention works and implement it step by step.\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "Attention allows the model to focus on different parts of the input when making predictions. The key insight is that not all parts of the input are equally important for predicting a given output.\n",
    "\n",
    "The attention mechanism computes a weighted sum of values, where the weights are determined by the similarity between queries and keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental attention operation is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ are the queries\n",
    "- $K$ are the keys  \n",
    "- $V$ are the values\n",
    "- $d_k$ is the dimension of the keys\n",
    "\n",
    "Let's implement this step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries [batch_size, seq_len, d_k]\n",
    "        K: Keys [batch_size, seq_len, d_k]\n",
    "        V: Values [batch_size, seq_len, d_v]\n",
    "        mask: Optional mask [batch_size, seq_len, seq_len]\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch_size, seq_len, d_v]\n",
    "        attention_weights: [batch_size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"Raw scores shape: {scores.shape}\")\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    print(f\"Scaled scores range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "    \n",
    "    # Step 3: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    print(f\"Attention weights sum: {attention_weights.sum(dim=-1)[0, 0]:.3f} (should be 1.0)\")\n",
    "    \n",
    "    # Step 5: Apply attention to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_model = 1, 4, 8\n",
    "\n",
    "# Create simple example data\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model) \n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(f\"Q: {Q.shape}, K: {K.shape}, V: {V.shape}\")\n",
    "print()\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Attention\n",
    "\n",
    "Let's create a more interpretable example and visualize the attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_sequence_data():\n",
    "    \"\"\"\n",
    "    Create a simple sequence where attention patterns should be interpretable.\n",
    "    \"\"\"\n",
    "    # Create embeddings for words: \"The\", \"cat\", \"sat\", \"down\"\n",
    "    seq_len, d_model = 4, 6\n",
    "    \n",
    "    # Manually create embeddings that should have interesting attention patterns\n",
    "    embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.0, 0.5, 0.0, 0.0],  # \"The\" - article\n",
    "        [0.0, 1.0, 1.0, 0.0, 0.5, 0.0],  # \"cat\" - noun\n",
    "        [0.0, 0.0, 0.5, 1.0, 1.0, 0.5],  # \"sat\" - verb\n",
    "        [0.5, 0.0, 0.0, 0.5, 1.0, 1.0],  # \"down\" - adverb\n",
    "    ]).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    words = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "    \n",
    "    return embeddings, words\n",
    "\n",
    "# Create interpretable data\n",
    "embeddings, words = create_simple_sequence_data()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "# Use embeddings as Q, K, V for self-attention\n",
    "output, attn_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attn_weights[0].detach().numpy(),\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    cmap='Blues',\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Keys (attending to)')\n",
    "plt.ylabel('Queries (attending from)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each row shows how much each word attends to other words.\")\n",
    "print(\"Higher values (darker blue) indicate stronger attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal (Masked) Attention\n",
    "\n",
    "For language modeling, we need causal attention where each position can only attend to previous positions (including itself). This prevents the model from \"cheating\" by looking at future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask where position i can only attend to positions j <= i.\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Create causal mask\n",
    "seq_len = 4\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal mask (1 = can attend, 0 = cannot attend):\")\n",
    "print(causal_mask.numpy())\n",
    "\n",
    "# Apply causal attention to our example\n",
    "output_causal, attn_weights_causal = scaled_dot_product_attention(\n",
    "    embeddings, embeddings, embeddings, mask=causal_mask.unsqueeze(0)\n",
    ")\n",
    "\n",
    "# Visualize causal attention\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Regular attention\n",
    "sns.heatmap(\n",
    "    attn_weights[0].detach().numpy(),\n",
    "    annot=True, fmt='.3f',\n",
    "    xticklabels=words, yticklabels=words,\n",
    "    cmap='Blues', ax=ax1\n",
    ")\n",
    "ax1.set_title('Regular Self-Attention')\n",
    "ax1.set_xlabel('Keys')\n",
    "ax1.set_ylabel('Queries')\n",
    "\n",
    "# Causal attention\n",
    "sns.heatmap(\n",
    "    attn_weights_causal[0].detach().numpy(),\n",
    "    annot=True, fmt='.3f',\n",
    "    xticklabels=words, yticklabels=words,\n",
    "    cmap='Blues', ax=ax2\n",
    ")\n",
    "ax2.set_title('Causal Self-Attention')\n",
    "ax2.set_xlabel('Keys')\n",
    "ax2.set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how causal attention has zeros in the upper triangle.\")\n",
    "print(\"Each word can only attend to itself and previous words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to attend to different types of relationships simultaneously. Instead of one attention computation, we run multiple \"heads\" in parallel and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.attention import MultiHeadAttention\n",
    "\n",
    "# Create multi-head attention layer\n",
    "d_model, n_heads = 8, 2\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "print(f\"Multi-head attention with {n_heads} heads\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Dimension per head: {d_model // n_heads}\")\n",
    "\n",
    "# Create input\n",
    "batch_size, seq_len = 1, 4\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Apply multi-head attention\n",
    "output, attention_weights = mha(x, x, x, return_attention=True)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"(batch_size, n_heads, seq_len, seq_len)\")\n",
    "\n",
    "# Visualize attention for each head\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(12, 5))\n",
    "if n_heads == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for head in range(n_heads):\n",
    "    head_attention = attention_weights[0, head].detach().numpy()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        head_attention,\n",
    "        annot=True, fmt='.3f',\n",
    "        cmap='Blues',\n",
    "        ax=axes[head],\n",
    "        cbar=head == n_heads - 1  # Only show colorbar for last plot\n",
    "    )\n",
    "    axes[head].set_title(f'Head {head + 1}')\n",
    "    axes[head].set_xlabel('Keys')\n",
    "    if head == 0:\n",
    "        axes[head].set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach head can learn to focus on different types of relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Attention Works\n",
    "\n",
    "Let's explore the key benefits of the attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_attention_benefits():\n",
    "    \"\"\"\n",
    "    Demonstrate key benefits of attention mechanism.\n",
    "    \"\"\"\n",
    "    print(\"🔍 Key Benefits of Attention:\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. **Parallel Processing**\")\n",
    "    print(\"   - Unlike RNNs, all positions can be computed in parallel\")\n",
    "    print(\"   - No sequential dependency during training\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. **Long-Range Dependencies**\")\n",
    "    print(\"   - Direct connections between any two positions\")\n",
    "    print(\"   - No degradation over distance like in RNNs\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. **Interpretability**\")\n",
    "    print(\"   - Attention weights show what the model is 'looking at'\")\n",
    "    print(\"   - Helps understand model behavior\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. **Flexibility**\")\n",
    "    print(\"   - Can attend to any part of the sequence\")\n",
    "    print(\"   - Multiple heads can capture different relationships\")\n",
    "    print()\n",
    "    \n",
    "    # Demonstrate computational complexity\n",
    "    seq_lengths = [100, 500, 1000, 2000]\n",
    "    \n",
    "    print(\"📊 Computational Complexity:\")\n",
    "    print(\"Sequence Length | Attention Ops | RNN Ops\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        attention_ops = seq_len ** 2  # O(n²) for attention matrix\n",
    "        rnn_ops = seq_len  # O(n) but sequential\n",
    "        print(f\"{seq_len:13} | {attention_ops:11,} | {rnn_ops:7,}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Attention is O(n²) in memory and computation,\")\n",
    "    print(\"but can be fully parallelized unlike RNNs.\")\n",
    "\n",
    "demonstrate_attention_benefits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise: Build Your Own Attention\n",
    "\n",
    "Try implementing a simple attention mechanism yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention_exercise():\n",
    "    \"\"\"\n",
    "    Exercise: Implement attention step by step.\n",
    "    \"\"\"\n",
    "    print(\"🎯 Exercise: Implement Simple Attention\")\n",
    "    print()\n",
    "    \n",
    "    # Given data\n",
    "    Q = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # 2x2 queries\n",
    "    K = torch.tensor([[1.0, 1.0], [1.0, 0.0]])  # 2x2 keys  \n",
    "    V = torch.tensor([[2.0, 0.0], [0.0, 3.0]])  # 2x2 values\n",
    "    \n",
    "    print(\"Given:\")\n",
    "    print(f\"Q = \\n{Q}\")\n",
    "    print(f\"K = \\n{K}\")\n",
    "    print(f\"V = \\n{V}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 1: Compute Q @ K.T\")\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"Scores = \\n{scores}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 2: Apply softmax\")\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    print(f\"Attention weights = \\n{attention_weights}\")\n",
    "    print(f\"Row sums: {attention_weights.sum(dim=-1)} (should be [1, 1])\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Step 3: Apply attention to values\")\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    print(f\"Output = \\n{output}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ Try calculating this by hand and compare with the result!\")\n",
    "\n",
    "simple_attention_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. **Scaled Dot-Product Attention** - The fundamental attention operation\n",
    "2. **Attention Visualization** - How to interpret attention weights\n",
    "3. **Causal Attention** - Preventing information leakage in language modeling\n",
    "4. **Multi-Head Attention** - Parallel attention heads for richer representations\n",
    "5. **Benefits of Attention** - Why it revolutionized NLP\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Attention allows models to dynamically focus on relevant parts of the input\n",
    "- The mechanism is based on similarity between queries and keys\n",
    "- Causal masking is essential for autoregressive language modeling\n",
    "- Multi-head attention captures different types of relationships\n",
    "- Attention enables parallelization and handles long-range dependencies\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll see how attention is combined with other components to build complete transformer blocks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}