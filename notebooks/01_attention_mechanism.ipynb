{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Understanding the Attention Mechanism\n\nAttention is the core innovation that makes transformers so powerful. Unlike previous models that process sequences linearly, attention allows models to directly connect any two positions in a sequence.\n\n## Why Attention Matters\n\nConsider this sentence: \"The bank by the river was closed because the bank couldn't process loans.\"\n\nHow do you know which \"bank\" means what? Your brain automatically focuses on context:\n- First \"bank\" + \"river\" ‚Üí riverside bank\n- Second \"bank\" + \"loans\" ‚Üí financial institution\n\nThis selective focusing is exactly what attention does in neural networks.\n\n## Core Concept\n\n**Attention computes a weighted average of values, where weights are determined by similarity between queries and keys.**\n\nFormula: `Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V`\n\nThink of it like a restaurant:\n- **Queries (Q)**: What the customer wants\n- **Keys (K)**: What's on the menu  \n- **Values (V)**: What's actually served\n\nThe waiter (attention) matches desires (Q) with options (K) to decide what to serve (V)!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Building Attention Step by Step\n\nLet's build the attention mechanism piece by piece to understand each component.\n\n### Step 1: The Core Idea\nWe want to compute how much each word should \"attend to\" (focus on) every other word.\n\n### Step 2: Measuring Similarity  \nWe measure similarity between words using **dot products**:\n- Similar words ‚Üí high dot product\n- Dissimilar words ‚Üí low dot product\n\n### Step 3: From Similarity to Attention Weights\n1. Compute similarity scores: `scores = query ¬∑ key`\n2. Scale to prevent extreme values: `scores = scores / ‚àöd_k`  \n3. Convert to probabilities: `weights = softmax(scores)`\n4. Use weights to combine values: `output = weights ¬∑ values`\n\n### The Complete Formula\nThis gives us the famous attention formula:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nWhere:\n- $Q$ are the queries (what we're looking for)\n- $K$ are the keys (what's available to attend to)  \n- $V$ are the values (what we actually use)\n- $d_k$ is the dimension of the keys (for numerical stability)\n\n**Why the scaling?** Without $\\sqrt{d_k}$, dot products become very large, making softmax too sharp (almost one-hot). Scaling keeps things smooth.\n\nLet's implement this step by step:"
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Attention From First Principles\n\n**The Problem**: How do we determine which parts of input are most relevant for each position?\n\n**The Solution**: Attention mechanism in 5 steps:\n\n1. **Compute similarity**: `scores = query ¬∑ key` (dot product measures similarity)\n2. **Scale for stability**: `scores = scores / ‚àöd_k` (prevents saturation)  \n3. **Apply causal mask**: `scores[future] = -‚àû` (for language modeling)\n4. **Normalize to probabilities**: `weights = softmax(scores)` (sums to 1)\n5. **Weighted combination**: `output = weights ¬∑ values` (final result)\n\n**Why scaling?** Without ‚àöd_k, large dot products make softmax too sharp (almost one-hot), losing the ability to attend to multiple positions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def scaled_dot_product_attention(Q, K, V, mask=None, show_steps=True):\n    d_k = Q.size(-1)\n    \n    # Step 1: Compute similarity scores\n    scores = torch.matmul(Q, K.transpose(-2, -1))\n    if show_steps:\n        print(f\"Step 1 - Raw scores range: [{scores.min():.3f}, {scores.max():.3f}]\")\n    \n    # Step 2: Scale for numerical stability\n    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n    if show_steps:\n        print(f\"Step 2 - Scaled range: [{scores.min():.3f}, {scores.max():.3f}]\")\n    \n    # Step 3: Apply mask if provided\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n        if show_steps:\n            print(f\"Step 3 - Applied causal mask\")\n    \n    # Step 4: Convert to probabilities\n    attention_weights = F.softmax(scores, dim=-1)\n    if show_steps:\n        print(f\"Step 4 - Attention weights sum: {attention_weights.sum(dim=-1)[0, 0]:.3f}\")\n    \n    # Step 5: Apply to values\n    output = torch.matmul(attention_weights, V)\n    if show_steps:\n        print(f\"Step 5 - Output shape: {output.shape}\")\n    \n    return output, attention_weights\n\n# Simple example to trace through\nQ = torch.tensor([[[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.]]])\nK = torch.tensor([[[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.]]])  \nV = torch.tensor([[[2., 1.], [1., 3.], [3., 2.]]])\n\nprint(\"üîç ATTENTION STEP-BY-STEP:\")\noutput, attn_weights = scaled_dot_product_attention(Q, K, V)\nprint(f\"\\nFinal attention weights:\\n{attn_weights[0].numpy()}\")\nprint(\"Notice: each query perfectly matches its corresponding key!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_sequence_data():\n",
    "    \"\"\"\n",
    "    Create a simple sequence where attention patterns should be interpretable.\n",
    "    \"\"\"\n",
    "    # Create embeddings for words: \"The\", \"cat\", \"sat\", \"down\"\n",
    "    seq_len, d_model = 4, 6\n",
    "    \n",
    "    # Manually create embeddings that should have interesting attention patterns\n",
    "    embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.0, 0.5, 0.0, 0.0],  # \"The\" - article\n",
    "        [0.0, 1.0, 1.0, 0.0, 0.5, 0.0],  # \"cat\" - noun\n",
    "        [0.0, 0.0, 0.5, 1.0, 1.0, 0.5],  # \"sat\" - verb\n",
    "        [0.5, 0.0, 0.0, 0.5, 1.0, 1.0],  # \"down\" - adverb\n",
    "    ]).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    words = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "    \n",
    "    return embeddings, words\n",
    "\n",
    "# Create interpretable data\n",
    "embeddings, words = create_simple_sequence_data()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "# Use embeddings as Q, K, V for self-attention\n",
    "output, attn_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attn_weights[0].detach().numpy(),\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    cmap='Blues',\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Keys (attending to)')\n",
    "plt.ylabel('Queries (attending from)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each row shows how much each word attends to other words.\")\n",
    "print(\"Higher values (darker blue) indicate stronger attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "def create_simple_sequence_data():\n    embeddings = torch.tensor([\n        [1.0, 0.5, 0.0, 0.5, 0.0, 0.0],  # \"The\" - article\n        [0.0, 1.0, 1.0, 0.0, 0.5, 0.0],  # \"cat\" - noun\n        [0.0, 0.0, 0.5, 1.0, 1.0, 0.5],  # \"sat\" - verb\n        [0.5, 0.0, 0.0, 0.5, 1.0, 1.0],  # \"down\" - adverb\n    ]).unsqueeze(0)\n    words = [\"The\", \"cat\", \"sat\", \"down\"]\n    return embeddings, words\n\nembeddings, words = create_simple_sequence_data()\noutput, attn_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings, show_steps=False)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    attn_weights[0].detach().numpy(),\n    annot=True, fmt='.3f',\n    xticklabels=words, yticklabels=words,\n    cmap='Blues', cbar_kws={'label': 'Attention Weight'}\n)\nplt.title('Self-Attention Weights')\nplt.xlabel('Keys (attending to)')\nplt.ylabel('Queries (attending from)')\nplt.show()\n\nprint(\"Each row shows how much each word attends to other words.\")\nprint(\"Higher values (darker blue) = stronger attention.\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Causal (Masked) Attention\n\nFor language modeling, we need **causal attention** - each position can only attend to previous positions, preventing the model from \"cheating\" by looking at future tokens.\n\n**Why mask future positions?** In language modeling, when predicting the next word, the model shouldn't see future words that haven't been generated yet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_causal_mask(seq_len):\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask\n\nseq_len = 4\ncausal_mask = create_causal_mask(seq_len)\nprint(\"Causal mask (1 = can attend, 0 = cannot attend):\")\nprint(causal_mask.numpy())\n\noutput_causal, attn_weights_causal = scaled_dot_product_attention(\n    embeddings, embeddings, embeddings, mask=causal_mask.unsqueeze(0), show_steps=False\n)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nsns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax1)\nax1.set_title('Regular Self-Attention')\n\nsns.heatmap(attn_weights_causal[0].detach().numpy(), annot=True, fmt='.3f',\n           xticklabels=words, yticklabels=words, cmap='Blues', ax=ax2)\nax2.set_title('Causal Self-Attention')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Causal attention: each word can only attend to itself and previous words.\")\nprint(\"Notice the upper triangle is zero - no peeking at future!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Multi-Head Attention\n\n**The Idea**: Instead of one attention computation, run multiple \"heads\" in parallel to capture different types of relationships.\n\n**Why multiple heads?** Different heads can learn to focus on different aspects:\n- Head 1: Subject-verb relationships  \n- Head 2: Adjective-noun relationships\n- Head 3: Long-range dependencies\n\nEach head uses different learned projections of Q, K, V."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.model.attention import MultiHeadAttention\n\nd_model, n_heads = 8, 2\nmha = MultiHeadAttention(d_model, n_heads)\nprint(f\"Multi-head attention: {n_heads} heads, {d_model // n_heads} dimensions per head\")\n\nbatch_size, seq_len = 1, 4\nx = torch.randn(batch_size, seq_len, d_model)\noutput, attention_weights = mha(x, x, x, return_attention=True)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attention_weights.shape}\")\n\nfig, axes = plt.subplots(1, n_heads, figsize=(12, 5))\nif n_heads == 1:\n    axes = [axes]\n\nfor head in range(n_heads):\n    head_attention = attention_weights[0, head].detach().numpy()\n    sns.heatmap(head_attention, annot=True, fmt='.3f', cmap='Blues', ax=axes[head])\n    axes[head].set_title(f'Head {head + 1}')\n\nplt.tight_layout()\nplt.show()\nprint(\"Each head learns different attention patterns!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Attention Mastery\n\nYou've learned the core mechanism that powers all transformers!\n\n### Key Concepts\n- **Attention formula**: `softmax(QK^T / ‚àöd_k)V` - weighted average based on similarity\n- **Scaling factor**: `‚àöd_k` prevents softmax saturation for numerical stability  \n- **Causal masking**: Prevents future information leakage in language modeling\n- **Multi-head attention**: Parallel heads capture different relationship types\n\n### Why Attention is Revolutionary\n**Before**: RNNs processed sequences step-by-step, limiting parallelization\n**After**: Attention connects any two positions directly, enabling parallelization\n\n### Applications\n- **Self-attention**: Each position attends to all positions in same sequence\n- **Cross-attention**: Queries from one sequence, keys/values from another (e.g., translation)\n- **Causal attention**: For autoregressive language modeling\n\n### Next Steps\nNow you understand attention! Next, we'll see how it combines with other components to build complete transformer blocks.\n\nThe foundation is solid - let's build transformers! üöÄ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}