{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Transformer Blocks\n\nAttention routes information between positions, but transformers need more components for complete functionality.\n\n## Why More Than Attention?\n\n**Attention limitations**:\n- Only mixes information (no position-wise processing)  \n- Can be unstable in deep networks\n- No complex transformations\n\n**Complete transformer blocks add**:\n- **Feed-Forward Networks**: Position-wise processing and transformations\n- **Layer Normalization**: Training stability  \n- **Residual Connections**: Enable deep network training\n\n## Architecture\nEach transformer block: `Attention + FFN + LayerNorm + Residuals`"
  },
  {
   "cell_type": "markdown",
   "source": "## Environment Setup\n\nImport required libraries and copy attention implementation from previous notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, Optional\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(\"Environment setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feed-Forward Networks\n\nPosition-wise processing with expand-contract architecture: `d_model → d_ff → d_model`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\n# Test feed-forward network\nd_model, d_ff = 8, 32\nff_net = FeedForward(d_model, d_ff)\n\nx = torch.randn(1, 4, d_model)\noutput = ff_net(x)\n\nprint(f\"Feed-Forward Network:\")\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Architecture: {d_model} → {d_ff} → {d_model} (expand-contract)\")\n\ntotal_params = sum(p.numel() for p in ff_net.parameters())\nprint(f\"Parameters: {total_params:,}\")\nprint(\"✅ Position-wise processing with non-linear transformations!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate layer normalization\nx = torch.tensor([\n    [[1.0, 2.0, 3.0, 4.0],\n     [100.0, 200.0, 300.0, 400.0]]\n])\n\nprint(\"Problem: Different scales break training!\")\nprint(f\"Position 1: {x[0,0].tolist()}\")\nprint(f\"  → mean={x[0,0].mean():.1f}, std={x[0,0].std():.1f}\")\nprint(f\"Position 2: {x[0,1].tolist()}\")\nprint(f\"  → mean={x[0,1].mean():.1f}, std={x[0,1].std():.1f}\")\n\nlayer_norm = nn.LayerNorm(4)\nx_normalized = layer_norm(x)\n\nprint(\"\\nSolution: LayerNorm fixes the scale problem!\")\nprint(f\"Position 1: {[round(val, 3) for val in x_normalized[0,0].tolist()]}\")\nprint(f\"  → mean={x_normalized[0,0].mean():.3f}, std={x_normalized[0,0].std():.3f}\")\nprint(f\"Position 2: {[round(val, 3) for val in x_normalized[0,1].tolist()]}\")\nprint(f\"  → mean={x_normalized[0,1].mean():.3f}, std={x_normalized[0,1].std():.3f}\")\n\nprint(\"✅ Both positions now have mean≈0, std≈1\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Residual Connections\n\nEnable deep networks by creating gradient highways: `output = x + f(x)`."
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate residual connections\nx = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\nprint(f\"Input: {x.squeeze().tolist()}\")\n\n# Weak transformation that would lose signal\nweak_transform = nn.Linear(4, 4)\nwith torch.no_grad():\n    weak_transform.weight.fill_(0.01)\n    weak_transform.bias.zero_()\n\noutput_no_res = weak_transform(x)\nprint(f\"Without residual: {[round(val, 3) for val in output_no_res.squeeze().tolist()]} (signal lost!)\")\n\noutput_with_res = x + weak_transform(x)\nprint(f\"With residual:    {[round(val, 3) for val in output_with_res.squeeze().tolist()]} (signal preserved!)\")\n\nprint(\"✅ Residual connections preserve signals and enable deep networks\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Complete Transformer Block\n\nIntegrate all components using pre-norm architecture for stability."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Attention sublayer with pre-norm and residual\n        normed = self.norm1(x)\n        attn_out = self.attention(normed, normed, normed, mask)\n        x = x + self.dropout(attn_out)\n        \n        # Feed-forward sublayer with pre-norm and residual\n        normed = self.norm2(x)\n        ff_out = self.feed_forward(normed)\n        x = x + self.dropout(ff_out)\n        \n        return x\n\n# Test complete transformer block\nd_model, n_heads, d_ff = 8, 2, 32\nblock = TransformerBlock(d_model, n_heads, d_ff)\n\nx = torch.randn(1, 4, d_model)\noutput = block(x)\n\nprint(f\"Complete Transformer Block:\")\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\ntotal_params = sum(p.numel() for p in block.parameters())\nprint(f\"Total parameters: {total_params:,}\")\nprint(\"✅ Successfully combines attention + FFN + LayerNorm + residuals!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stacking Transformer Blocks\n\nStack multiple blocks to build deep transformers with hierarchical learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleTransformer(nn.Module):\n    def __init__(self, n_layers, d_model, n_heads, d_ff):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacked transformer\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\n\nprint(f\"Stacked Transformer:\")\nprint(f\"Layers: {n_layers}\")\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\nHierarchical learning:\")\nprint(f\"• Layer 1: Basic features and simple patterns\")\nprint(f\"• Layer 2: More complex relationships\")\nprint(f\"• Layer 3: High-level abstractions\")\nprint(\"✅ Deep networks learn increasingly complex representations!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've built complete transformer blocks from first principles!\n\n**Key Components**:\n- **Feed-Forward Networks**: Position-wise processing with expand-contract architecture\n- **Layer Normalization**: Stabilizes training by normalizing feature scales\n- **Residual Connections**: Enable deep networks via gradient highways\n- **Integration**: Pre-norm architecture for stable deep training\n\n**Architecture Pattern**: `x → LayerNorm → Attention → Residual → LayerNorm → FFN → Residual`\n\n**Why It Works**:\n- Attention routes information between positions\n- FFN processes each position independently  \n- LayerNorm maintains stable scales\n- Residuals preserve gradient flow\n\n**Next**: Add positional encoding to give transformers spatial awareness!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "class SimpleTransformer(nn.Module):\n    def __init__(self, n_layers, d_model, n_heads, d_ff):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacked transformer\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\n\nprint(f\"Stacked Transformer:\")\nprint(f\"Layers: {n_layers}\")\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\nHierarchical learning:\")\nprint(f\"• Layer 1: Basic features and simple patterns\")\nprint(f\"• Layer 2: More complex relationships\")\nprint(f\"• Layer 3: High-level abstractions\")\nprint(\"✅ Deep networks learn increasingly complex representations!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stacking Transformer Blocks\n\nThe power of transformers comes from stacking multiple blocks. Each layer can learn increasingly complex patterns and relationships.\n\n**Why stacking works**:\n- Layer 1: Basic features and simple attention patterns  \n- Layer 2: More complex interactions between positions\n- Layer 3+: High-level reasoning and abstract relationships\n\nResidual connections make deep stacking possible by preserving gradient flow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple multi-layer transformer for demonstration\nclass SimpleTransformer(nn.Module):\n    \"\"\"Stack of transformer blocks.\"\"\"\n    \n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        \n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacking\nprint(\"📚 STACKING TRANSFORMER BLOCKS\")\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"Layers:           {n_layers}\")\nprint(f\"Input shape:      {x.shape}\")\nprint(f\"Output shape:     {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Per layer:        {params_per_layer:,}\")\n\nprint(f\"\\n✨ Each layer can learn different patterns:\")\nprint(f\"• Layer 1: Basic features and attention patterns\")\nprint(f\"• Layer 2: More complex relationships\") \nprint(f\"• Layer 3: High-level abstractions and reasoning\")\n\nprint(f\"\\n🔑 KEY INSIGHT: Deep networks can learn hierarchical representations!\")\nprint(f\"✅ Residual connections make deep stacking possible\")\nprint(f\"✅ Each layer builds on previous layers' understanding\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}