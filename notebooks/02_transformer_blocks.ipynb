{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Transformer Blocks\n\nIn the previous notebook, we explored the attention mechanism. Now we'll see how attention is combined with other components to create complete transformer blocks.\n\n## The Big Picture: Why Do We Need More Than Attention?\n\nAttention is powerful, but it has limitations:\n\n1. **Attention only mixes information** - it's like shuffling cards but not changing their values\n2. **No position-wise processing** - each word is processed identically  \n3. **Training instability** - deep networks can be hard to train\n4. **Information bottlenecks** - gradients can vanish in deep networks\n\nTransformer blocks solve these problems by adding:\n- **Feed-Forward Networks** ‚Üí Transform information, not just mix it\n- **Layer Normalization** ‚Üí Stabilize training  \n- **Residual Connections** ‚Üí Preserve gradient flow\n\nThink of it like this:\n- **Attention**: \"Let me gather relevant information from other words\"\n- **Feed-Forward**: \"Now let me think about what this information means\"\n- **Layer Norm**: \"Keep everything balanced and stable\"\n- **Residuals**: \"Don't forget what I started with\"\n\n## What You'll Learn\n\n1. **Feed-Forward Networks** - The \"thinking\" component of transformers\n2. **Layer Normalization** - Stabilizing training dynamics\n3. **Residual Connections** - Enabling deep networks to train\n4. **Complete Transformer Block** - How everything fits together\n5. **Stacking Blocks** - Building deep transformers\n\nLet's start building!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Feed-Forward Networks: Position-wise Processing\n\nAfter attention routes information between positions, each position needs **individual processing**. That's where Feed-Forward Networks (FFNs) come in.\n\n### The Core Problem ü§î\n- **Attention**: Routes information (\"look at relevant context\")\n- **FFN**: Processes information (\"think about what this means\")\n\n### FFN Architecture\n```\nFFN(x) = ReLU(xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ\n```\n\n**Key properties:**\n- **Position-wise**: Same transformation applied to each position independently\n- **Expand-contract**: `d_model ‚Üí d_ff ‚Üí d_model` (typically 4√ó expansion)  \n- **Non-linear**: ReLU enables complex transformations\n\nThink of it like workstations on an assembly line - same tools, different inputs at each position."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the actual FFN implementation we'll use\nfrom src.model.feedforward import FeedForward\n\n# Understanding the dimensions - this is crucial!\nprint(\"üß† UNDERSTANDING TRANSFORMER DIMENSIONS\")\nprint(\"=\" * 40)\n\nd_model = 8   # Model dimension (embedding size) - how we represent each word\nd_ff = 32     # Feed-forward dimension - internal processing width  \n\nprint(f\"d_model = {d_model}\")\nprint(\"  ‚Ü≥ This is how many features each word position has\")\nprint(\"  ‚Ü≥ Like having 8 attributes to describe each word\")\nprint()\nprint(f\"d_ff = {d_ff}\")  \nprint(\"  ‚Ü≥ FFN expands to this size for internal processing\")\nprint(f\"  ‚Ü≥ {d_ff // d_model}x expansion (standard is 4x)\")\nprint(\"  ‚Ü≥ More space = more complex transformations\")\n\nprint(\"\\nüîÑ FFN PROCESSING FLOW:\")\nprint(f\"Input:  [seq_len, {d_model}]    # Each position has {d_model} features\")\nprint(f\"Expand: [seq_len, {d_ff}]    # Expand to {d_ff} for processing\") \nprint(f\"Output: [seq_len, {d_model}]    # Contract back to {d_model}\")\n\n# Now demonstrate with actual data\nbatch_size, seq_len = 1, 4\nff_net = FeedForward(d_model, d_ff, dropout=0.1)\nx = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"\\nüìä PRACTICAL DEMONSTRATION:\")\nprint(f\"Input shape:  {x.shape}\")\noutput = ff_net(x)\nprint(f\"Output shape: {output.shape}\")\n\n# Show parameter breakdown\ntotal_params = sum(p.numel() for p in ff_net.parameters())\nlinear1_params = d_model * d_ff + d_ff  # W1 + b1\nlinear2_params = d_ff * d_model + d_model  # W2 + b2\n\nprint(f\"\\nüîß PARAMETER BREAKDOWN:\")\nprint(f\"Linear1 ({d_model}‚Üí{d_ff}): {linear1_params:,} params\")\nprint(f\"Linear2 ({d_ff}‚Üí{d_model}): {linear2_params:,} params\")\nprint(f\"Total FFN params:    {total_params:,}\")\n\nprint(f\"\\n‚ú® Why this architecture works:\")\nprint(f\"‚úÖ Expansion gives more 'thinking space' for each position\") \nprint(f\"‚úÖ Each position processed independently (no cross-talk)\")\nprint(f\"‚úÖ Same output size maintains compatibility with attention\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Layer Normalization: Why We Need It\n\nBefore we dive into how Layer Normalization works, let's understand **why** transformers need it.\n\n### The Problem: Training Instability üå™Ô∏è\n\nImagine training a transformer without normalization:\n- **Early training**: Features have small values like `[0.1, 0.2, 0.3]`\n- **Later training**: Same features explode to `[100, 200, 300]`  \n- **Result**: Gradients become too large or too small ‚Üí training fails\n\n### What Layer Normalization Does üéØ\n\nLayer Norm solves this by **normalizing each position's features**:\n\n```\nFor each position in the sequence:\n1. Calculate mean and std of that position's features\n2. Normalize: (features - mean) / std  \n3. Apply learnable scale (Œ≥) and shift (Œ≤) parameters\n```\n\n### LayerNorm Formula\n$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n\n- Œº, œÉ: mean and std **across features for each position**\n- Œ≥, Œ≤: learnable parameters (scale and shift)\n- Œµ: small value (1e-5) to prevent division by zero\n\n### Key Insight: Position-wise Normalization\n- Each position in the sequence is normalized **independently**\n- If position 1 has values `[1, 2, 3, 4]` and position 2 has `[100, 200, 300, 400]`\n- Both get normalized to have mean‚âà0, std‚âà1 **separately**\n\nThink of it as giving each position a \"fresh start\" with well-behaved values!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's see LayerNorm in action with a clear, step-by-step example\nprint(\"üîß LAYER NORMALIZATION STEP-BY-STEP\")\nprint(\"=\" * 50)\n\n# Create data with problematic scales (this breaks training!)\nx = torch.tensor([\n    [[1.0, 2.0, 3.0, 4.0],        # Position 1: small values  \n     [100.0, 200.0, 300.0, 400.0]]  # Position 2: huge values!\n])\nprint(\"üö® PROBLEM: Different positions have wildly different scales!\")\nprint(f\"Position 1: {x[0,0].tolist()}\")\nprint(f\"  ‚Üí mean={x[0,0].mean():.1f}, std={x[0,0].std():.1f}\")\nprint(f\"Position 2: {x[0,1].tolist()}\")  \nprint(f\"  ‚Üí mean={x[0,1].mean():.1f}, std={x[0,1].std():.1f}\")\nprint(\"  ‚Üí These scale differences will break gradient descent!\")\n\nprint(\"\\n‚ú® SOLUTION: Apply LayerNorm to each position\")\n\n# Apply layer normalization\nlayer_norm = nn.LayerNorm(4)  # Normalize across the 4 features\nx_normalized = layer_norm(x)\n\nprint(\"After LayerNorm:\")\nprint(f\"Position 1: {[round(val, 3) for val in x_normalized[0,0].tolist()]}\")\nprint(f\"  ‚Üí mean={x_normalized[0,0].mean():.3f}, std={x_normalized[0,0].std():.3f}\")\nprint(f\"Position 2: {[round(val, 3) for val in x_normalized[0,1].tolist()]}\")\nprint(f\"  ‚Üí mean={x_normalized[0,1].mean():.3f}, std={x_normalized[0,1].std():.3f}\")\n\n# Show what LayerNorm learned\nprint(f\"\\nüéõÔ∏è  LayerNorm learned parameters:\")\nprint(f\"Scale (Œ≥): {layer_norm.weight.tolist()}\")\nprint(f\"Shift (Œ≤): {layer_norm.bias.tolist()}\")\n\nprint(f\"\\n‚úÖ SUCCESS: LayerNorm fixed the scale problem!\")\nprint(f\"‚Ä¢ Both positions now have mean‚âà0, std‚âà1\")\nprint(f\"‚Ä¢ Gradients can flow properly during training\")\nprint(f\"‚Ä¢ Each position normalized independently\")\n\n# Manual calculation to show how it works\nprint(f\"\\nüîç HOW IT WORKS (Position 1 example):\")\npos1_original = x[0, 0]\npos1_mean = pos1_original.mean()\npos1_std = pos1_original.std()\npos1_normalized = (pos1_original - pos1_mean) / pos1_std\nprint(f\"Original: {pos1_original.tolist()}\")\nprint(f\"Mean: {pos1_mean:.1f}, Std: {pos1_std:.1f}\")\nprint(f\"(x - mean) / std: {[round(val.item(), 3) for val in pos1_normalized]}\")\nprint(f\"Final (with Œ≥,Œ≤): {[round(val, 3) for val in x_normalized[0,0].tolist()]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Residual Connections: Gradient Highways\n\nDeep networks suffer from **vanishing gradients** - signals become weaker as they pass through many layers. Residual connections solve this.\n\n### The Solution: Skip Connections üõ£Ô∏è\nInstead of `output = f(x)`, use:\n$$\\text{output} = x + f(x)$$\n\n**Why this works:**\n- Gradient flows directly through the `+ x` path (always = 1)  \n- Even if `‚àáf(x)` vanishes, gradients still flow via the skip connection\n- Like having highway and local roads for traffic\n\n### Pre-norm vs Post-norm Architecture\n- **Post-norm**: `LayerNorm(x + sublayer(x))` (original)\n- **Pre-norm**: `x + sublayer(LayerNorm(x))` (modern, more stable)\n\nWe'll use pre-norm because it's more stable for deep networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick demonstration of residual connections\nprint(\"üõ£Ô∏è  RESIDUAL CONNECTION DEMONSTRATION\")\n\nx = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\nprint(f\"Input:           {x.squeeze().tolist()}\")\n\n# Simulate a layer that might cause vanishing gradients\nweak_transform = nn.Linear(4, 4)\nwith torch.no_grad():\n    weak_transform.weight.fill_(0.01)  # Very small weights\n    weak_transform.bias.zero_()\n\n# Without residual connection\noutput_no_res = weak_transform(x)\nprint(f\"Without residual: {output_no_res.squeeze().tolist()!r} (signal lost!)\")\n\n# With residual connection  \noutput_with_res = x + weak_transform(x)\nprint(f\"With residual:    {output_with_res.squeeze().tolist()!r} (signal preserved!)\")\n\nprint(f\"\\n‚ú® Residual connections preserve the original signal!\")\nprint(f\"‚úÖ Enable training of very deep networks\")\nprint(f\"‚úÖ Gradient highways prevent vanishing gradients\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Complete Transformer Block: Putting It All Together\n\nNow we can assemble a complete transformer block using all three components:\n\n1. **Multi-Head Attention** (from notebook 1) + Layer Norm + Residual\n2. **Feed-Forward Network** + Layer Norm + Residual\n\n### Pre-norm Architecture (Modern Standard)\n```python\n# Step 1: Attention with pre-norm\nnormed = LayerNorm(x)\nattention_out = MultiHeadAttention(normed)\nx = x + attention_out  # Residual connection\n\n# Step 2: Feed-forward with pre-norm  \nnormed = LayerNorm(x)\nff_out = FeedForward(normed)\nx = x + ff_out  # Residual connection\n```\n\nThis creates a stable, trainable building block that we can stack into deep networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.model.attention import MultiHeadAttention\nfrom src.model.feedforward import FeedForward\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Complete transformer block with pre-norm architecture.\"\"\"\n    \n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        \n        # Core components\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        \n        # Normalization layers  \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        \"\"\"Pre-norm transformer block forward pass.\"\"\"\n        \n        # Step 1: Multi-head attention with pre-norm and residual\n        normed = self.norm1(x)\n        attn_out = self.attention(normed, normed, normed, mask)\n        x = x + self.dropout(attn_out)\n        \n        # Step 2: Feed-forward with pre-norm and residual\n        normed = self.norm2(x)\n        ff_out = self.feed_forward(normed)\n        x = x + self.dropout(ff_out)\n        \n        return x\n\n# Test the complete transformer block\nprint(\"üèóÔ∏è  COMPLETE TRANSFORMER BLOCK\")\nd_model, n_heads, d_ff = 8, 2, 32\nblock = TransformerBlock(d_model, n_heads, d_ff)\n\n# Forward pass\nx = torch.randn(1, 4, d_model)\noutput = block(x)\n\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\n# Analyze parameter distribution\nattention_params = sum(p.numel() for p in block.attention.parameters())\nff_params = sum(p.numel() for p in block.feed_forward.parameters())\nnorm_params = sum(p.numel() for p in block.norm1.parameters()) + sum(p.numel() for p in block.norm2.parameters())\ntotal_params = attention_params + ff_params + norm_params\n\nprint(f\"\\nüìä PARAMETER BREAKDOWN:\")\nprint(f\"Attention:     {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\nprint(f\"Feed-forward:  {ff_params:,} ({ff_params/total_params*100:.1f}%)\")  \nprint(f\"Layer norms:   {norm_params:,} ({norm_params/total_params*100:.1f}%)\")\nprint(f\"Total:         {total_params:,}\")\n\nprint(f\"\\n‚ú® The transformer block successfully combines all components!\")\nprint(f\"‚úÖ Attention routes information between positions\")\nprint(f\"‚úÖ FFN processes each position independently\") \nprint(f\"‚úÖ LayerNorm provides training stability\")\nprint(f\"‚úÖ Residuals enable deep network training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking Transformer Blocks\n",
    "\n",
    "The power of transformers comes from stacking multiple blocks. Each block can learn different types of patterns and relationships. Let's see how information flows through a stack of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple stacked transformer for clear demonstration\nclass SimpleTransformer(nn.Module):\n    \"\"\"Simple transformer with multiple blocks for analysis.\"\"\"\n    \n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        self.n_layers = n_layers\n        \n        # Stack of transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        \n        # Final layer norm\n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        # Store representations at each layer for analysis\n        layer_outputs = [x.detach().clone()]\n        \n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            layer_outputs.append(x.detach().clone())\n            print(f\"After Layer {i+1}: mean={x.mean().item():.3f}, std={x.std().item():.3f}\")\n        \n        # Final normalization\n        x = self.final_norm(x)\n        layer_outputs.append(x.detach().clone())\n        print(f\"After Final Norm: mean={x.mean().item():.3f}, std={x.std().item():.3f}\")\n        \n        return x, layer_outputs\n\nprint(\"üìö STACKING TRANSFORMER BLOCKS\")\nprint(\"=\" * 40)\n\n# Create a 3-layer transformer with clear dimensions\nd_model, n_heads, d_ff = 8, 2, 32  # Same dimensions we explained earlier\nn_layers = 3\n\ntransformer = SimpleTransformer(n_layers, d_model, n_heads, d_ff)\n\n# Create input representing a sequence of 4 tokens\nx = torch.randn(1, 4, d_model)  # [batch=1, seq_len=4, d_model=8]\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"d_model: {d_model}, n_heads: {n_heads}, d_ff: {d_ff}\")\nprint(f\"Number of layers: {n_layers}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"\\nüìä PARAMETER ANALYSIS:\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Parameters per layer: {params_per_layer:,}\")\n\n# Forward pass with monitoring\nprint(f\"\\nüîÑ FORWARD PASS THROUGH ALL LAYERS:\")\nprint(f\"Input: mean={x.mean().item():.3f}, std={x.std().item():.3f}\")\n\noutput, layer_outputs = transformer(x)\n\nprint(f\"\\n‚ú® WHAT EACH LAYER LEARNS:\")\nprint(f\"‚Ä¢ Layer 1: Basic feature combinations and simple attention patterns\")\nprint(f\"‚Ä¢ Layer 2: More complex interactions between positions\")\nprint(f\"‚Ä¢ Layer 3: High-level reasoning and abstract relationships\")\n\nprint(f\"\\nüîë KEY OBSERVATIONS:\")\nprint(f\"‚úÖ Each layer transforms the representation differently\")\nprint(f\"‚úÖ LayerNorm keeps values well-behaved throughout\")\nprint(f\"‚úÖ Residual connections preserve important information\")\nprint(f\"‚úÖ The network can learn increasingly complex patterns\")\n\n# Show how the representation changes\nprint(f\"\\nüìà REPRESENTATION EVOLUTION:\")\nfor i, layer_out in enumerate(layer_outputs):\n    magnitude = torch.norm(layer_out).item()\n    if i == 0:\n        print(f\"Input:        magnitude={magnitude:.2f}\")\n    elif i <= n_layers:\n        print(f\"Layer {i}:      magnitude={magnitude:.2f}\")\n    else:\n        print(f\"Final output: magnitude={magnitude:.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple multi-layer transformer for demonstration\nclass SimpleTransformer(nn.Module):\n    \"\"\"Stack of transformer blocks.\"\"\"\n    \n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        \n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacking\nprint(\"üìö STACKING TRANSFORMER BLOCKS\")\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"Layers:           {n_layers}\")\nprint(f\"Input shape:      {x.shape}\")\nprint(f\"Output shape:     {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Per layer:        {params_per_layer:,}\")\n\nprint(f\"\\n‚ú® Each layer can learn different patterns:\")\nprint(f\"‚Ä¢ Layer 1: Basic features and attention patterns\")\nprint(f\"‚Ä¢ Layer 2: More complex relationships\") \nprint(f\"‚Ä¢ Layer 3: High-level abstractions and reasoning\")\n\nprint(f\"\\nüîë KEY INSIGHT: Deep networks can learn hierarchical representations!\")\nprint(f\"‚úÖ Residual connections make deep stacking possible\")\nprint(f\"‚úÖ Each layer builds on previous layers' understanding\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}