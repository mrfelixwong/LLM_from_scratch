{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Transformer Blocks\n\nIn the previous notebook, we explored the attention mechanism. Now we'll see how attention is combined with other components to create complete transformer blocks.\n\n## The Big Picture: Why Do We Need More Than Attention?\n\nAttention is powerful, but it has limitations:\n\n1. **Attention only mixes information** - it's like shuffling cards but not changing their values\n2. **No position-wise processing** - each word is processed identically  \n3. **Training instability** - deep networks can be hard to train\n4. **Information bottlenecks** - gradients can vanish in deep networks\n\nTransformer blocks solve these problems by adding:\n- **Feed-Forward Networks** ‚Üí Transform information, not just mix it\n- **Layer Normalization** ‚Üí Stabilize training  \n- **Residual Connections** ‚Üí Preserve gradient flow\n\nThink of it like this:\n- **Attention**: \"Let me gather relevant information from other words\"\n- **Feed-Forward**: \"Now let me think about what this information means\"\n- **Layer Norm**: \"Keep everything balanced and stable\"\n- **Residuals**: \"Don't forget what I started with\"\n\n## What You'll Learn\n\n1. **Feed-Forward Networks** - The \"thinking\" component of transformers\n2. **Layer Normalization** - Stabilizing training dynamics\n3. **Residual Connections** - Enabling deep networks to train\n4. **Complete Transformer Block** - How everything fits together\n5. **Stacking Blocks** - Building deep transformers\n\nLet's start building!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Feed-Forward Networks: Position-wise Processing\n\nAfter attention routes information between positions, each position needs **individual processing**. That's where Feed-Forward Networks (FFNs) come in.\n\n### The Core Problem ü§î\n- **Attention**: Routes information (\"look at relevant context\")\n- **FFN**: Processes information (\"think about what this means\")\n\n### FFN Architecture\n```\nFFN(x) = ReLU(xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ\n```\n\n**Key properties:**\n- **Position-wise**: Same transformation applied to each position independently\n- **Expand-contract**: `d_model ‚Üí d_ff ‚Üí d_model` (typically 4√ó expansion)  \n- **Non-linear**: ReLU enables complex transformations\n\nThink of it like workstations on an assembly line - same tools, different inputs at each position."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the actual FFN implementation we'll use\nfrom src.model.feedforward import FeedForward\n\n# Create and test feed-forward network  \nd_model, d_ff = 8, 32  # 4x expansion\nbatch_size, seq_len = 1, 4\n\nff_net = FeedForward(d_model, d_ff, dropout=0.1)\nx = torch.randn(batch_size, seq_len, d_model)\n\nprint(f\"üìä FFN DEMONSTRATION\")\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"d_model ‚Üí d_ff: {d_model} ‚Üí {d_ff} (4x expansion)\")\n\n# Forward pass\noutput = ff_net(x)\nprint(f\"Output shape: {output.shape}\")\n\n# Parameter analysis\ntotal_params = sum(p.numel() for p in ff_net.parameters())\nlinear1_params = d_model * d_ff + d_ff  # weights + bias\nlinear2_params = d_ff * d_model + d_model  # weights + bias\n\nprint(f\"\\nüîß PARAMETER BREAKDOWN:\")\nprint(f\"Linear 1 (expand):   {linear1_params:,}\")\nprint(f\"Linear 2 (contract): {linear2_params:,}\") \nprint(f\"Total FFN params:    {total_params:,}\")\nprint(f\"\\n‚ú® The FFN transforms each position's representation independently!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Layer Normalization: Training Stability\n\nDeep networks can be unstable during training due to internal covariate shift. Layer normalization solves this by normalizing each position's features.\n\n### Why Layer Norm? üéØ\n- **Problem**: Feature values can vary wildly across layers and training steps\n- **Solution**: Normalize features to have mean=0, std=1 for each position\n- **Benefit**: Stable gradients and faster convergence\n\n### LayerNorm Formula\n$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta$$\n\nWhere Œº and œÉ are computed across the feature dimension for each position independently."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate LayerNorm effect with a clear example\nprint(\"üîß LAYER NORMALIZATION DEMONSTRATION\")\n\n# Create data with different scales (this causes training instability)\nx = torch.tensor([\n    [[1.0, 2.0, 3.0, 4.0],      # Position 1: small values  \n     [100.0, 200.0, 300.0, 400.0]], # Position 2: large values\n])\n\nprint(\"Before LayerNorm:\")\nprint(f\"Position 1: {x[0,0].tolist()} (mean={x[0,0].mean():.1f}, std={x[0,0].std():.1f})\")\nprint(f\"Position 2: {x[0,1].tolist()} (mean={x[0,1].mean():.1f}, std={x[0,1].std():.1f})\")\n\n# Apply layer normalization\nlayer_norm = nn.LayerNorm(4)\nx_normalized = layer_norm(x)\n\nprint(f\"\\nAfter LayerNorm:\")\nprint(f\"Position 1: {x_normalized[0,0].tolist()!r} (mean={x_normalized[0,0].mean():.3f})\")\nprint(f\"Position 2: {x_normalized[0,1].tolist()!r} (mean={x_normalized[0,1].mean():.3f})\")\n\nprint(f\"\\n‚ú® LayerNorm normalizes each position independently!\")\nprint(f\"‚úÖ Mean ‚âà 0, Std ‚âà 1 for both positions\")\nprint(f\"‚úÖ Removes scale differences that hurt training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Residual Connections: Gradient Highways\n\nDeep networks suffer from **vanishing gradients** - signals become weaker as they pass through many layers. Residual connections solve this.\n\n### The Solution: Skip Connections üõ£Ô∏è\nInstead of `output = f(x)`, use:\n$$\\text{output} = x + f(x)$$\n\n**Why this works:**\n- Gradient flows directly through the `+ x` path (always = 1)  \n- Even if `‚àáf(x)` vanishes, gradients still flow via the skip connection\n- Like having highway and local roads for traffic\n\n### Pre-norm vs Post-norm Architecture\n- **Post-norm**: `LayerNorm(x + sublayer(x))` (original)\n- **Pre-norm**: `x + sublayer(LayerNorm(x))` (modern, more stable)\n\nWe'll use pre-norm because it's more stable for deep networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick demonstration of residual connections\nprint(\"üõ£Ô∏è  RESIDUAL CONNECTION DEMONSTRATION\")\n\nx = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\nprint(f\"Input:           {x.squeeze().tolist()}\")\n\n# Simulate a layer that might cause vanishing gradients\nweak_transform = nn.Linear(4, 4)\nwith torch.no_grad():\n    weak_transform.weight.fill_(0.01)  # Very small weights\n    weak_transform.bias.zero_()\n\n# Without residual connection\noutput_no_res = weak_transform(x)\nprint(f\"Without residual: {output_no_res.squeeze().tolist()!r} (signal lost!)\")\n\n# With residual connection  \noutput_with_res = x + weak_transform(x)\nprint(f\"With residual:    {output_with_res.squeeze().tolist()!r} (signal preserved!)\")\n\nprint(f\"\\n‚ú® Residual connections preserve the original signal!\")\nprint(f\"‚úÖ Enable training of very deep networks\")\nprint(f\"‚úÖ Gradient highways prevent vanishing gradients\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Complete Transformer Block: Putting It All Together\n\nNow we can assemble a complete transformer block using all three components:\n\n1. **Multi-Head Attention** (from notebook 1) + Layer Norm + Residual\n2. **Feed-Forward Network** + Layer Norm + Residual\n\n### Pre-norm Architecture (Modern Standard)\n```python\n# Step 1: Attention with pre-norm\nnormed = LayerNorm(x)\nattention_out = MultiHeadAttention(normed)\nx = x + attention_out  # Residual connection\n\n# Step 2: Feed-forward with pre-norm  \nnormed = LayerNorm(x)\nff_out = FeedForward(normed)\nx = x + ff_out  # Residual connection\n```\n\nThis creates a stable, trainable building block that we can stack into deep networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.model.attention import MultiHeadAttention\nfrom src.model.feedforward import FeedForward\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Complete transformer block with pre-norm architecture.\"\"\"\n    \n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        \n        # Core components\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        \n        # Normalization layers  \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        \"\"\"Pre-norm transformer block forward pass.\"\"\"\n        \n        # Step 1: Multi-head attention with pre-norm and residual\n        normed = self.norm1(x)\n        attn_out = self.attention(normed, normed, normed, mask)\n        x = x + self.dropout(attn_out)\n        \n        # Step 2: Feed-forward with pre-norm and residual\n        normed = self.norm2(x)\n        ff_out = self.feed_forward(normed)\n        x = x + self.dropout(ff_out)\n        \n        return x\n\n# Test the complete transformer block\nprint(\"üèóÔ∏è  COMPLETE TRANSFORMER BLOCK\")\nd_model, n_heads, d_ff = 8, 2, 32\nblock = TransformerBlock(d_model, n_heads, d_ff)\n\n# Forward pass\nx = torch.randn(1, 4, d_model)\noutput = block(x)\n\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\n# Analyze parameter distribution\nattention_params = sum(p.numel() for p in block.attention.parameters())\nff_params = sum(p.numel() for p in block.feed_forward.parameters())\nnorm_params = sum(p.numel() for p in block.norm1.parameters()) + sum(p.numel() for p in block.norm2.parameters())\ntotal_params = attention_params + ff_params + norm_params\n\nprint(f\"\\nüìä PARAMETER BREAKDOWN:\")\nprint(f\"Attention:     {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\nprint(f\"Feed-forward:  {ff_params:,} ({ff_params/total_params*100:.1f}%)\")  \nprint(f\"Layer norms:   {norm_params:,} ({norm_params/total_params*100:.1f}%)\")\nprint(f\"Total:         {total_params:,}\")\n\nprint(f\"\\n‚ú® The transformer block successfully combines all components!\")\nprint(f\"‚úÖ Attention routes information between positions\")\nprint(f\"‚úÖ FFN processes each position independently\") \nprint(f\"‚úÖ LayerNorm provides training stability\")\nprint(f\"‚úÖ Residuals enable deep network training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking Transformer Blocks\n",
    "\n",
    "The power of transformers comes from stacking multiple blocks. Each block can learn different types of patterns and relationships. Let's see how information flows through a stack of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Simple transformer with multiple blocks for analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, return_all_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass through all transformer blocks.\n",
    "        \"\"\"\n",
    "        all_attention_weights = []\n",
    "        layer_outputs = [x.clone()]  # Store output from each layer\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if return_all_attention:\n",
    "                x, attention_weights, _ = block(x, return_attention=True)\n",
    "                all_attention_weights.append(attention_weights)\n",
    "            else:\n",
    "                x, _ = block(x)\n",
    "            \n",
    "            layer_outputs.append(x.clone())\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.final_norm(x)\n",
    "        layer_outputs.append(x.clone())\n",
    "        \n",
    "        if return_all_attention:\n",
    "            return x, all_attention_weights, layer_outputs\n",
    "        return x, layer_outputs\n",
    "\n",
    "# Create a 3-layer transformer\n",
    "n_layers = 3\n",
    "transformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(1, 4, 8)\n",
    "\n",
    "print(f\"Transformer with {n_layers} layers\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Forward pass\n",
    "output, all_attention, layer_outputs = transformer(x, return_all_attention=True)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention matrices: {len(all_attention)}\")\n",
    "print(f\"Number of layer outputs: {len(layer_outputs)}\")\n",
    "\n",
    "# Analyze how representations change through layers\n",
    "print(\"\\nRepresentation analysis through layers:\")\n",
    "for i, layer_output in enumerate(layer_outputs):\n",
    "    norm = torch.norm(layer_output).item()\n",
    "    mean = layer_output.mean().item()\n",
    "    std = layer_output.std().item()\n",
    "    \n",
    "    if i == 0:\n",
    "        layer_name = \"Input\"\n",
    "    elif i <= n_layers:\n",
    "        layer_name = f\"Layer {i}\"\n",
    "    else:\n",
    "        layer_name = \"Final Norm\"\n",
    "    \n",
    "    print(f\"{layer_name:12}: norm={norm:6.3f}, mean={mean:6.3f}, std={std:6.3f}\")\n",
    "\n",
    "# Visualize attention patterns across layers\n",
    "fig, axes = plt.subplots(1, n_layers, figsize=(15, 4))\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    # Average attention across heads\n",
    "    avg_attention = all_attention[layer_idx][0].mean(dim=0).detach().numpy()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        avg_attention,\n",
    "        annot=True, fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        ax=axes[layer_idx],\n",
    "        cbar=layer_idx == n_layers - 1\n",
    "    )\n",
    "    axes[layer_idx].set_title(f'Layer {layer_idx + 1}\\nAttention')\n",
    "    axes[layer_idx].set_xlabel('Keys')\n",
    "    if layer_idx == 0:\n",
    "        axes[layer_idx].set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how different layers learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Scaling Up: Stacking Blocks\n\nThe power of transformers comes from stacking multiple blocks. Each layer learns increasingly complex patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple multi-layer transformer for demonstration\nclass SimpleTransformer(nn.Module):\n    \"\"\"Stack of transformer blocks.\"\"\"\n    \n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        \n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacking\nprint(\"üìö STACKING TRANSFORMER BLOCKS\")\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"Layers:           {n_layers}\")\nprint(f\"Input shape:      {x.shape}\")\nprint(f\"Output shape:     {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Per layer:        {params_per_layer:,}\")\n\nprint(f\"\\n‚ú® Each layer can learn different patterns:\")\nprint(f\"‚Ä¢ Layer 1: Basic features and attention patterns\")\nprint(f\"‚Ä¢ Layer 2: More complex relationships\") \nprint(f\"‚Ä¢ Layer 3: High-level abstractions and reasoning\")\n\nprint(f\"\\nüîë KEY INSIGHT: Deep networks can learn hierarchical representations!\")\nprint(f\"‚úÖ Residual connections make deep stacking possible\")\nprint(f\"‚úÖ Each layer builds on previous layers' understanding\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Building Blocks of Transformers\n\nWe've learned how to build complete transformer blocks by combining four essential components:\n\n### The Architecture Stack üìö\n1. **Feed-Forward Networks** ‚Üí Process each position independently  \n2. **Layer Normalization** ‚Üí Stabilize training dynamics\n3. **Residual Connections** ‚Üí Enable deep network training via gradient highways\n4. **Multi-Head Attention** ‚Üí Route information between positions\n\n### Key Design Choices ‚öôÔ∏è\n- **Pre-norm architecture**: More stable than post-norm for deep networks\n- **4x FFN expansion**: Standard ratio for d_ff = 4 √ó d_model  \n- **Parameter distribution**: ~67% in FFN, ~33% in attention, <1% in norms\n\n### The Big Picture üéØ\nEach transformer block performs two main operations:\n1. **Communication**: Attention mixes information between positions\n2. **Computation**: FFN processes each position's updated representation\n\nStacking multiple blocks creates increasingly sophisticated representations, enabled by residual connections that maintain gradient flow through arbitrary depth.\n\n---\n\n**Next**: We'll explore how transformers understand word order through positional encoding!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}