{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Transformer Blocks\n\nIn the previous notebook, we explored the attention mechanism. Now we'll see how attention is combined with other components to create complete transformer blocks.\n\n## The Big Picture: Why Do We Need More Than Attention?\n\nAttention is powerful, but it has limitations:\n\n1. **Attention only mixes information** - it's like shuffling cards but not changing their values\n2. **No position-wise processing** - each word is processed identically  \n3. **Training instability** - deep networks can be hard to train\n4. **Information bottlenecks** - gradients can vanish in deep networks\n\nTransformer blocks solve these problems by adding:\n- **Feed-Forward Networks** ‚Üí Transform information, not just mix it\n- **Layer Normalization** ‚Üí Stabilize training  \n- **Residual Connections** ‚Üí Preserve gradient flow\n\nThink of it like this:\n- **Attention**: \"Let me gather relevant information from other words\"\n- **Feed-Forward**: \"Now let me think about what this information means\"\n- **Layer Norm**: \"Keep everything balanced and stable\"\n- **Residuals**: \"Don't forget what I started with\"\n\n## What You'll Learn\n\n1. **Feed-Forward Networks** - The \"thinking\" component of transformers\n2. **Layer Normalization** - Stabilizing training dynamics\n3. **Residual Connections** - Enabling deep networks to train\n4. **Complete Transformer Block** - How everything fits together\n5. **Stacking Blocks** - Building deep transformers\n\nLet's start building!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Feed-Forward Networks: The \"Thinking\" Component\n\nAfter attention gathers relevant information from other positions, we need to **process** that information. That's where Feed-Forward Networks (FFNs) come in.\n\n### Why Do We Need FFNs? ü§î\n\nAttention is great at **routing information** but it can't **transform** it:\n- Attention: \"The word 'bank' should look at 'river' and 'loans'\"  \n- FFN: \"Based on seeing 'river', this 'bank' means 'riverside'\" \n\n### The FFN Formula\nFFNs apply the same transformation to each position independently:\n\n$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n\n**Key insights:**\n- **Position-wise**: Each word gets the same transformation (but with different inputs)\n- **Non-linear**: ReLU allows complex transformations\n- **Expand-contract**: Typically `d_ff = 4 √ó d_model` for more expressivity\n\n### The Factory Analogy üè≠\nThink of FFNs like assembly line stations:\n- Each position is a workstation\n- Same tools (weights) at each station  \n- Each workstation processes different items (word representations)\n- First layer **expands** the representation (more features)\n- Second layer **contracts** back to original size"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    \"\"\"Simple feed-forward network with visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # First linear transformation\n",
    "        hidden = self.linear1(x)  # [batch_size, seq_len, d_ff]\n",
    "        \n",
    "        # ReLU activation\n",
    "        activated = self.relu(hidden)  # [batch_size, seq_len, d_ff]\n",
    "        \n",
    "        # Second linear transformation back to d_model\n",
    "        output = self.linear2(activated)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return output, {'hidden': hidden, 'activated': activated}\n",
    "        return output\n",
    "\n",
    "# Create and test feed-forward network\n",
    "d_model, d_ff = 8, 32  # 4x expansion\n",
    "batch_size, seq_len = 2, 4\n",
    "\n",
    "ff_net = SimpleFeedForward(d_model, d_ff)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass with intermediate values\n",
    "output, intermediates = ff_net(x, return_intermediate=True)\n",
    "\n",
    "print(f\"Hidden shape (after linear1): {intermediates['hidden'].shape}\")\n",
    "print(f\"Activated shape (after ReLU): {intermediates['activated'].shape}\")\n",
    "print(f\"Output shape (after linear2): {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in ff_net.parameters())\n",
    "print(f\"\\nFeed-forward parameters: {total_params:,}\")\n",
    "print(f\"  Linear1: {d_model * d_ff + d_ff:,} (weights + bias)\")\n",
    "print(f\"  Linear2: {d_ff * d_model + d_model:,} (weights + bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Normalization\n",
    "\n",
    "Layer normalization stabilizes training by normalizing inputs to each layer. Unlike batch normalization, it normalizes across the feature dimension for each individual example.\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "Where $\\mu$ and $\\sigma$ are the mean and standard deviation across the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_layer_norm():\n",
    "    \"\"\"Demonstrate how layer normalization works.\"\"\"\n",
    "    \n",
    "    # Create example data with different scales\n",
    "    batch_size, seq_len, d_model = 2, 3, 4\n",
    "    \n",
    "    # Create data where different examples have different scales\n",
    "    x = torch.tensor([\n",
    "        [[1.0, 2.0, 3.0, 4.0],    # Example 1, position 1\n",
    "         [10.0, 20.0, 30.0, 40.0], # Example 1, position 2  \n",
    "         [0.1, 0.2, 0.3, 0.4]],   # Example 1, position 3\n",
    "        \n",
    "        [[100.0, 200.0, 300.0, 400.0],  # Example 2, position 1\n",
    "         [5.0, 6.0, 7.0, 8.0],          # Example 2, position 2\n",
    "         [0.01, 0.02, 0.03, 0.04]]      # Example 2, position 3\n",
    "    ])\n",
    "    \n",
    "    print(\"Original data:\")\n",
    "    print(f\"Shape: {x.shape}\")\n",
    "    print(\"Example 1:\")\n",
    "    print(x[0])\n",
    "    print(\"Example 2:\")\n",
    "    print(x[1])\n",
    "    \n",
    "    # Apply layer normalization\n",
    "    layer_norm = nn.LayerNorm(d_model)\n",
    "    x_normalized = layer_norm(x)\n",
    "    \n",
    "    print(\"\\nAfter Layer Normalization:\")\n",
    "    print(\"Example 1:\")\n",
    "    print(x_normalized[0])\n",
    "    print(\"Example 2:\")\n",
    "    print(x_normalized[1])\n",
    "    \n",
    "    # Check normalization properties\n",
    "    print(\"\\nNormalization properties:\")\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            mean = x_normalized[i, j].mean()\n",
    "            std = x_normalized[i, j].std()\n",
    "            print(f\"Example {i+1}, Position {j+1}: mean={mean:.6f}, std={std:.6f}\")\n",
    "    \n",
    "    return x, x_normalized\n",
    "\n",
    "# Demonstrate layer normalization\n",
    "original, normalized = demonstrate_layer_norm()\n",
    "\n",
    "# Visualize the effect\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original data\n",
    "im1 = ax1.imshow(original.view(-1, original.size(-1)).numpy(), cmap='viridis', aspect='auto')\n",
    "ax1.set_title('Original Data')\n",
    "ax1.set_xlabel('Feature Dimension')\n",
    "ax1.set_ylabel('Batch √ó Sequence Position')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Normalized data\n",
    "im2 = ax2.imshow(normalized.view(-1, normalized.size(-1)).detach().numpy(), cmap='viridis', aspect='auto')\n",
    "ax2.set_title('After Layer Normalization')\n",
    "ax2.set_xlabel('Feature Dimension')\n",
    "ax2.set_ylabel('Batch √ó Sequence Position')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how layer norm brings all values to a similar scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Residual Connections: The Gradient Highway\n\nResidual connections are one of the most important innovations in deep learning. They solve the fundamental problem of **vanishing gradients** in deep networks.\n\n### The Problem: Vanishing Gradients üìâ\nWithout residual connections, gradients get smaller and smaller as they flow backwards through layers:\n- Layer 10: gradient = 1.0\n- Layer 5: gradient = 0.1  \n- Layer 1: gradient = 0.001 ‚Üí can't learn!\n\n### The Solution: Gradient Highways üõ£Ô∏è\nResidual connections create \"highways\" for gradients:\n\n$$\\text{output} = x + \\text{Sublayer}(x)$$\n\n**Why this works:**\n- The gradient of `x + f(x)` includes both `‚àáf(x)` and `1` (from the identity)\n- Even if `‚àáf(x)` vanishes, the `1` ensures gradients flow back\n- It's like having both local roads (sublayer) and highways (residual) for traffic\n\n### Architecture Choices: Pre-norm vs Post-norm\n\n**Post-norm (original)**: `LayerNorm(x + Sublayer(x))`  \n**Pre-norm (modern)**: `x + Sublayer(LayerNorm(x))`\n\nPre-norm is more stable because:\n- Normalization happens before potentially destabilizing operations\n- Direct path from output to input preserves gradients better\n\n### The Highway Analogy üöó\nThink of residuals like highway systems:\n- **Local roads** (sublayers): Can get congested or blocked\n- **Highway** (residual): Always provides a direct route\n- **Traffic** (gradients): Can always flow even if local roads are slow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_residual_connections():\n",
    "    \"\"\"Show why residual connections are important.\"\"\"\n",
    "    \n",
    "    d_model = 4\n",
    "    x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])  # [1, d_model]\n",
    "    \n",
    "    # Simulate a transformation that might hurt the signal\n",
    "    # (e.g., a poorly initialized layer)\n",
    "    transformation = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    # Make weights very small to simulate vanishing gradients\n",
    "    with torch.no_grad():\n",
    "        transformation.weight.fill_(0.01)\n",
    "        transformation.bias.fill_(0.0)\n",
    "    \n",
    "    # Without residual connection\n",
    "    output_no_residual = transformation(x)\n",
    "    \n",
    "    # With residual connection\n",
    "    output_with_residual = x + transformation(x)\n",
    "    \n",
    "    print(\"Demonstrating Residual Connections:\")\n",
    "    print(f\"Original input: {x.squeeze()}\")\n",
    "    print(f\"Transformation output: {output_no_residual.squeeze()}\")\n",
    "    print(f\"With residual connection: {output_with_residual.squeeze()}\")\n",
    "    \n",
    "    print(\"\\nKey insights:\")\n",
    "    print(\"- Without residual: signal becomes very small (vanishing gradients)\")\n",
    "    print(\"- With residual: original signal is preserved + small modification\")\n",
    "    print(\"- This allows training very deep networks!\")\n",
    "    \n",
    "    return x, output_no_residual, output_with_residual\n",
    "\n",
    "# Demonstrate residual connections\n",
    "original, no_res, with_res = demonstrate_residual_connections()\n",
    "\n",
    "# Visualize the effect\n",
    "positions = range(len(original.squeeze()))\n",
    "values_original = original.squeeze().numpy()\n",
    "values_no_res = no_res.squeeze().detach().numpy()\n",
    "values_with_res = with_res.squeeze().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(positions, values_original, 'o-', label='Original Input', linewidth=2, markersize=8)\n",
    "plt.plot(positions, values_no_res, 's-', label='Without Residual', linewidth=2, markersize=8)\n",
    "plt.plot(positions, values_with_res, '^-', label='With Residual', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Feature Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Effect of Residual Connections')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Transformer Block\n",
    "\n",
    "Now let's combine everything into a complete transformer block. The standard architecture is:\n",
    "\n",
    "1. **Multi-Head Attention** with residual connection and layer norm\n",
    "2. **Feed-Forward Network** with residual connection and layer norm\n",
    "\n",
    "Using Pre-Norm architecture:\n",
    "```\n",
    "x = x + attention(layer_norm(x))\n",
    "x = x + ffn(layer_norm(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.attention import MultiHeadAttention\n",
    "from src.model.feedforward import FeedForward\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete transformer block with visualization capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            mask: Optional attention mask\n",
    "            return_attention: Whether to return attention weights\n",
    "        \"\"\"\n",
    "        # Store for visualization\n",
    "        intermediates = {}\n",
    "        intermediates['input'] = x.clone()\n",
    "        \n",
    "        # Multi-head attention with residual connection\n",
    "        normed1 = self.norm1(x)\n",
    "        intermediates['normed1'] = normed1.clone()\n",
    "        \n",
    "        if return_attention:\n",
    "            attn_out, attention_weights = self.attention(normed1, normed1, normed1, mask, return_attention=True)\n",
    "            intermediates['attention_weights'] = attention_weights\n",
    "        else:\n",
    "            attn_out = self.attention(normed1, normed1, normed1, mask)\n",
    "            attention_weights = None\n",
    "        \n",
    "        attn_out = self.dropout(attn_out)\n",
    "        x = x + attn_out  # Residual connection\n",
    "        intermediates['after_attention'] = x.clone()\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        normed2 = self.norm2(x)\n",
    "        intermediates['normed2'] = normed2.clone()\n",
    "        \n",
    "        ff_out = self.feed_forward(normed2)\n",
    "        ff_out = self.dropout(ff_out)\n",
    "        x = x + ff_out  # Residual connection\n",
    "        intermediates['output'] = x.clone()\n",
    "        \n",
    "        if return_attention:\n",
    "            return x, attention_weights, intermediates\n",
    "        return x, intermediates\n",
    "\n",
    "# Create and test transformer block\n",
    "d_model, n_heads, d_ff = 8, 2, 32\n",
    "block = TransformerBlock(d_model, n_heads, d_ff)\n",
    "\n",
    "# Create input\n",
    "batch_size, seq_len = 1, 4\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in block.parameters()):,}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attention_weights, intermediates = block(x, return_attention=True)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Analyze the transformations\n",
    "print(\"\\nStep-by-step analysis:\")\n",
    "input_norm = torch.norm(intermediates['input']).item()\n",
    "after_attn_norm = torch.norm(intermediates['after_attention']).item()\n",
    "output_norm = torch.norm(intermediates['output']).item()\n",
    "\n",
    "print(f\"Input norm: {input_norm:.3f}\")\n",
    "print(f\"After attention norm: {after_attn_norm:.3f}\")\n",
    "print(f\"Final output norm: {output_norm:.3f}\")\n",
    "\n",
    "# Show that residual connections preserve information\n",
    "attention_contribution = torch.norm(intermediates['after_attention'] - intermediates['input']).item()\n",
    "ff_contribution = torch.norm(intermediates['output'] - intermediates['after_attention']).item()\n",
    "\n",
    "print(f\"\\nContribution analysis:\")\n",
    "print(f\"Attention contribution: {attention_contribution:.3f}\")\n",
    "print(f\"Feed-forward contribution: {ff_contribution:.3f}\")\n",
    "print(\"\\nBoth components modify the input while preserving the original signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking Transformer Blocks\n",
    "\n",
    "The power of transformers comes from stacking multiple blocks. Each block can learn different types of patterns and relationships. Let's see how information flows through a stack of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Simple transformer with multiple blocks for analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, return_all_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass through all transformer blocks.\n",
    "        \"\"\"\n",
    "        all_attention_weights = []\n",
    "        layer_outputs = [x.clone()]  # Store output from each layer\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if return_all_attention:\n",
    "                x, attention_weights, _ = block(x, return_attention=True)\n",
    "                all_attention_weights.append(attention_weights)\n",
    "            else:\n",
    "                x, _ = block(x)\n",
    "            \n",
    "            layer_outputs.append(x.clone())\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.final_norm(x)\n",
    "        layer_outputs.append(x.clone())\n",
    "        \n",
    "        if return_all_attention:\n",
    "            return x, all_attention_weights, layer_outputs\n",
    "        return x, layer_outputs\n",
    "\n",
    "# Create a 3-layer transformer\n",
    "n_layers = 3\n",
    "transformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(1, 4, 8)\n",
    "\n",
    "print(f\"Transformer with {n_layers} layers\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Forward pass\n",
    "output, all_attention, layer_outputs = transformer(x, return_all_attention=True)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention matrices: {len(all_attention)}\")\n",
    "print(f\"Number of layer outputs: {len(layer_outputs)}\")\n",
    "\n",
    "# Analyze how representations change through layers\n",
    "print(\"\\nRepresentation analysis through layers:\")\n",
    "for i, layer_output in enumerate(layer_outputs):\n",
    "    norm = torch.norm(layer_output).item()\n",
    "    mean = layer_output.mean().item()\n",
    "    std = layer_output.std().item()\n",
    "    \n",
    "    if i == 0:\n",
    "        layer_name = \"Input\"\n",
    "    elif i <= n_layers:\n",
    "        layer_name = f\"Layer {i}\"\n",
    "    else:\n",
    "        layer_name = \"Final Norm\"\n",
    "    \n",
    "    print(f\"{layer_name:12}: norm={norm:6.3f}, mean={mean:6.3f}, std={std:6.3f}\")\n",
    "\n",
    "# Visualize attention patterns across layers\n",
    "fig, axes = plt.subplots(1, n_layers, figsize=(15, 4))\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    # Average attention across heads\n",
    "    avg_attention = all_attention[layer_idx][0].mean(dim=0).detach().numpy()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        avg_attention,\n",
    "        annot=True, fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        ax=axes[layer_idx],\n",
    "        cbar=layer_idx == n_layers - 1\n",
    "    )\n",
    "    axes[layer_idx].set_title(f'Layer {layer_idx + 1}\\nAttention')\n",
    "    axes[layer_idx].set_xlabel('Keys')\n",
    "    if layer_idx == 0:\n",
    "        axes[layer_idx].set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how different layers learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Analysis\n",
    "\n",
    "Let's analyze where most parameters are located in a transformer and how this scales with model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_transformer_parameters():\n    \"\"\"Analyze parameter distribution in transformers.\"\"\"\n    \n    # Test different model sizes\n    configs = [\n        {'name': 'Tiny', 'd_model': 64, 'n_heads': 2, 'd_ff': 256, 'n_layers': 2},\n        {'name': 'Small', 'd_model': 128, 'n_heads': 4, 'd_ff': 512, 'n_layers': 6},\n        {'name': 'Medium', 'd_model': 256, 'n_heads': 8, 'd_ff': 1024, 'n_layers': 12},\n        {'name': 'Large', 'd_model': 512, 'n_heads': 16, 'd_ff': 2048, 'n_layers': 24},\n    ]\n    \n    results = []\n    \n    for config in configs:\n        # Create single transformer block\n        block = TransformerBlock(\n            d_model=config['d_model'],\n            n_heads=config['n_heads'],\n            d_ff=config['d_ff']\n        )\n        \n        # Count parameters by component\n        attention_params = sum(p.numel() for p in block.attention.parameters())\n        ff_params = sum(p.numel() for p in block.feed_forward.parameters())\n        norm_params = sum(p.numel() for p in block.norm1.parameters()) + sum(p.numel() for p in block.norm2.parameters())\n        \n        total_per_block = attention_params + ff_params + norm_params\n        total_model = total_per_block * config['n_layers']\n        \n        results.append({\n            'name': config['name'],\n            'attention': attention_params,\n            'ff': ff_params,\n            'norm': norm_params,\n            'per_block': total_per_block,\n            'total': total_model,\n            'n_layers': config['n_layers']\n        })\n    \n    # Display results\n    print(\"Parameter Analysis by Model Size:\")\n    print(\"=\" * 80)\n    print(f\"{'Model':<8} {'Attention':<12} {'Feed-Forward':<14} {'Layer Norm':<12} {'Per Block':<12} {'Total':<12}\")\n    print(\"-\" * 80)\n    \n    for r in results:\n        print(f\"{r['name']:<8} {r['attention']:>10,} {r['ff']:>12,} {r['norm']:>10,} {r['per_block']:>10,} {r['total']:>10,}\")\n    \n    # Calculate percentages for the medium model\n    medium = results[2]  # Medium model\n    print(f\"\\nParameter Distribution (Medium Model):\")\n    print(f\"Attention: {medium['attention']/medium['per_block']*100:.1f}%\")\n    print(f\"Feed-Forward: {medium['ff']/medium['per_block']*100:.1f}%\")\n    print(f\"Layer Norm: {medium['norm']/medium['per_block']*100:.1f}%\")\n    \n    # Visualize parameter distribution\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Model size comparison\n    model_names = [r['name'] for r in results]\n    total_params = [r['total'] for r in results]\n    \n    ax1.bar(model_names, total_params, color='skyblue')\n    ax1.set_ylabel('Total Parameters')\n    ax1.set_title('Total Parameters by Model Size')\n    ax1.set_yscale('log')\n    \n    # Add parameter counts as labels\n    for i, v in enumerate(total_params):\n        ax1.text(i, v, f'{v:,}', ha='center', va='bottom')\n    \n    # Component breakdown for medium model\n    components = ['Attention', 'Feed-Forward', 'Layer Norm']\n    component_params = [medium['attention'], medium['ff'], medium['norm']]\n    colors = ['lightcoral', 'lightgreen', 'lightblue']\n    \n    ax2.pie(component_params, labels=components, colors=colors, autopct='%1.1f%%')\n    ax2.set_title('Parameter Distribution\\n(Medium Model, Per Block)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nKey Insights:\")\n    print(\"‚Ä¢ Most parameters are in the feed-forward networks (~67%)\")\n    print(\"‚Ä¢ Attention mechanisms use ~33% of parameters\")\n    print(\"‚Ä¢ Layer normalization uses <1% of parameters\")\n    print(\"‚Ä¢ Parameters scale roughly as O(d_model¬≤) due to linear layers\")\n\nanalyze_transformer_parameters()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we've built complete transformer blocks by combining:\n\n1. **Feed-Forward Networks** - Process each position independently with 2-layer MLPs\n2. **Layer Normalization** - Stabilize training by normalizing features\n3. **Residual Connections** - Enable deep networks by preserving gradient flow\n4. **Complete Blocks** - Combine attention + FFN with proper normalization\n5. **Stacking** - Multiple blocks learn hierarchical representations\n\n### Key Architecture Insights:\n\n- **Pre-Norm vs Post-Norm**: Pre-norm (norm before sublayer) is more stable\n- **Parameter Distribution**: ~67% in FFN, ~33% in attention\n- **Residual Connections**: Essential for training deep networks\n- **Layer Normalization**: Provides training stability\n\n### Design Principles:\n\n- Each component serves a specific purpose\n- Residual connections preserve information flow\n- Normalization enables stable training\n- Stacking enables learning complex patterns\n\n## üö® Common Misconceptions to Avoid\n\n**‚ùå \"Attention is like human attention\"**  \n‚Üí ‚úÖ It's more like **information routing** - deciding which information to send where\n\n**‚ùå \"Residuals just add skip connections\"**  \n‚Üí ‚úÖ They create **gradient highways** that enable deep network training\n\n**‚ùå \"Layer norm just normalizes\"**  \n‚Üí ‚úÖ It **stabilizes training dynamics** and enables faster convergence\n\n**‚ùå \"FFNs are just simple MLPs\"**  \n‚Üí ‚úÖ They're the **knowledge storage** - where most parameters and \"facts\" live\n\n**‚ùå \"Bigger models are always better\"**  \n‚Üí ‚úÖ There are **efficiency trade-offs** - bigger isn't always better for your use case\n\n## üî¨ Try This Yourself!\n\nBefore moving on, experiment with these questions:\n1. What happens if you remove residual connections? (Hint: try a 6-layer model)\n2. How does attention change from layer 1 to layer 6?  \n3. What if you make d_ff smaller or larger than 4√ód_model?\n\nNext, we'll explore how transformers understand position with positional encoding!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}