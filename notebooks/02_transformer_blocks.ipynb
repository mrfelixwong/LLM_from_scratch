{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Transformer Blocks\n\nAttention is powerful, but alone it's just information routing. To build complete transformers, we need three more components:\n\n## Why More Than Attention?\n\n**Attention limitations**:\n- Only mixes information (like shuffling cards)\n- No position-wise processing\n- Training instability in deep networks\n\n**Complete transformer blocks add**:\n- **Feed-Forward Networks**: Transform information, not just mix it\n- **Layer Normalization**: Stabilize training dynamics  \n- **Residual Connections**: Enable deep network training\n\n## Learning Objectives\n\n1. **Feed-Forward Networks**: The \"thinking\" component \n2. **Layer Normalization**: Training stability\n3. **Residual Connections**: Gradient flow in deep networks\n4. **Complete Transformer Block**: Integration of all components\n5. **Stacking Blocks**: Building deep transformers\n\nLet's build complete transformer blocks!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, Optional\n\nplt.style.use('default')\nsns.set_palette(\"husl\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(\"Environment setup complete!\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feed-Forward Networks: Position-wise Processing\n\n**The Role**: After attention routes information between positions, each position needs individual processing.\n\n**Architecture**: `FFN(x) = ReLU(xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ`\n\n**Key Properties**:\n- **Position-wise**: Same transformation applied to each position independently\n- **Expand-contract**: `d_model ‚Üí d_ff ‚Üí d_model` (typically 4√ó expansion)\n- **Non-linear**: ReLU enables complex transformations\n\nThink of it like specialized workstations - same tools, different inputs at each position."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.model.feedforward import FeedForward\n\nd_model, d_ff = 8, 32\nff_net = FeedForward(d_model, d_ff, dropout=0.1)\n\nprint(f\"Feed-Forward Network:\")\nprint(f\"d_model = {d_model} (input/output dimension)\")\nprint(f\"d_ff = {d_ff} (internal expansion, {d_ff//d_model}x larger)\")\n\nprint(f\"\\nProcessing flow:\")\nprint(f\"Input:  [seq_len, {d_model}] ‚Üí Each position has {d_model} features\")\nprint(f\"Expand: [seq_len, {d_ff}] ‚Üí More space for complex processing\")\nprint(f\"Output: [seq_len, {d_model}] ‚Üí Back to original dimension\")\n\nbatch_size, seq_len = 1, 4\nx = torch.randn(batch_size, seq_len, d_model)\noutput = ff_net(x)\n\nprint(f\"\\nDemonstration:\")\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\ntotal_params = sum(p.numel() for p in ff_net.parameters())\nlinear1_params = d_model * d_ff + d_ff\nlinear2_params = d_ff * d_model + d_model\n\nprint(f\"\\nParameter breakdown:\")\nprint(f\"Linear1 ({d_model}‚Üí{d_ff}): {linear1_params:,} params\")\nprint(f\"Linear2 ({d_ff}‚Üí{d_model}): {linear2_params:,} params\")\nprint(f\"Total: {total_params:,} params\")\n\nprint(f\"\\n‚úÖ FFN provides position-wise transformation capability!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Layer Normalization: Training Stability\n\n**The Problem**: Without normalization, different positions can have wildly different feature scales, breaking gradient descent.\n\n**The Solution**: Normalize each position's features independently.\n\n**Formula**: `LayerNorm(x) = Œ≥ ¬∑ (x - Œº) / œÉ + Œ≤`\n- Œº, œÉ: mean and std across features for each position\n- Œ≥, Œ≤: learnable scale and shift parameters\n\n**Key Insight**: Each position is normalized separately, giving every position a \"fresh start\" with well-behaved values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x = torch.tensor([\n    [[1.0, 2.0, 3.0, 4.0],\n     [100.0, 200.0, 300.0, 400.0]]\n])\n\nprint(\"üö® PROBLEM: Different scales break training!\")\nprint(f\"Position 1: {x[0,0].tolist()}\")\nprint(f\"  ‚Üí mean={x[0,0].mean():.1f}, std={x[0,0].std():.1f}\")\nprint(f\"Position 2: {x[0,1].tolist()}\")\nprint(f\"  ‚Üí mean={x[0,1].mean():.1f}, std={x[0,1].std():.1f}\")\n\nlayer_norm = nn.LayerNorm(4)\nx_normalized = layer_norm(x)\n\nprint(\"\\n‚ú® SOLUTION: LayerNorm fixes the scale problem!\")\nprint(f\"Position 1: {[round(val, 3) for val in x_normalized[0,0].tolist()]}\")\nprint(f\"  ‚Üí mean={x_normalized[0,0].mean():.3f}, std={x_normalized[0,0].std():.3f}\")\nprint(f\"Position 2: {[round(val, 3) for val in x_normalized[0,1].tolist()]}\")\nprint(f\"  ‚Üí mean={x_normalized[0,1].mean():.3f}, std={x_normalized[0,1].std():.3f}\")\n\nprint(f\"\\nLayerNorm parameters:\")\nprint(f\"Scale (Œ≥): {layer_norm.weight.tolist()}\")\nprint(f\"Shift (Œ≤): {layer_norm.bias.tolist()}\")\n\nprint(f\"\\n‚úÖ Both positions now have mean‚âà0, std‚âà1\")\nprint(f\"‚úÖ Gradients can flow properly during training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Residual Connections: Gradient Highways\n\n**The Problem**: Deep networks suffer from vanishing gradients - signals become weaker through many layers.\n\n**The Solution**: Skip connections enable direct gradient flow.\n\n**Formula**: `output = x + f(x)` instead of `output = f(x)`\n\n**Why it works**: Gradient flows directly through the `+ x` path (gradient = 1), creating \"gradient highways\" even when `‚àáf(x)` vanishes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\nprint(f\"Input: {x.squeeze().tolist()}\")\n\nweak_transform = nn.Linear(4, 4)\nwith torch.no_grad():\n    weak_transform.weight.fill_(0.01)\n    weak_transform.bias.zero_()\n\noutput_no_res = weak_transform(x)\nprint(f\"Without residual: {[round(val, 3) for val in output_no_res.squeeze().tolist()]} (signal lost!)\")\n\noutput_with_res = x + weak_transform(x)\nprint(f\"With residual:    {[round(val, 3) for val in output_with_res.squeeze().tolist()]} (signal preserved!)\")\n\nprint(f\"\\n‚úÖ Residual connections preserve the original signal\")\nprint(f\"‚úÖ Enable training of very deep networks\")\nprint(f\"‚úÖ Create gradient highways preventing vanishing gradients\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Complete Transformer Block\n\nNow we integrate all components using **pre-norm architecture** (modern standard):\n\n```\n# Step 1: Attention sublayer\nnormed = LayerNorm(x)\nattention_out = MultiHeadAttention(normed)  \nx = x + attention_out  # Residual\n\n# Step 2: Feed-forward sublayer\nnormed = LayerNorm(x)\nff_out = FeedForward(normed)\nx = x + ff_out  # Residual\n```\n\nPre-norm is more stable than post-norm for deep networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.model.attention import MultiHeadAttention\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Attention sublayer with pre-norm and residual\n        normed = self.norm1(x)\n        attn_out = self.attention(normed, normed, normed, mask)\n        x = x + self.dropout(attn_out)\n        \n        # Feed-forward sublayer with pre-norm and residual\n        normed = self.norm2(x)\n        ff_out = self.feed_forward(normed)\n        x = x + self.dropout(ff_out)\n        \n        return x\n\nd_model, n_heads, d_ff = 8, 2, 32\nblock = TransformerBlock(d_model, n_heads, d_ff)\n\nx = torch.randn(1, 4, d_model)\noutput = block(x)\n\nprint(f\"Complete Transformer Block:\")\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\nattention_params = sum(p.numel() for p in block.attention.parameters())\nff_params = sum(p.numel() for p in block.feed_forward.parameters())\nnorm_params = sum(p.numel() for p in [block.norm1, block.norm2])\n\nprint(f\"\\nParameter breakdown:\")\nprint(f\"Attention:    {attention_params:,}\")\nprint(f\"Feed-forward: {ff_params:,}\")\nprint(f\"Layer norms:  {norm_params:,}\")\nprint(f\"Total:        {attention_params + ff_params + norm_params:,}\")\n\nprint(f\"\\n‚úÖ Successfully combines all transformer components!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stacking Transformer Blocks\n\nThe power of transformers comes from stacking multiple blocks. Each layer can learn increasingly complex patterns and relationships.\n\n**Why stacking works**:\n- Layer 1: Basic features and simple attention patterns  \n- Layer 2: More complex interactions between positions\n- Layer 3+: High-level reasoning and abstract relationships\n\nResidual connections make deep stacking possible by preserving gradient flow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleTransformer(nn.Module):\n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"Stacked Transformer:\")\nprint(f\"Layers:           {n_layers}\")\nprint(f\"Input shape:      {x.shape}\")\nprint(f\"Output shape:     {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Per layer:        {params_per_layer:,}\")\n\nprint(f\"\\n‚ú® Each layer learns different abstraction levels:\")\nprint(f\"‚Ä¢ Layer 1: Basic features and attention patterns\")\nprint(f\"‚Ä¢ Layer 2: More complex relationships\")\nprint(f\"‚Ä¢ Layer 3: High-level abstractions and reasoning\")\n\nprint(f\"\\nüîë Residual connections enable deep stacking!\")\nprint(f\"‚úÖ Each layer builds on previous understanding\")\nprint(f\"‚úÖ Deep networks learn hierarchical representations\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: Complete Transformer Architecture\n\nYou've built complete transformer blocks from first principles!\n\n### Key Components\n- **Feed-Forward Networks**: Position-wise processing with expand-contract architecture\n- **Layer Normalization**: Stabilizes training by normalizing each position's features\n- **Residual Connections**: Enable deep networks via gradient highways\n- **Integration**: Pre-norm architecture for stable deep training\n\n### Architecture Pattern\nEach transformer block follows: `x ‚Üí LayerNorm ‚Üí Attention ‚Üí Residual ‚Üí LayerNorm ‚Üí FFN ‚Üí Residual`\n\n### Why It Works\n- **Attention**: Routes information between positions\n- **FFN**: Processes each position independently\n- **LayerNorm**: Maintains stable feature scales\n- **Residuals**: Preserve gradient flow in deep networks\n\n### Next Steps\nNow you understand complete transformer blocks! Next we'll explore positional encoding to give transformers spatial awareness.\n\nFoundation complete - let's add position information! üß≠",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple multi-layer transformer for demonstration\nclass SimpleTransformer(nn.Module):\n    \"\"\"Stack of transformer blocks.\"\"\"\n    \n    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int):\n        super().__init__()\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff)\n            for _ in range(n_layers)\n        ])\n        \n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return self.final_norm(x)\n\n# Test stacking\nprint(\"üìö STACKING TRANSFORMER BLOCKS\")\nn_layers = 3\ntransformer = SimpleTransformer(n_layers, d_model=8, n_heads=2, d_ff=32)\n\nx = torch.randn(1, 4, 8)\noutput = transformer(x)\n\ntotal_params = sum(p.numel() for p in transformer.parameters())\nparams_per_layer = total_params // n_layers\n\nprint(f\"Layers:           {n_layers}\")\nprint(f\"Input shape:      {x.shape}\")\nprint(f\"Output shape:     {output.shape}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Per layer:        {params_per_layer:,}\")\n\nprint(f\"\\n‚ú® Each layer can learn different patterns:\")\nprint(f\"‚Ä¢ Layer 1: Basic features and attention patterns\")\nprint(f\"‚Ä¢ Layer 2: More complex relationships\") \nprint(f\"‚Ä¢ Layer 3: High-level abstractions and reasoning\")\n\nprint(f\"\\nüîë KEY INSIGHT: Deep networks can learn hierarchical representations!\")\nprint(f\"‚úÖ Residual connections make deep stacking possible\")\nprint(f\"‚úÖ Each layer builds on previous layers' understanding\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}