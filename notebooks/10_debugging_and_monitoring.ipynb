{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Debugging Transformer Training\n\nTraining transformers is like learning to drive - things can go wrong in predictable ways. This notebook teaches you to diagnose problems and fix them quickly.\n\n## Why Training Fails\n\nNeural network training fails for fundamental reasons:\n\n1. **Gradient Problems**: Gradients become too large (exploding) or too small (vanishing)\n2. **Learning Rate Issues**: Too high causes instability, too low causes stagnation\n3. **Data Problems**: Poor quality, wrong size, or insufficient quantity\n4. **Architecture Issues**: Model too deep, bad initialization, or wrong configuration\n\n## What You'll Master\n\n1. **Recognize Symptoms**: Learn the warning signs of different failures\n2. **Monitor Health**: Track gradient and loss patterns during training\n3. **Diagnose Problems**: Systematically identify root causes\n4. **Apply Fixes**: Know exactly how to solve each problem\n\nThink of this as becoming a \"transformer doctor\" - diagnosing symptoms and prescribing cures!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "from src.data.dataset import SimpleTextDataset, create_dataloader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Debugging toolkit loaded! üîß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. The Big Three Training Failures\n\n### Understanding Why Training Breaks\n\nEvery training failure falls into three categories. Understanding the physics behind each helps you diagnose problems instantly:\n\n### A. Exploding Gradients\n**The Physics**: During backpropagation, gradients multiply through layers. In deep networks, small errors can compound exponentially, like compound interest in reverse.\n\n**What Happens**: \n- Loss suddenly jumps to infinity\n- Model parameters become NaN (Not a Number)\n- Training completely breaks\n\n**Root Causes**:\n- Learning rate too high (most common)\n- Poor weight initialization\n- No gradient clipping\n- Unstable operations in model\n\n### B. Vanishing Gradients  \n**The Physics**: Gradients become smaller as they travel backward through layers. Eventually they become so small that early layers stop learning entirely.\n\n**What Happens**:\n- Loss decreases extremely slowly or plateaus\n- Early layers don't learn (parameters barely change)\n- Model underperforms despite long training\n\n**Root Causes**:\n- Weights initialized too small\n- Activation functions that saturate (like sigmoid)\n- Too many layers without residual connections\n\n### C. Wrong Learning Rate\n**The Physics**: Learning rate controls step size in the loss landscape. Too big and you overshoot the minimum; too small and you barely move.\n\n**What Happens**:\n- Too high: Loss oscillates wildly\n- Too low: Painfully slow convergence\n- Just right: Smooth, steady improvement\n\nLet's simulate each failure to see their signatures:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Create a simple training environment to simulate failures\nimport sys\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom collections import defaultdict\n\nfrom src.model.transformer import GPTModel\n\n# Simple setup for demonstrations\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Create a tiny model for fast demonstrations\nconfig = {\n    'vocab_size': 100,\n    'd_model': 64,\n    'n_heads': 4,\n    'n_layers': 2,\n    'd_ff': 128,\n    'max_seq_len': 32,\n    'dropout': 0.1\n}\n\ndef create_dummy_batch(batch_size=4, seq_len=16):\n    \"\"\"Create dummy training data for experiments.\"\"\"\n    x = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n    targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n    return x, targets\n\ndef run_training_experiment(model, optimizer, steps=20, experiment_name=\"\"):\n    \"\"\"Run a training experiment and collect loss/gradient data.\"\"\"\n    print(f\"\\nüß™ {experiment_name}\")\n    print(\"-\" * 40)\n    \n    criterion = nn.CrossEntropyLoss()\n    losses = []\n    grad_norms = []\n    \n    for step in range(steps):\n        x, targets = create_dummy_batch()\n        \n        optimizer.zero_grad()\n        \n        try:\n            outputs = model(x)\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n            \n            # Check if loss is reasonable\n            if not torch.isfinite(loss) or loss.item() > 100:\n                print(f\"üí• Step {step}: Loss exploded to {loss.item():.2f}\")\n                break\n                \n            loss.backward()\n            \n            # Calculate gradient norm\n            grad_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    grad_norm += p.grad.data.norm(2).item() ** 2\n            grad_norm = grad_norm ** 0.5\n            \n            optimizer.step()\n            \n            losses.append(loss.item())\n            grad_norms.append(grad_norm)\n            \n            if step % 5 == 0:\n                print(f\"Step {step}: Loss = {loss.item():.4f}, Grad Norm = {grad_norm:.4f}\")\n                \n        except Exception as e:\n            print(f\"üí• Error at step {step}: {e}\")\n            break\n    \n    return losses, grad_norms\n\nprint(\"Experimental setup ready! üî¨\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment 1: Exploding Gradients\n# We'll use an extremely high learning rate to cause gradient explosion\n\nmodel = GPTModel(config).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1.0)  # Dangerously high!\n\nexploding_losses, exploding_grads = run_training_experiment(\n    model, optimizer, steps=15, \n    experiment_name=\"EXPLODING GRADIENTS (LR = 1.0)\"\n)\n\nprint(\"\\nüîç DIAGNOSIS:\")\nprint(\"‚úì Loss increases or becomes NaN\")\nprint(\"‚úì Gradient norms grow exponentially\") \nprint(\"‚úì Training completely breaks\")\nprint(\"\\nüíä CURE: Lower learning rate to ~1e-4, add gradient clipping\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment 2: Vanishing Gradients  \n# We'll create a deep model with poor initialization\n\n# Create deeper model with tiny initialization\ndeep_config = config.copy()\ndeep_config['n_layers'] = 8  # Much deeper\n\nmodel = GPTModel(deep_config).to(device)\n\n# Initialize weights to be extremely small (bad practice!)\nfor p in model.parameters():\n    if p.dim() > 1:\n        nn.init.normal_(p, 0, 0.001)  # Way too small\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nvanishing_losses, vanishing_grads = run_training_experiment(\n    model, optimizer, steps=25,\n    experiment_name=\"VANISHING GRADIENTS (Deep + Tiny Init)\"\n)\n\nprint(\"\\nüîç DIAGNOSIS:\")\nprint(\"‚úì Loss decreases extremely slowly\")\nprint(\"‚úì Gradient norms are tiny (< 1e-4)\")\nprint(\"‚úì Model barely learns despite many steps\")\nprint(\"\\nüíä CURE: Better initialization (Xavier/He), residual connections, layer norm\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment 3: Learning Rate Too Low\n# We'll use an extremely small learning rate\n\nmodel = GPTModel(config).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-8)  # Way too small!\n\nslow_losses, slow_grads = run_training_experiment(\n    model, optimizer, steps=30,\n    experiment_name=\"LEARNING RATE TOO LOW (LR = 1e-8)\"\n)\n\nprint(\"\\nüîç DIAGNOSIS:\")\nprint(\"‚úì Loss decreases painfully slowly\")\nprint(\"‚úì Gradients are reasonable but updates are tiny\")\nprint(\"‚úì Would take forever to converge\")\nprint(\"\\nüíä CURE: Increase learning rate to 1e-4 to 1e-3 range\")\n\n# Experiment 4: Healthy Training (for comparison)\nprint(\"\\n\" + \"=\"*50)\nmodel = GPTModel(config).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n\nhealthy_losses, healthy_grads = run_training_experiment(\n    model, optimizer, steps=25,\n    experiment_name=\"HEALTHY TRAINING (LR = 3e-4 + AdamW)\"\n)\n\nprint(\"\\nüîç DIAGNOSIS:\")\nprint(\"‚úì Loss decreases smoothly and consistently\")\nprint(\"‚úì Gradient norms are stable (0.1 - 10 range)\")\nprint(\"‚úì No instabilities or plateaus\")\nprint(\"\\nüíä This is what good training looks like!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize all failure modes side by side\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot loss curves\naxes[0, 0].plot(exploding_losses, 'r-', linewidth=3, label='Exploding')\naxes[0, 0].plot(slow_losses, 'b-', linewidth=2, label='Too Slow') \naxes[0, 0].plot(healthy_losses, 'g-', linewidth=2, label='Healthy')\naxes[0, 0].set_title('Loss Curves: Failure vs Success')\naxes[0, 0].set_xlabel('Training Steps')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot gradient norms\naxes[0, 1].plot(exploding_grads, 'r-', linewidth=3, label='Exploding')\naxes[0, 1].plot(vanishing_grads, 'orange', linewidth=2, label='Vanishing')\naxes[0, 1].plot(slow_grads, 'b-', linewidth=2, label='Too Slow')\naxes[0, 1].plot(healthy_grads, 'g-', linewidth=2, label='Healthy')\naxes[0, 1].set_title('Gradient Norms: Different Signatures')\naxes[0, 1].set_xlabel('Training Steps')\naxes[0, 1].set_ylabel('Gradient Norm')\naxes[0, 1].set_yscale('log')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Add healthy zones for gradient norms\naxes[0, 1].axhspan(0.1, 10, alpha=0.2, color='green', label='Healthy Zone')\naxes[0, 1].axhspan(10, 1000, alpha=0.2, color='red', label='Danger Zone')\naxes[0, 1].axhspan(1e-6, 0.1, alpha=0.2, color='orange', label='Vanishing Zone')\n\n# Create diagnostic summary\naxes[1, 0].text(0.1, 0.9, 'üö® EXPLODING GRADIENTS', fontsize=14, weight='bold', color='red', transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.8, '‚Ä¢ Loss increases rapidly', fontsize=12, transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.7, '‚Ä¢ Grad norms > 10', fontsize=12, transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.6, '‚Ä¢ Training breaks', fontsize=12, transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.5, 'üíä Fix: Lower LR, clip gradients', fontsize=12, weight='bold', color='blue', transform=axes[1, 0].transAxes)\n\naxes[1, 0].text(0.1, 0.3, 'üêå VANISHING GRADIENTS', fontsize=14, weight='bold', color='orange', transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.2, '‚Ä¢ Loss decreases slowly', fontsize=12, transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.1, '‚Ä¢ Grad norms < 1e-4', fontsize=12, transform=axes[1, 0].transAxes)\naxes[1, 0].text(0.1, 0.0, 'üíä Fix: Better init, residuals', fontsize=12, weight='bold', color='blue', transform=axes[1, 0].transAxes)\naxes[1, 0].set_xlim(0, 1)\naxes[1, 0].set_ylim(0, 1)\naxes[1, 0].axis('off')\naxes[1, 0].set_title('Quick Diagnosis Guide')\n\naxes[1, 1].text(0.1, 0.9, 'üê¢ LEARNING RATE TOO LOW', fontsize=14, weight='bold', color='blue', transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.8, '‚Ä¢ Painfully slow progress', fontsize=12, transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.7, '‚Ä¢ Reasonable gradients', fontsize=12, transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.6, '‚Ä¢ Takes forever', fontsize=12, transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.5, 'üíä Fix: Increase LR 5-10x', fontsize=12, weight='bold', color='blue', transform=axes[1, 1].transAxes)\n\naxes[1, 1].text(0.1, 0.3, '‚úÖ HEALTHY TRAINING', fontsize=14, weight='bold', color='green', transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.2, '‚Ä¢ Smooth loss decrease', fontsize=12, transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.1, '‚Ä¢ Stable grad norms (0.1-10)', fontsize=12, transform=axes[1, 1].transAxes)\naxes[1, 1].text(0.1, 0.0, 'üíä Keep going!', fontsize=12, weight='bold', color='green', transform=axes[1, 1].transAxes)\naxes[1, 1].set_xlim(0, 1)\naxes[1, 1].set_ylim(0, 1)\naxes[1, 1].axis('off')\naxes[1, 1].set_title('Treatment Guide')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüéØ KEY INSIGHT: Each failure has a unique signature!\")\nprint(\"Learn to recognize these patterns and you can debug any training problem.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Real-Time Health Monitoring\n\n### Why Monitor During Training?\n\nTraining can go wrong at any moment. Problems often start small and then explode. Real-time monitoring catches issues early, like a smoke detector for your model.\n\n### What to Monitor\n\n**Essential Metrics** (monitor these always):\n1. **Loss value**: Should decrease smoothly\n2. **Gradient norm**: Should stay in 0.1-10 range  \n3. **Learning rate**: Should follow your schedule\n4. **Training speed**: Should be consistent\n\n**Advanced Metrics** (for deeper insights):\n1. **Gradient-to-parameter ratio**: Measures update magnitude\n2. **Layer-wise gradients**: Detects vanishing/exploding by layer\n3. **Parameter norms**: Tracks model evolution\n4. **Memory usage**: Prevents OOM crashes\n\n### The Physics of Gradient Monitoring\n\nGradients tell you everything about training health:\n- **Too large (>10)**: Updates are too big, causing instability\n- **Too small (<0.001)**: Updates are too tiny, learning is slow\n- **Just right (0.1-10)**: Model learns efficiently and stably\n\nLet's build a monitoring system that tracks these metrics:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class HealthMonitor:\n    \"\"\"Real-time training health monitor - your model's vital signs tracker.\"\"\"\n    \n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Start fresh monitoring.\"\"\"\n        self.losses = []\n        self.grad_norms = []\n        self.learning_rates = []\n        self.grad_param_ratios = []\n        self.steps = []\n        \n    def check_vitals(self, model, loss, optimizer, step):\n        \"\"\"Check model's vital signs after each training step.\"\"\"\n        \n        # Record basic metrics\n        self.losses.append(loss.item())\n        self.learning_rates.append(optimizer.param_groups[0]['lr'])\n        self.steps.append(step)\n        \n        # Calculate gradient norm\n        total_grad_norm = 0.0\n        total_param_norm = 0.0\n        \n        for param in model.parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.data.norm(2).item()\n                param_norm = param.data.norm(2).item()\n                \n                total_grad_norm += grad_norm ** 2\n                total_param_norm += param_norm ** 2\n        \n        total_grad_norm = total_grad_norm ** 0.5\n        total_param_norm = total_param_norm ** 0.5\n        \n        self.grad_norms.append(total_grad_norm)\n        \n        # Gradient-to-parameter ratio (key health indicator)\n        ratio = total_grad_norm / (total_param_norm + 1e-8)\n        self.grad_param_ratios.append(ratio)\n        \n        return self.diagnose_current_health()\n    \n    def diagnose_current_health(self):\n        \"\"\"Instant diagnosis of current training health.\"\"\"\n        if not self.grad_norms:\n            return \"No data yet\"\n        \n        current_grad = self.grad_norms[-1]\n        current_ratio = self.grad_param_ratios[-1]\n        current_loss = self.losses[-1]\n        \n        # Check for critical issues\n        if not np.isfinite(current_loss):\n            return \"üö® CRITICAL: Loss is NaN or infinite\"\n        \n        if current_grad > 10:\n            return \"üö® DANGER: Gradient norm too high, risk of explosion\"\n        \n        if current_grad < 1e-5:\n            return \"üêå WARNING: Gradient norm too low, vanishing gradients\"\n        \n        if current_ratio > 0.1:\n            return \"‚ö†Ô∏è CAUTION: Learning rate might be too high\"\n        \n        if current_ratio < 1e-5:\n            return \"‚ö†Ô∏è CAUTION: Learning rate might be too low\"\n        \n        # Check trend if we have enough data\n        if len(self.losses) > 5:\n            recent_losses = self.losses[-5:]\n            if all(l > recent_losses[0] for l in recent_losses[2:]):\n                return \"üìà WARNING: Loss is increasing, check for overfitting\"\n        \n        return \"‚úÖ HEALTHY: All vitals normal\"\n    \n    def emergency_stop_needed(self):\n        \"\"\"Check if training should be stopped immediately.\"\"\"\n        if not self.grad_norms:\n            return False\n        \n        current_grad = self.grad_norms[-1]\n        current_loss = self.losses[-1]\n        \n        # Emergency conditions\n        return (not np.isfinite(current_loss) or \n                current_grad > 100 or \n                current_loss > 50)\n    \n    def plot_dashboard(self):\n        \"\"\"Plot a comprehensive health dashboard.\"\"\"\n        if len(self.steps) < 2:\n            print(\"Need more data for dashboard\")\n            return\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Loss curve with health zones\n        axes[0, 0].plot(self.steps, self.losses, 'b-', linewidth=2)\n        axes[0, 0].set_title('üìâ Loss Curve')\n        axes[0, 0].set_xlabel('Steps')\n        axes[0, 0].set_ylabel('Loss')\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # Gradient norms with danger zones\n        axes[0, 1].plot(self.steps, self.grad_norms, 'g-', linewidth=2)\n        axes[0, 1].axhspan(0.1, 10, alpha=0.2, color='green', label='Healthy Zone')\n        axes[0, 1].axhspan(10, 1000, alpha=0.2, color='red', label='Danger Zone')\n        axes[0, 1].axhspan(1e-6, 0.1, alpha=0.2, color='orange', label='Vanishing Zone')\n        axes[0, 1].set_title('üå°Ô∏è Gradient Health')\n        axes[0, 1].set_xlabel('Steps')\n        axes[0, 1].set_ylabel('Gradient Norm')\n        axes[0, 1].set_yscale('log')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # Learning rate schedule\n        axes[1, 0].plot(self.steps, self.learning_rates, 'r-', linewidth=2)\n        axes[1, 0].set_title('üìä Learning Rate')\n        axes[1, 0].set_xlabel('Steps')\n        axes[1, 0].set_ylabel('Learning Rate')\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # Gradient-to-parameter ratio\n        axes[1, 1].plot(self.steps, self.grad_param_ratios, 'm-', linewidth=2)\n        axes[1, 1].axhspan(1e-5, 0.1, alpha=0.2, color='green', label='Healthy Zone')\n        axes[1, 1].axhspan(0.1, 10, alpha=0.2, color='red', label='Too High')\n        axes[1, 1].set_title('‚öñÔ∏è Update Magnitude (Grad/Param Ratio)')\n        axes[1, 1].set_xlabel('Steps')\n        axes[1, 1].set_ylabel('Ratio')\n        axes[1, 1].set_yscale('log')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print summary\n        print(f\"\\nüìä HEALTH SUMMARY (Step {self.steps[-1]}):\")\n        print(f\"   Current Status: {self.diagnose_current_health()}\")\n        print(f\"   Loss: {self.losses[-1]:.4f}\")\n        print(f\"   Grad Norm: {self.grad_norms[-1]:.4f}\")\n        print(f\"   Learning Rate: {self.learning_rates[-1]:.6f}\")\n        print(f\"   Update Ratio: {self.grad_param_ratios[-1]:.6f}\")\n\nprint(\"Health monitoring system ready! üè•\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate healthy training with monitoring\n",
    "print(\"üè• DEMONSTRATING HEALTHY TRAINING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create fresh model and monitor\n",
    "model = GPTModel(**config).to(device)\n",
    "monitor = GradientMonitor(model)\n",
    "\n",
    "# Healthy training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Train with monitoring\n",
    "for step in range(50):\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(data_loader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    input_ids, target_ids = batch\n",
    "    input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(input_ids)\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping (good practice!)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # Monitor before stepping\n",
    "    monitor.update(step)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        diagnosis = monitor.diagnose()\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "        for key, value in diagnosis.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# Plot the healthy training\n",
    "monitor.plot_health()\n",
    "\n",
    "print(\"\\n‚úÖ This is what healthy training looks like!\")\n",
    "print(\"Notice the stable, well-behaved gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Curve Interpretation\n",
    "\n",
    "Understanding what different loss curve patterns mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurveAnalyzer:\n",
    "    \"\"\"Analyze and interpret loss curves.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_loss_curve(losses: List[float], window_size: int = 10) -> Dict[str, str]:\n",
    "        \"\"\"Analyze loss curve patterns.\"\"\"\n",
    "        if len(losses) < window_size:\n",
    "            return {\"status\": \"Insufficient data\"}\n",
    "        \n",
    "        diagnosis = {}\n",
    "        \n",
    "        # Overall trend\n",
    "        start_loss = np.mean(losses[:window_size])\n",
    "        end_loss = np.mean(losses[-window_size:])\n",
    "        improvement = (start_loss - end_loss) / start_loss\n",
    "        \n",
    "        if improvement > 0.1:\n",
    "            diagnosis[\"overall_trend\"] = \"‚úÖ GOOD - Loss decreasing well\"\n",
    "        elif improvement > 0.01:\n",
    "            diagnosis[\"overall_trend\"] = \"‚ö†Ô∏è SLOW - Loss decreasing slowly\"\n",
    "        elif improvement < -0.01:\n",
    "            diagnosis[\"overall_trend\"] = \"üö® BAD - Loss increasing\"\n",
    "        else:\n",
    "            diagnosis[\"overall_trend\"] = \"üìä FLAT - Loss plateaued\"\n",
    "        \n",
    "        # Volatility\n",
    "        recent_losses = losses[-window_size:]\n",
    "        volatility = np.std(recent_losses) / np.mean(recent_losses)\n",
    "        \n",
    "        if volatility > 0.1:\n",
    "            diagnosis[\"stability\"] = \"üåä NOISY - High variance in loss\"\n",
    "        elif volatility < 0.01:\n",
    "            diagnosis[\"stability\"] = \"üìâ SMOOTH - Very stable loss\"\n",
    "        else:\n",
    "            diagnosis[\"stability\"] = \"‚úÖ STABLE - Normal variance\"\n",
    "        \n",
    "        # Check for oscillations\n",
    "        if len(losses) > 20:\n",
    "            recent = losses[-20:]\n",
    "            # Count direction changes\n",
    "            direction_changes = 0\n",
    "            for i in range(1, len(recent) - 1):\n",
    "                if (recent[i] > recent[i-1] and recent[i] > recent[i+1]) or \\\n",
    "                   (recent[i] < recent[i-1] and recent[i] < recent[i+1]):\n",
    "                    direction_changes += 1\n",
    "            \n",
    "            if direction_changes > len(recent) * 0.3:\n",
    "                diagnosis[\"pattern\"] = \"üåÄ OSCILLATING - Loss bouncing up and down\"\n",
    "            else:\n",
    "                diagnosis[\"pattern\"] = \"‚û°Ô∏è MONOTONIC - Smooth trend\"\n",
    "        \n",
    "        # Check for NaN or infinite values\n",
    "        if any(not np.isfinite(l) for l in losses[-10:]):\n",
    "            diagnosis[\"health\"] = \"üíÄ BROKEN - NaN or infinite loss\"\n",
    "        elif max(losses[-10:]) > 100:\n",
    "            diagnosis[\"health\"] = \"üö® EXPLODING - Loss too high\"\n",
    "        else:\n",
    "            diagnosis[\"health\"] = \"‚úÖ HEALTHY - Normal loss values\"\n",
    "        \n",
    "        return diagnosis\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_example_curves():\n",
    "        \"\"\"Generate example loss curves for different scenarios.\"\"\"\n",
    "        steps = np.arange(100)\n",
    "        \n",
    "        curves = {}\n",
    "        \n",
    "        # Healthy learning curve\n",
    "        healthy = 5.0 * np.exp(-steps / 30) + 1.0 + 0.1 * np.random.randn(100) * 0.1\n",
    "        curves[\"Healthy Learning\"] = healthy\n",
    "        \n",
    "        # Overfitting curve\n",
    "        overfitting = 5.0 * np.exp(-steps / 20) + 1.0\n",
    "        # Add upturn after step 60\n",
    "        upturn = np.where(steps > 60, 0.02 * (steps - 60), 0)\n",
    "        overfitting += upturn + 0.05 * np.random.randn(100)\n",
    "        curves[\"Overfitting\"] = overfitting\n",
    "        \n",
    "        # Learning rate too high (oscillating)\n",
    "        oscillating = 3.0 + 0.5 * np.sin(steps * 0.3) + 0.1 * np.random.randn(100)\n",
    "        curves[\"LR Too High (Oscillating)\"] = oscillating\n",
    "        \n",
    "        # Learning rate too low (plateau)\n",
    "        plateau = 5.0 * np.exp(-steps / 100) + 3.0 + 0.05 * np.random.randn(100)\n",
    "        curves[\"LR Too Low (Plateau)\"] = plateau\n",
    "        \n",
    "        # Exploding gradients\n",
    "        exploding = 2.0 + np.where(steps > 30, 0.5 * (steps - 30) ** 1.5, 0) + 0.1 * np.random.randn(100)\n",
    "        exploding = np.clip(exploding, 0, 50)  # Clip to avoid infinity\n",
    "        curves[\"Exploding Gradients\"] = exploding\n",
    "        \n",
    "        # Vanishing gradients (no learning)\n",
    "        vanishing = 5.0 + 0.05 * np.random.randn(100)\n",
    "        curves[\"Vanishing Gradients\"] = vanishing\n",
    "        \n",
    "        return curves\n",
    "\n",
    "# Generate and analyze example curves\n",
    "analyzer = LossCurveAnalyzer()\n",
    "example_curves = analyzer.generate_example_curves()\n",
    "\n",
    "# Plot all example curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, curve) in enumerate(example_curves.items()):\n",
    "    if i < len(axes):\n",
    "        axes[i].plot(curve, linewidth=2)\n",
    "        axes[i].set_title(f'{name}')\n",
    "        axes[i].set_xlabel('Training Step')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add diagnosis\n",
    "        diagnosis = analyzer.analyze_loss_curve(curve.tolist())\n",
    "        diagnosis_text = \"\\n\".join([f\"{k}: {v}\" for k, v in diagnosis.items()])\n",
    "        axes[i].text(0.02, 0.98, diagnosis_text, transform=axes[i].transAxes,\n",
    "                    verticalalignment='top', fontsize=8,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä LOSS CURVE INTERPRETATION GUIDE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ Healthy: Smooth decrease, low noise, good convergence\")\n",
    "print(\"üìà Overfitting: Initial decrease then increase (validation loss)\")\n",
    "print(\"üåÄ Oscillating: Learning rate too high, bouncy loss\")\n",
    "print(\"üìä Plateau: Learning rate too low, stuck at high loss\")\n",
    "print(\"üö® Exploding: Gradients explode, loss shoots up\")\n",
    "print(\"üò¥ Vanishing: No learning, loss stays flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Troubleshooting Guide\n",
    "\n",
    "A systematic approach to diagnosing and fixing training problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDoctor:\n",
    "    \"\"\"Comprehensive transformer training diagnostics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.symptoms = []\n",
    "        self.diagnosis = []\n",
    "        self.treatments = []\n",
    "    \n",
    "    def examine_patient(self, model, losses, grad_monitor, data_loader, optimizer):\n",
    "        \"\"\"Comprehensive examination of training health.\"\"\"\n",
    "        report = {\n",
    "            \"model_health\": self._check_model_health(model),\n",
    "            \"loss_health\": self._check_loss_health(losses),\n",
    "            \"gradient_health\": self._check_gradient_health(grad_monitor),\n",
    "            \"data_health\": self._check_data_health(data_loader),\n",
    "            \"optimizer_health\": self._check_optimizer_health(optimizer),\n",
    "            \"overall_diagnosis\": \"\",\n",
    "            \"treatment_plan\": []\n",
    "        }\n",
    "        \n",
    "        # Generate overall diagnosis\n",
    "        critical_issues = []\n",
    "        warnings = []\n",
    "        \n",
    "        for category, findings in report.items():\n",
    "            if isinstance(findings, dict):\n",
    "                for finding in findings.values():\n",
    "                    if \"üö®\" in str(finding) or \"üíÄ\" in str(finding):\n",
    "                        critical_issues.append(finding)\n",
    "                    elif \"‚ö†Ô∏è\" in str(finding) or \"üêå\" in str(finding):\n",
    "                        warnings.append(finding)\n",
    "        \n",
    "        if critical_issues:\n",
    "            report[\"overall_diagnosis\"] = \"üö® CRITICAL: Immediate attention required\"\n",
    "            report[\"treatment_plan\"] = self._generate_critical_treatment(critical_issues)\n",
    "        elif warnings:\n",
    "            report[\"overall_diagnosis\"] = \"‚ö†Ô∏è CONCERNING: Optimization needed\"\n",
    "            report[\"treatment_plan\"] = self._generate_warning_treatment(warnings)\n",
    "        else:\n",
    "            report[\"overall_diagnosis\"] = \"‚úÖ HEALTHY: Training looks good\"\n",
    "            report[\"treatment_plan\"] = [\"Continue current training regimen\"]\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _check_model_health(self, model):\n",
    "        \"\"\"Check model architecture health.\"\"\"\n",
    "        health = {}\n",
    "        \n",
    "        # Parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        if total_params > 10**9:\n",
    "            health[\"size\"] = \"üö® VERY LARGE - May need distributed training\"\n",
    "        elif total_params > 10**6:\n",
    "            health[\"size\"] = \"‚ö†Ô∏è LARGE - Monitor memory usage\"\n",
    "        else:\n",
    "            health[\"size\"] = \"‚úÖ REASONABLE\"\n",
    "        \n",
    "        # Check for NaN parameters\n",
    "        has_nan = any(torch.isnan(p).any() for p in model.parameters())\n",
    "        if has_nan:\n",
    "            health[\"parameters\"] = \"üíÄ NaN DETECTED - Model corrupted\"\n",
    "        else:\n",
    "            health[\"parameters\"] = \"‚úÖ CLEAN\"\n",
    "        \n",
    "        # Parameter initialization check\n",
    "        param_stds = [p.data.std().item() for p in model.parameters() if p.dim() > 1]\n",
    "        if param_stds:\n",
    "            avg_std = np.mean(param_stds)\n",
    "            if avg_std > 0.5:\n",
    "                health[\"initialization\"] = \"‚ö†Ô∏è LARGE - Parameters might be too large\"\n",
    "            elif avg_std < 0.001:\n",
    "                health[\"initialization\"] = \"‚ö†Ô∏è SMALL - Parameters might be too small\"\n",
    "            else:\n",
    "                health[\"initialization\"] = \"‚úÖ REASONABLE\"\n",
    "        \n",
    "        return health\n",
    "    \n",
    "    def _check_loss_health(self, losses):\n",
    "        \"\"\"Check loss curve health.\"\"\"\n",
    "        if not losses:\n",
    "            return {\"status\": \"No loss data\"}\n",
    "        \n",
    "        analyzer = LossCurveAnalyzer()\n",
    "        return analyzer.analyze_loss_curve(losses)\n",
    "    \n",
    "    def _check_gradient_health(self, grad_monitor):\n",
    "        \"\"\"Check gradient health.\"\"\"\n",
    "        return grad_monitor.diagnose()\n",
    "    \n",
    "    def _check_data_health(self, data_loader):\n",
    "        \"\"\"Check data health.\"\"\"\n",
    "        health = {}\n",
    "        \n",
    "        # Check batch size\n",
    "        batch_size = data_loader.batch_size\n",
    "        if batch_size < 2:\n",
    "            health[\"batch_size\"] = \"‚ö†Ô∏è VERY SMALL - May cause instability\"\n",
    "        elif batch_size > 64:\n",
    "            health[\"batch_size\"] = \"‚ö†Ô∏è LARGE - May need gradient accumulation\"\n",
    "        else:\n",
    "            health[\"batch_size\"] = \"‚úÖ REASONABLE\"\n",
    "        \n",
    "        # Check dataset size\n",
    "        dataset_size = len(data_loader.dataset)\n",
    "        if dataset_size < 100:\n",
    "            health[\"dataset_size\"] = \"‚ö†Ô∏è TINY - Risk of overfitting\"\n",
    "        elif dataset_size < 1000:\n",
    "            health[\"dataset_size\"] = \"‚ö†Ô∏è SMALL - Limited generalization\"\n",
    "        else:\n",
    "            health[\"dataset_size\"] = \"‚úÖ ADEQUATE\"\n",
    "        \n",
    "        return health\n",
    "    \n",
    "    def _check_optimizer_health(self, optimizer):\n",
    "        \"\"\"Check optimizer configuration.\"\"\"\n",
    "        health = {}\n",
    "        \n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if lr > 1e-2:\n",
    "            health[\"learning_rate\"] = \"üö® TOO HIGH - Risk of exploding gradients\"\n",
    "        elif lr < 1e-6:\n",
    "            health[\"learning_rate\"] = \"üêå TOO LOW - Very slow learning\"\n",
    "        elif lr < 1e-5:\n",
    "            health[\"learning_rate\"] = \"‚ö†Ô∏è LOW - May learn slowly\"\n",
    "        else:\n",
    "            health[\"learning_rate\"] = \"‚úÖ REASONABLE\"\n",
    "        \n",
    "        optimizer_type = type(optimizer).__name__\n",
    "        if optimizer_type in ['AdamW', 'Adam']:\n",
    "            health[\"optimizer_type\"] = \"‚úÖ GOOD CHOICE\"\n",
    "        elif optimizer_type == 'SGD':\n",
    "            health[\"optimizer_type\"] = \"‚ö†Ô∏è BASIC - Consider Adam/AdamW\"\n",
    "        else:\n",
    "            health[\"optimizer_type\"] = \"‚ùì UNKNOWN\"\n",
    "        \n",
    "        return health\n",
    "    \n",
    "    def _generate_critical_treatment(self, issues):\n",
    "        \"\"\"Generate treatment for critical issues.\"\"\"\n",
    "        treatments = []\n",
    "        \n",
    "        issue_text = \" \".join(str(issue) for issue in issues)\n",
    "        \n",
    "        if \"NaN\" in issue_text:\n",
    "            treatments.extend([\n",
    "                \"üö® IMMEDIATE: Stop training, model is corrupted\",\n",
    "                \"üíä Restore from last good checkpoint\",\n",
    "                \"üîß Check for division by zero in loss function\",\n",
    "                \"üîß Add gradient clipping: clip_grad_norm_(params, 1.0)\"\n",
    "            ])\n",
    "        \n",
    "        if \"exploding\" in issue_text.lower() or \"TOO HIGH\" in issue_text:\n",
    "            treatments.extend([\n",
    "                \"üìâ Reduce learning rate by 10x\",\n",
    "                \"‚úÇÔ∏è Add gradient clipping\",\n",
    "                \"üîÑ Restart with better initialization\"\n",
    "            ])\n",
    "        \n",
    "        if \"BROKEN\" in issue_text:\n",
    "            treatments.extend([\n",
    "                \"üõë Stop training immediately\",\n",
    "                \"üîç Debug loss computation\",\n",
    "                \"üíæ Restore from checkpoint\"\n",
    "            ])\n",
    "        \n",
    "        return treatments if treatments else [\"üÜò Seek expert help\"]\n",
    "    \n",
    "    def _generate_warning_treatment(self, warnings):\n",
    "        \"\"\"Generate treatment for warning issues.\"\"\"\n",
    "        treatments = []\n",
    "        \n",
    "        warning_text = \" \".join(str(warning) for warning in warnings)\n",
    "        \n",
    "        if \"SLOW\" in warning_text or \"TOO LOW\" in warning_text:\n",
    "            treatments.extend([\n",
    "                \"üìà Increase learning rate by 2-5x\",\n",
    "                \"‚è∞ Consider learning rate scheduling\",\n",
    "                \"üîß Try different optimizer (AdamW)\"\n",
    "            ])\n",
    "        \n",
    "        if \"OSCILLATING\" in warning_text or \"NOISY\" in warning_text:\n",
    "            treatments.extend([\n",
    "                \"üìâ Reduce learning rate by 2-3x\",\n",
    "                \"üìä Increase batch size\",\n",
    "                \"üéØ Add learning rate decay\"\n",
    "            ])\n",
    "        \n",
    "        if \"overfitting\" in warning_text.lower():\n",
    "            treatments.extend([\n",
    "                \"üõë Add early stopping\",\n",
    "                \"üíß Increase dropout\",\n",
    "                \"üìö Get more training data\",\n",
    "                \"‚öñÔ∏è Add weight decay\"\n",
    "            ])\n",
    "        \n",
    "        return treatments if treatments else [\"üìä Monitor closely, minor optimizations needed\"]\n",
    "    \n",
    "    def print_diagnosis(self, report):\n",
    "        \"\"\"Print a nicely formatted diagnosis report.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üè• TRANSFORMER TRAINING HEALTH REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nüéØ OVERALL DIAGNOSIS: {report['overall_diagnosis']}\")\n",
    "        \n",
    "        print(\"\\nüìã DETAILED EXAMINATION:\")\n",
    "        for category, findings in report.items():\n",
    "            if category not in ['overall_diagnosis', 'treatment_plan'] and isinstance(findings, dict):\n",
    "                print(f\"\\n  {category.replace('_', ' ').title()}:\")\n",
    "                for key, value in findings.items():\n",
    "                    print(f\"    ‚Ä¢ {key}: {value}\")\n",
    "        \n",
    "        print(\"\\nüíä TREATMENT PLAN:\")\n",
    "        for i, treatment in enumerate(report['treatment_plan'], 1):\n",
    "            print(f\"  {i}. {treatment}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"Transformer Doctor ready for consultation! üë®‚Äç‚öïÔ∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Transformer Doctor\n",
    "print(\"üè• RUNNING COMPREHENSIVE HEALTH CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a problematic scenario\n",
    "model = GPTModel(**config).to(device)\n",
    "monitor = GradientMonitor(model)\n",
    "\n",
    "# Problematic setup - learning rate too high\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)  # Too high!\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate some problematic training\n",
    "losses = []\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "for step in range(20):\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(data_loader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    input_ids, target_ids = batch\n",
    "    input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(input_ids)\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "    \n",
    "    # Stop if loss explodes\n",
    "    if loss.item() > 50:\n",
    "        print(f\"‚ö†Ô∏è Stopping at step {step}, loss exploded to {loss.item():.2f}\")\n",
    "        break\n",
    "    \n",
    "    loss.backward()\n",
    "    monitor.update(step)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Get diagnosis from the doctor\n",
    "doctor = TransformerDoctor()\n",
    "health_report = doctor.examine_patient(model, losses, monitor, data_loader, optimizer)\n",
    "\n",
    "# Print the diagnosis\n",
    "doctor.print_diagnosis(health_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Your Transformer Training Toolkit üîß\n",
    "\n",
    "You now have a comprehensive debugging and monitoring system for transformer training!\n",
    "\n",
    "### üéØ Key Debugging Skills\n",
    "\n",
    "**1. Recognizing Failure Modes**\n",
    "- **Exploding Gradients**: Loss shoots up, grad norms explode ‚Üí Lower LR, add clipping\n",
    "- **Vanishing Gradients**: Loss barely changes, tiny grad norms ‚Üí Better init, check architecture\n",
    "- **Wrong Learning Rate**: Too high = oscillations, too low = plateau ‚Üí Tune LR carefully\n",
    "\n",
    "**2. Gradient Health Monitoring**\n",
    "- **Healthy range**: Grad norms between 1e-6 and 10\n",
    "- **Grad/param ratio**: Should be between 1e-6 and 0.1\n",
    "- **Layer analysis**: All layers should receive reasonable gradients\n",
    "\n",
    "**3. Loss Curve Reading**\n",
    "- **Healthy**: Smooth decrease with low noise\n",
    "- **Overfitting**: Initial decrease then increase (for validation)\n",
    "- **LR issues**: Oscillations (too high) or plateau (too low)\n",
    "\n",
    "**4. Systematic Diagnosis**\n",
    "- **Check model**: Parameters, initialization, architecture\n",
    "- **Check data**: Batch size, dataset size, quality\n",
    "- **Check optimizer**: Learning rate, type, configuration\n",
    "- **Check gradients**: Norms, ratios, layer distribution\n",
    "\n",
    "### üö® Emergency Procedures\n",
    "\n",
    "**If training explodes:**\n",
    "1. Stop immediately\n",
    "2. Restore from last good checkpoint\n",
    "3. Reduce learning rate by 10x\n",
    "4. Add gradient clipping\n",
    "5. Restart training\n",
    "\n",
    "**If training stagnates:**\n",
    "1. Check gradient norms\n",
    "2. Increase learning rate by 2-5x\n",
    "3. Verify data quality\n",
    "4. Consider architecture changes\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "- **Always monitor gradients** during training\n",
    "- **Use gradient clipping** as safety net\n",
    "- **Save checkpoints frequently** for recovery\n",
    "- **Start with proven hyperparameters** then tune\n",
    "- **Validate on held-out data** to catch overfitting\n",
    "\n",
    "### üéì What You've Mastered\n",
    "\n",
    "You can now:\n",
    "- **Diagnose** common training failures\n",
    "- **Monitor** training health in real-time\n",
    "- **Interpret** loss curves and gradient behavior\n",
    "- **Troubleshoot** systematically when things go wrong\n",
    "- **Prevent** most common training disasters\n",
    "\n",
    "This debugging toolkit will save you countless hours of frustration and help you train transformers reliably! üõ†Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}