{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Monitoring Transformer Training\n",
    "\n",
    "Training transformers is like flying a plane - you need instruments to know if you're crashing. This notebook teaches you to diagnose problems before they destroy your training.\n",
    "\n",
    "## The Physics of Training Failure\n",
    "\n",
    "Every training failure has a root cause in the mathematics:\n",
    "\n",
    "**Gradient Explosion**: When gradients compound through layers, they can grow exponentially. If each layer multiplies gradients by factor > 1, then after L layers: gradient ‚àù (factor)^L ‚Üí ‚àû\n",
    "\n",
    "**Gradient Vanishing**: When gradients shrink through layers. If each layer multiplies by factor < 1, then: gradient ‚àù (factor)^L ‚Üí 0\n",
    "\n",
    "**Learning Rate Problems**: Learning rate Œ∑ controls step size in loss landscape. Too high = overshoot minimum, too low = barely move.\n",
    "\n",
    "## What You'll Master\n",
    "\n",
    "1. **Recognize symptoms** of common failures instantly\n",
    "2. **Monitor training health** with key metrics\n",
    "3. **Diagnose root causes** systematically\n",
    "4. **Apply targeted fixes** to save your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.model.transformer import GPTModel, create_model_config\n",
    "from src.data.tokenizer import create_tokenizer\n",
    "from src.data.dataset import SimpleTextDataset, create_dataloader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Training debugger loaded! üîß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Three Deadly Training Failures\n",
    "\n",
    "Understanding the fundamental physics helps you diagnose problems instantly.\n",
    "\n",
    "### A. Exploding Gradients\n",
    "\n",
    "**Root Cause**: During backpropagation, gradients multiply through layers. In deep networks:\n",
    "```\n",
    "gradient_layer_1 = gradient_output √ó weight_L √ó weight_(L-1) √ó ... √ó weight_2\n",
    "```\n",
    "\n",
    "If weights are large or learning rate is high, this product explodes exponentially.\n",
    "\n",
    "**Symptoms**: Loss suddenly jumps to infinity, parameters become NaN, training crashes\n",
    "\n",
    "### B. Vanishing Gradients\n",
    "\n",
    "**Root Cause**: Same chain rule, but weights are too small or activations saturate:\n",
    "```\n",
    "If each weight < 1, then product ‚Üí 0 as depth increases\n",
    "```\n",
    "\n",
    "**Symptoms**: Loss barely decreases, early layers don't learn, painfully slow progress\n",
    "\n",
    "### C. Wrong Learning Rate\n",
    "\n",
    "**Root Cause**: Learning rate controls step size in parameter space:\n",
    "```\n",
    "Œ∏_new = Œ∏_old - Œ∑ √ó gradient\n",
    "```\n",
    "\n",
    "Too high Œ∑ = overshoot minimum, too low Œ∑ = tiny steps\n",
    "\n",
    "**Symptoms**: High Œ∑ causes oscillations, low Œ∑ causes stagnation\n",
    "\n",
    "Let's simulate each failure to see their signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Create simulation environment for training failures\n",
    "\n",
    "# Simple model configuration for fast experiments\n",
    "config = {\n",
    "    'vocab_size': 100,\n",
    "    'd_model': 64,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 2,\n",
    "    'd_ff': 128,\n",
    "    'max_seq_len': 32,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "def create_dummy_batch(batch_size=4, seq_len=16):\n",
    "    \"\"\"Create dummy training data for experiments.\"\"\"\n",
    "    x = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n",
    "    targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len), device=device)\n",
    "    return x, targets\n",
    "\n",
    "def simulate_training_failure(model, optimizer, steps=20, name=\"\"):\n",
    "    \"\"\"Simulate training and track failure patterns.\"\"\"\n",
    "    print(f\"\\nüß™ Simulating: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        x, targets = create_dummy_batch()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "            \n",
    "            # Check for explosion\n",
    "            if not torch.isfinite(loss) or loss.item() > 100:\n",
    "                print(f\"üí• Step {step}: Loss exploded to {loss.item():.2f}\")\n",
    "                break\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calculate gradient norm (key diagnostic)\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_norm += p.grad.data.norm(2).item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record metrics\n",
    "            losses.append(loss.item())\n",
    "            grad_norms.append(total_norm)\n",
    "            \n",
    "            if step % 5 == 0:\n",
    "                print(f\"Step {step}: Loss = {loss.item():.4f}, Grad Norm = {total_norm:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"üí• Crashed at step {step}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return losses, grad_norms\n",
    "\n",
    "print(\"Failure simulation setup complete! üî¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Exploding Gradients (Learning Rate Too High)\n",
    "\n",
    "model1 = GPTModel(**config).to(device)\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=1.0)  # Dangerously high!\n",
    "\n",
    "exploding_losses, exploding_grads = simulate_training_failure(\n",
    "    model1, optimizer1, steps=15, \n",
    "    name=\"EXPLODING GRADIENTS (LR = 1.0)\"\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Key Insight: Notice how gradients grow exponentially before explosion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Vanishing Gradients (Deep Model + Poor Initialization)\n",
    "\n",
    "# Create deeper model with tiny weights\n",
    "deep_config = config.copy()\n",
    "deep_config['n_layers'] = 6  # Much deeper\n",
    "\n",
    "model2 = GPTModel(**deep_config).to(device)\n",
    "\n",
    "# Initialize weights extremely small (bad practice!)\n",
    "for p in model2.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.normal_(p, 0, 0.001)  # Way too small\n",
    "\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=1e-4)\n",
    "\n",
    "vanishing_losses, vanishing_grads = simulate_training_failure(\n",
    "    model2, optimizer2, steps=25,\n",
    "    name=\"VANISHING GRADIENTS (Deep + Tiny Init)\"\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Key Insight: Gradients are tiny, model barely learns despite many steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Learning Rate Too Low\n",
    "\n",
    "model3 = GPTModel(**config).to(device)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=1e-8)  # Way too small!\n",
    "\n",
    "slow_losses, slow_grads = simulate_training_failure(\n",
    "    model3, optimizer3, steps=30,\n",
    "    name=\"LEARNING RATE TOO LOW (LR = 1e-8)\"\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Key Insight: Gradients are reasonable but progress is painfully slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Healthy Training (For Comparison)\n",
    "\n",
    "model4 = GPTModel(**config).to(device)\n",
    "optimizer4 = optim.AdamW(model4.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "healthy_losses, healthy_grads = simulate_training_failure(\n",
    "    model4, optimizer4, steps=25,\n",
    "    name=\"HEALTHY TRAINING (LR = 3e-4)\"\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Key Insight: This is what good training looks like - stable and consistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize All Failure Modes Side by Side\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves comparison\n",
    "axes[0, 0].plot(exploding_losses, 'r-', linewidth=3, label='Exploding', marker='o')\n",
    "axes[0, 0].plot(slow_losses, 'b-', linewidth=2, label='Too Slow', marker='s')\n",
    "axes[0, 0].plot(healthy_losses, 'g-', linewidth=2, label='Healthy', marker='^')\n",
    "axes[0, 0].set_title('Loss Patterns: Success vs Failure', fontsize=14, weight='bold')\n",
    "axes[0, 0].set_xlabel('Training Steps')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm comparison\n",
    "axes[0, 1].plot(exploding_grads, 'r-', linewidth=3, label='Exploding', marker='o')\n",
    "axes[0, 1].plot(vanishing_grads, 'orange', linewidth=2, label='Vanishing', marker='x')\n",
    "axes[0, 1].plot(slow_grads, 'b-', linewidth=2, label='Too Slow', marker='s')\n",
    "axes[0, 1].plot(healthy_grads, 'g-', linewidth=2, label='Healthy', marker='^')\n",
    "axes[0, 1].set_title('Gradient Signatures', fontsize=14, weight='bold')\n",
    "axes[0, 1].set_xlabel('Training Steps')\n",
    "axes[0, 1].set_ylabel('Gradient Norm')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add healthy zones\n",
    "axes[0, 1].axhspan(0.1, 10, alpha=0.2, color='green', label='Healthy Zone')\n",
    "axes[0, 1].axhspan(10, 1000, alpha=0.2, color='red', label='Danger Zone')\n",
    "axes[0, 1].axhspan(1e-6, 0.1, alpha=0.2, color='orange', label='Vanishing Zone')\n",
    "\n",
    "# Diagnostic guide\n",
    "diagnostic_text = [\n",
    "    \"üö® EXPLODING GRADIENTS:\",\n",
    "    \"‚Ä¢ Loss increases rapidly\",\n",
    "    \"‚Ä¢ Grad norms > 10\",\n",
    "    \"‚Ä¢ Training crashes\",\n",
    "    \"üíä Fix: Lower LR, clip gradients\",\n",
    "    \"\",\n",
    "    \"üêå VANISHING GRADIENTS:\",\n",
    "    \"‚Ä¢ Loss decreases slowly\",\n",
    "    \"‚Ä¢ Grad norms < 1e-4\",\n",
    "    \"‚Ä¢ Model barely learns\",\n",
    "    \"üíä Fix: Better init, residuals\"\n",
    "]\n",
    "\n",
    "axes[1, 0].text(0.05, 0.95, '\\n'.join(diagnostic_text), \n",
    "                transform=axes[1, 0].transAxes, fontsize=10,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 0].set_title('Quick Diagnosis Guide', fontsize=14, weight='bold')\n",
    "\n",
    "treatment_text = [\n",
    "    \"üê¢ LEARNING RATE TOO LOW:\",\n",
    "    \"‚Ä¢ Painfully slow progress\",\n",
    "    \"‚Ä¢ Reasonable gradients\",\n",
    "    \"‚Ä¢ Takes forever\",\n",
    "    \"üíä Fix: Increase LR 5-10x\",\n",
    "    \"\",\n",
    "    \"‚úÖ HEALTHY TRAINING:\",\n",
    "    \"‚Ä¢ Smooth loss decrease\",\n",
    "    \"‚Ä¢ Stable grad norms (0.1-10)\",\n",
    "    \"‚Ä¢ Consistent progress\",\n",
    "    \"üíä Keep going!\"\n",
    "]\n",
    "\n",
    "axes[1, 1].text(0.05, 0.95, '\\n'.join(treatment_text), \n",
    "                transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].set_title('Treatment Guide', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHT: Each failure has a unique signature!\")\n",
    "print(\"Learn these patterns and you can diagnose any training problem instantly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real-Time Health Monitor\n",
    "\n",
    "Training problems start small then explode. Early detection saves your model.\n",
    "\n",
    "### The Physics of Gradient Health\n",
    "\n",
    "Gradients tell you everything about training state:\n",
    "\n",
    "**Gradient Norm**: ||‚àáL|| measures total update magnitude\n",
    "- Too large (>10): Updates are too big, causing instability  \n",
    "- Too small (<0.001): Updates are too tiny, learning is slow\n",
    "- Just right (0.1-10): Model learns efficiently\n",
    "\n",
    "**Gradient-to-Parameter Ratio**: ||‚àáL|| / ||Œ∏|| measures relative update size\n",
    "- Healthy range: 1e-6 to 0.1\n",
    "- Too high: Learning rate might be excessive\n",
    "- Too low: Learning rate might be insufficient\n",
    "\n",
    "**Loss Trends**: First derivative of loss curve\n",
    "- Decreasing: Good progress\n",
    "- Oscillating: Learning rate too high\n",
    "- Flat: Learning rate too low or convergence\n",
    "\n",
    "Let's build a monitoring system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHealthMonitor:\n",
    "    \"\"\"Real-time health monitoring for transformer training.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all monitoring data.\"\"\"\n",
    "        self.losses = []\n",
    "        self.grad_norms = []\n",
    "        self.learning_rates = []\n",
    "        self.grad_param_ratios = []\n",
    "        self.steps = []\n",
    "        \n",
    "    def check_vitals(self, model, loss, optimizer, step):\n",
    "        \"\"\"Check model's vital signs after each step.\"\"\"\n",
    "        \n",
    "        # Basic metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        self.steps.append(step)\n",
    "        \n",
    "        # Calculate gradient and parameter norms\n",
    "        total_grad_norm = 0.0\n",
    "        total_param_norm = 0.0\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.data.norm(2).item()\n",
    "                param_norm = param.data.norm(2).item()\n",
    "                \n",
    "                total_grad_norm += grad_norm ** 2\n",
    "                total_param_norm += param_norm ** 2\n",
    "        \n",
    "        total_grad_norm = total_grad_norm ** 0.5\n",
    "        total_param_norm = total_param_norm ** 0.5\n",
    "        \n",
    "        self.grad_norms.append(total_grad_norm)\n",
    "        \n",
    "        # Key diagnostic: gradient-to-parameter ratio\n",
    "        ratio = total_grad_norm / (total_param_norm + 1e-8)\n",
    "        self.grad_param_ratios.append(ratio)\n",
    "        \n",
    "        return self._diagnose_current_health()\n",
    "    \n",
    "    def _diagnose_current_health(self):\n",
    "        \"\"\"Instant health diagnosis.\"\"\"\n",
    "        if not self.grad_norms:\n",
    "            return \"No data yet\"\n",
    "        \n",
    "        current_grad = self.grad_norms[-1]\n",
    "        current_ratio = self.grad_param_ratios[-1]\n",
    "        current_loss = self.losses[-1]\n",
    "        \n",
    "        # Critical issues (emergency stop needed)\n",
    "        if not np.isfinite(current_loss):\n",
    "            return \"üö® CRITICAL: Loss is NaN or infinite - STOP TRAINING\"\n",
    "        \n",
    "        if current_grad > 10:\n",
    "            return \"üö® DANGER: Gradient explosion imminent\"\n",
    "        \n",
    "        if current_grad < 1e-5:\n",
    "            return \"üêå WARNING: Vanishing gradients detected\"\n",
    "        \n",
    "        # Learning rate issues\n",
    "        if current_ratio > 0.1:\n",
    "            return \"‚ö†Ô∏è CAUTION: Learning rate might be too high\"\n",
    "        \n",
    "        if current_ratio < 1e-5:\n",
    "            return \"‚ö†Ô∏è CAUTION: Learning rate might be too low\"\n",
    "        \n",
    "        # Trend analysis (if enough data)\n",
    "        if len(self.losses) > 5:\n",
    "            recent_losses = self.losses[-5:]\n",
    "            if all(l > recent_losses[0] for l in recent_losses[2:]):\n",
    "                return \"üìà WARNING: Loss trending upward - possible overfitting\"\n",
    "        \n",
    "        return \"‚úÖ HEALTHY: All vitals normal\"\n",
    "    \n",
    "    def emergency_stop_needed(self):\n",
    "        \"\"\"Check if training should stop immediately.\"\"\"\n",
    "        if not self.grad_norms:\n",
    "            return False\n",
    "        \n",
    "        current_grad = self.grad_norms[-1]\n",
    "        current_loss = self.losses[-1]\n",
    "        \n",
    "        return (not np.isfinite(current_loss) or \n",
    "                current_grad > 100 or \n",
    "                current_loss > 50)\n",
    "    \n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"Plot comprehensive health dashboard.\"\"\"\n",
    "        if len(self.steps) < 2:\n",
    "            print(\"Need more data for dashboard\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss curve with trend\n",
    "        axes[0, 0].plot(self.steps, self.losses, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "        axes[0, 0].set_title('üìâ Loss Curve', fontsize=14, weight='bold')\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient health with zones\n",
    "        axes[0, 1].plot(self.steps, self.grad_norms, 'g-', linewidth=2, marker='s', markersize=4)\n",
    "        axes[0, 1].axhspan(0.1, 10, alpha=0.2, color='green', label='Healthy Zone')\n",
    "        axes[0, 1].axhspan(10, 1000, alpha=0.2, color='red', label='Danger Zone')\n",
    "        axes[0, 1].axhspan(1e-6, 0.1, alpha=0.2, color='orange', label='Vanishing Zone')\n",
    "        axes[0, 1].set_title('üå°Ô∏è Gradient Health', fontsize=14, weight='bold')\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Gradient Norm')\n",
    "        axes[0, 1].set_yscale('log')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        axes[1, 0].plot(self.steps, self.learning_rates, 'r-', linewidth=2, marker='^', markersize=4)\n",
    "        axes[1, 0].set_title('üìä Learning Rate', fontsize=14, weight='bold')\n",
    "        axes[1, 0].set_xlabel('Steps')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Update magnitude (key diagnostic)\n",
    "        axes[1, 1].plot(self.steps, self.grad_param_ratios, 'm-', linewidth=2, marker='d', markersize=4)\n",
    "        axes[1, 1].axhspan(1e-5, 0.1, alpha=0.2, color='green', label='Healthy Zone')\n",
    "        axes[1, 1].axhspan(0.1, 10, alpha=0.2, color='red', label='Too High')\n",
    "        axes[1, 1].set_title('‚öñÔ∏è Update Magnitude (Grad/Param Ratio)', fontsize=14, weight='bold')\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('Ratio')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print current status\n",
    "        print(f\"\\nüìä HEALTH SUMMARY (Step {self.steps[-1]}):\")\n",
    "        print(f\"   Status: {self._diagnose_current_health()}\")\n",
    "        print(f\"   Loss: {self.losses[-1]:.4f}\")\n",
    "        print(f\"   Grad Norm: {self.grad_norms[-1]:.4f}\")\n",
    "        print(f\"   Learning Rate: {self.learning_rates[-1]:.6f}\")\n",
    "        print(f\"   Update Ratio: {self.grad_param_ratios[-1]:.6f}\")\n",
    "\n",
    "print(\"Health monitoring system ready! üè•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Monitor Healthy Training in Real-Time\n",
    "\n",
    "print(\"üè• DEMONSTRATING REAL-TIME HEALTH MONITORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup healthy training\n",
    "model = GPTModel(**config).to(device)\n",
    "monitor = TrainingHealthMonitor()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with monitoring\n",
    "for step in range(30):\n",
    "    x, targets = create_dummy_batch()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Apply gradient clipping (good practice!)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # Monitor health before stepping\n",
    "    diagnosis = monitor.check_vitals(model, loss, optimizer, step)\n",
    "    \n",
    "    # Emergency stop check\n",
    "    if monitor.emergency_stop_needed():\n",
    "        print(f\"üö® EMERGENCY STOP at step {step}!\")\n",
    "        break\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Report every 5 steps\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f} | {diagnosis}\")\n",
    "\n",
    "# Show comprehensive dashboard\n",
    "monitor.plot_dashboard()\n",
    "\n",
    "print(\"\\n‚úÖ This is what healthy, monitored training looks like!\")\n",
    "print(\"Notice how all metrics stay in healthy ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Curve Interpretation\n",
    "\n",
    "Loss curves tell the story of your training. Learn to read the signs.\n",
    "\n",
    "### The Mathematics of Loss Patterns\n",
    "\n",
    "**Healthy Decreasing Loss**: L(t) ‚àù e^(-t/œÑ) where œÑ is the time constant\n",
    "- Exponential decay toward minimum\n",
    "- Smooth curve with low noise\n",
    "- Consistent rate of improvement\n",
    "\n",
    "**Oscillating Loss**: L(t) = baseline + A¬∑sin(œât) + noise\n",
    "- Learning rate too high causes overshooting\n",
    "- Model bounces around minimum\n",
    "- High frequency variations\n",
    "\n",
    "**Plateau Loss**: dL/dt ‚âà 0\n",
    "- Learning rate too low (can't escape local minimum)\n",
    "- Model capacity exhausted\n",
    "- Need architectural changes\n",
    "\n",
    "**Exploding Loss**: L(t) ‚àù e^(t/œÑ_explode)\n",
    "- Unstable dynamics\n",
    "- Gradients grow exponentially\n",
    "- Training becomes chaotic\n",
    "\n",
    "Let's analyze different loss patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCurveAnalyzer:\n",
    "    \"\"\"Analyze and interpret loss curve patterns.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_pattern(losses: List[float], window_size: int = 10) -> Dict[str, str]:\n",
    "        \"\"\"Analyze loss curve and provide diagnosis.\"\"\"\n",
    "        if len(losses) < window_size:\n",
    "            return {\"status\": \"Insufficient data for analysis\"}\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        # Overall trend analysis\n",
    "        start_avg = np.mean(losses[:window_size])\n",
    "        end_avg = np.mean(losses[-window_size:])\n",
    "        improvement = (start_avg - end_avg) / start_avg\n",
    "        \n",
    "        if improvement > 0.2:\n",
    "            analysis[\"trend\"] = \"‚úÖ EXCELLENT - Strong improvement\"\n",
    "        elif improvement > 0.05:\n",
    "            analysis[\"trend\"] = \"‚úÖ GOOD - Steady improvement\"\n",
    "        elif improvement > 0.01:\n",
    "            analysis[\"trend\"] = \"‚ö†Ô∏è SLOW - Marginal improvement\"\n",
    "        elif improvement < -0.01:\n",
    "            analysis[\"trend\"] = \"üö® BAD - Loss increasing\"\n",
    "        else:\n",
    "            analysis[\"trend\"] = \"üìä FLAT - No clear trend\"\n",
    "        \n",
    "        # Stability analysis\n",
    "        recent_losses = losses[-window_size:]\n",
    "        volatility = np.std(recent_losses) / np.mean(recent_losses)\n",
    "        \n",
    "        if volatility > 0.2:\n",
    "            analysis[\"stability\"] = \"üåä VERY NOISY - High variance\"\n",
    "        elif volatility > 0.05:\n",
    "            analysis[\"stability\"] = \"üåÄ NOISY - Some oscillation\"\n",
    "        else:\n",
    "            analysis[\"stability\"] = \"üìâ SMOOTH - Stable convergence\"\n",
    "        \n",
    "        # Oscillation detection\n",
    "        if len(losses) > 20:\n",
    "            recent = losses[-20:]\n",
    "            direction_changes = 0\n",
    "            for i in range(1, len(recent) - 1):\n",
    "                # Count local maxima and minima\n",
    "                if (recent[i] > recent[i-1] and recent[i] > recent[i+1]) or \\\n",
    "                   (recent[i] < recent[i-1] and recent[i] < recent[i+1]):\n",
    "                    direction_changes += 1\n",
    "            \n",
    "            oscillation_rate = direction_changes / len(recent)\n",
    "            if oscillation_rate > 0.3:\n",
    "                analysis[\"pattern\"] = \"üåÄ OSCILLATING - Learning rate too high\"\n",
    "            elif oscillation_rate > 0.1:\n",
    "                analysis[\"pattern\"] = \"üìà BOUNCY - Some instability\"\n",
    "            else:\n",
    "                analysis[\"pattern\"] = \"‚û°Ô∏è MONOTONIC - Smooth progress\"\n",
    "        \n",
    "        # Health check\n",
    "        if any(not np.isfinite(l) for l in losses[-5:]):\n",
    "            analysis[\"health\"] = \"üíÄ BROKEN - NaN or infinite loss\"\n",
    "        elif max(losses[-5:]) > 100:\n",
    "            analysis[\"health\"] = \"üö® EXPLODING - Loss too high\"\n",
    "        elif min(losses[-5:]) < 1e-6:\n",
    "            analysis[\"health\"] = \"üéØ CONVERGED - Loss very low\"\n",
    "        else:\n",
    "            analysis[\"health\"] = \"‚úÖ HEALTHY - Normal values\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_example_patterns():\n",
    "        \"\"\"Generate example loss curves for different scenarios.\"\"\"\n",
    "        steps = np.arange(100)\n",
    "        patterns = {}\n",
    "        \n",
    "        # Healthy exponential decay\n",
    "        healthy = 4.0 * np.exp(-steps / 25) + 0.5 + 0.05 * np.random.randn(100)\n",
    "        patterns[\"Healthy Learning\"] = np.maximum(healthy, 0.1)\n",
    "        \n",
    "        # Overfitting (U-shape)\n",
    "        overfitting = 4.0 * np.exp(-steps / 15) + 0.5\n",
    "        upturn = np.where(steps > 50, 0.03 * (steps - 50) ** 1.2, 0)\n",
    "        overfitting += upturn + 0.08 * np.random.randn(100)\n",
    "        patterns[\"Overfitting\"] = np.maximum(overfitting, 0.1)\n",
    "        \n",
    "        # Learning rate too high (oscillating)\n",
    "        oscillating = 2.5 + 0.8 * np.sin(steps * 0.4) * np.exp(-steps / 50)\n",
    "        oscillating += 0.1 * np.random.randn(100)\n",
    "        patterns[\"LR Too High\"] = np.maximum(oscillating, 0.1)\n",
    "        \n",
    "        # Learning rate too low (plateau)\n",
    "        plateau = 4.0 * np.exp(-steps / 80) + 2.5 + 0.05 * np.random.randn(100)\n",
    "        patterns[\"LR Too Low\"] = np.maximum(plateau, 0.1)\n",
    "        \n",
    "        # Exploding gradients\n",
    "        exploding = 2.0 + np.where(steps > 20, 0.3 * (steps - 20) ** 1.3, 0)\n",
    "        exploding += 0.1 * np.random.randn(100)\n",
    "        patterns[\"Exploding\"] = np.clip(exploding, 0.1, 30)\n",
    "        \n",
    "        # No learning (flat)\n",
    "        flat = 4.5 + 0.1 * np.random.randn(100)\n",
    "        patterns[\"No Learning\"] = np.maximum(flat, 0.1)\n",
    "        \n",
    "        return patterns\n",
    "\n",
    "# Generate and analyze example patterns\n",
    "analyzer = LossCurveAnalyzer()\n",
    "example_patterns = analyzer.generate_example_patterns()\n",
    "\n",
    "# Visualize all patterns with analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, curve) in enumerate(example_patterns.items()):\n",
    "    if i < len(axes):\n",
    "        # Plot the curve\n",
    "        axes[i].plot(curve, linewidth=2, color=f'C{i}')\n",
    "        axes[i].set_title(f'{name}', fontsize=14, weight='bold')\n",
    "        axes[i].set_xlabel('Training Step')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add analysis as text box\n",
    "        analysis = analyzer.analyze_pattern(curve.tolist())\n",
    "        analysis_text = '\\n'.join([f'{k}: {v.split(\" - \")[0]}' for k, v in analysis.items()])\n",
    "        \n",
    "        axes[i].text(0.02, 0.98, analysis_text, transform=axes[i].transAxes,\n",
    "                    verticalalignment='top', fontsize=9, fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor=f'C{i}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä LOSS CURVE INTERPRETATION GUIDE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Healthy: Smooth exponential decay, low noise\")\n",
    "print(\"üìà Overfitting: Initial decrease then increase (U-shape)\")\n",
    "print(\"üåÄ Oscillating: Learning rate too high, bouncy loss\")\n",
    "print(\"üìä Plateau: Learning rate too low or convergence\")\n",
    "print(\"üö® Exploding: Unstable dynamics, loss shoots up\")\n",
    "print(\"üò¥ No Learning: Flat line, model not learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Master Transformer Training Diagnostics\n",
    "\n",
    "You now have a complete diagnostic toolkit for transformer training!\n",
    "\n",
    "### üîç Key Diagnostic Skills\n",
    "\n",
    "**1. Recognize Failure Signatures**\n",
    "- **Exploding Gradients**: Loss shoots up, grad norms > 10 ‚Üí Lower LR, add clipping\n",
    "- **Vanishing Gradients**: Loss flat, grad norms < 1e-4 ‚Üí Better init, check architecture  \n",
    "- **Wrong Learning Rate**: Oscillations (too high) or plateau (too low) ‚Üí Adjust carefully\n",
    "\n",
    "**2. Monitor Critical Metrics**\n",
    "- **Gradient Norm**: Healthy range 0.1-10\n",
    "- **Grad/Param Ratio**: Should be 1e-6 to 0.1  \n",
    "- **Loss Trend**: Should decrease smoothly\n",
    "- **Learning Rate**: Typically 1e-5 to 1e-3 for transformers\n",
    "\n",
    "**3. Read Loss Curves**\n",
    "- **Exponential decay** = healthy learning\n",
    "- **Oscillations** = learning rate too high\n",
    "- **Plateau** = learning rate too low or convergence\n",
    "- **U-shape** = overfitting\n",
    "- **Explosion** = unstable dynamics\n",
    "\n",
    "### üö® Emergency Procedures\n",
    "\n",
    "**Critical Issues (Stop immediately):**\n",
    "- NaN or infinite loss\n",
    "- Gradient explosion (norm > 100)\n",
    "- Loss > 50\n",
    "\n",
    "**Emergency Response:**\n",
    "1. Stop training\n",
    "2. Restore from checkpoint\n",
    "3. Reduce learning rate 10x\n",
    "4. Add gradient clipping\n",
    "5. Restart carefully\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "- **Always monitor** gradients during training\n",
    "- **Use gradient clipping** as safety net (max_norm=1.0)\n",
    "- **Save checkpoints frequently** for recovery\n",
    "- **Start with proven hyperparameters** then tune\n",
    "- **Validate regularly** to catch overfitting\n",
    "\n",
    "### üéØ What You've Mastered\n",
    "\n",
    "- **Instant diagnosis** of common training failures\n",
    "- **Real-time monitoring** with key health metrics\n",
    "- **Loss curve interpretation** for all patterns\n",
    "- **Emergency procedures** to save crashed training\n",
    "\n",
    "With these skills, you can debug any transformer training problem and keep your models healthy! üõ†Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}