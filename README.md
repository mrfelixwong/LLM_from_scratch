# Transformer LLM From Scratch

A comprehensive educational project implementing a GPT-style transformer language model from scratch using PyTorch.

## ğŸ¯ Goal

Build a complete understanding of the transformer architecture by implementing every component from the ground up, with clear, commented code and educational materials.

## âš¡ Quick Demo

**Start here to see a working transformer in action:**

```bash
# Setup environment
pip install -r requirements.txt

# Run the simple demo (trains in seconds!)
python tests/simple_demo.py
```

This trains a tiny 5,162-parameter transformer that learns to count (0â†’1â†’2â†’3...) with 100% accuracy.

## ğŸ—ï¸ Project Structure

```
transformer-llm-from-scratch/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ simple_demo.py        # ğŸŒŸ START HERE - Working transformer demo
â”‚   â””â”€â”€ test_model.py         # Component tests
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ attention.py       # Multi-head attention implementation  
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # Token and positional embeddings
â”‚   â”‚   â”œâ”€â”€ feedforward.py     # Feed-forward networks
â”‚   â”‚   â””â”€â”€ transformer.py     # Complete transformer model
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ tokenizer.py       # Tokenization logic
â”‚       â””â”€â”€ dataset.py         # Dataset handling
â”œâ”€â”€ notebooks/               # Educational Jupyter notebooks
â”‚   â”œâ”€â”€ 01_attention_mechanism.ipynb
â”‚   â”œâ”€â”€ 02_transformer_blocks.ipynb
â”‚   â”œâ”€â”€ 03_positional_encoding.ipynb
â”‚   â”œâ”€â”€ 04_training_process.ipynb
â”‚   â””â”€â”€ 05_text_generation.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # Full training script
â”‚   â””â”€â”€ generate.py           # Text generation script
â””â”€â”€ data/
    â”œâ”€â”€ shakespeare.txt       # Training data
    â””â”€â”€ simple_patterns.txt   # Simple pattern data
```

## ğŸš€ Learning Path

### 1. **Start with the Simple Demo** â­
```bash
python tests/simple_demo.py
```
- 5K parameter model that actually works
- Learns counting in 100 epochs (~10 seconds)
- Perfect for understanding basic transformer mechanics

### 2. **Explore the Full Implementation**
```bash
# Test the complete implementation
python tests/test_model.py

# Train on Shakespeare (6.8M parameters)
python scripts/train.py --model_size tiny --epochs 10 --batch_size 4
```

### 3. **Educational Notebooks**
```bash
jupyter notebook notebooks/01_attention_mechanism.ipynb
```

### 4. **Advanced Training**
```bash
# Train larger models
python scripts/train.py --model_size small --epochs 20 --text_file data/shakespeare.txt
```

## ğŸ”§ Features

- **ğŸ¯ Immediate Results**: Simple demo shows working transformer in seconds
- **ğŸ“š Educational Focus**: Every component heavily commented with mathematical explanations
- **ğŸ§© Modular Design**: Each component can be studied independently
- **ğŸ“ˆ Multiple Scales**: From 5K to 6.8M parameter models
- **ğŸ² Generation Strategies**: Greedy, top-k, top-p, and temperature sampling
- **ğŸ“Š Visualization**: Attention maps and training curves in notebooks

## ğŸ“Š Model Configurations

| Model | Parameters | Use Case | Training Time |
|-------|------------|----------|---------------|
| **Simple Demo** | 5,162 | Learning basics | 10 seconds |
| **Tiny** | 6.8M | Quick experiments | Minutes |
| **Small** | ~25M | Better text quality | Hours |
| **Medium** | ~100M | Research projects | Days |

## ğŸ§  Key Concepts Demonstrated

1. **Attention Mechanism** - How transformers "focus" on relevant tokens
2. **Multi-Head Attention** - Parallel attention for different representation spaces
3. **Positional Encoding** - Teaching models about sequence order
4. **Autoregressive Generation** - Next-token prediction
5. **Training Dynamics** - Loss curves, optimization, checkpointing

## ğŸ“ Why This Implementation?

- **From Scratch**: No black-box libraries - understand every line
- **Right-Sized Examples**: Start tiny, scale up gradually  
- **Immediate Feedback**: See transformers work before diving deep
- **Complete Pipeline**: Data â†’ Training â†’ Generation â†’ Evaluation